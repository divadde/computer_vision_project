{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divadde/computer_vision_project/blob/main/project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpPlRbfPFqUh"
      },
      "source": [
        "# Preparazione del dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w5ZdOV-FzX1"
      },
      "source": [
        "Come primo passo, importo tutte le librerie che sono necessarie per la realizzazione del task di segmentazione e object detection. Dalla preparazione del dataset, all'addestramento del modello fino alla fase di testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfxkTvRTka08"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from google.colab import drive\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import pycocotools\n",
        "from pycocotools import mask\n",
        "from torchvision.tv_tensors import BoundingBoxes, BoundingBoxFormat, Mask\n",
        "from torchvision.transforms.v2 import ConvertBoundingBoxFormat\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V44bsw-5Gaof"
      },
      "source": [
        "Scrivo una funzione ausiliaria *createLabels* che verrà utilizzata all'interno del dataset custom per l'estrazione e la creazione delle labels in un formato adatto al training del modello. Si può osservare dal file json delle etichette, che il formato delle box è del tipo XYWH, per l'addestramento del modello è richiesto che il formato sia del tipo XYXY. Per raggiungere tale scopo verrà usato il convertitore messo a disposizione da torchvision. Inoltre si è notato che all'interno del training set, sono presenti delle box degenerate (ovvero quando ampiezza o altezza è pari a 0), per questa ragione viene effettuato un controllo e se viene riscontrata una box degenerata, viene incrementato di un numero piccolo (=0.1) l'ampiezza o l'altezza degenere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRwyuuchU_TO"
      },
      "outputs": [],
      "source": [
        "def createLabels(json_file):\n",
        "  data = json_file['annotations']\n",
        "  grouped_data = defaultdict(list)\n",
        "\n",
        "  [grouped_data[k['image_id']].append(k) for k in data]\n",
        "\n",
        "  result_dict = dict(grouped_data)\n",
        "\n",
        "  ret_dict = {}\n",
        "  image_id = 0\n",
        "  for image in result_dict.values():\n",
        "    bbox = []\n",
        "    seg = []\n",
        "    cls = []\n",
        "    for obj in image:\n",
        "      image_id = obj['image_id']\n",
        "      seg.append(mask.decode(mask.merge(mask.frPyObjects(obj['segmentation'],700,700))))\n",
        "      b = obj['bbox']\n",
        "      #print(b)\n",
        "      #controllo se ci sono boxes degenerate\n",
        "      if b[2]==0: b[2]=b[2]+0.1\n",
        "      if b[3]==0: b[3]=b[3]+0.1\n",
        "      box = [b[0],b[1],b[2],b[3]] #formato XYWH\n",
        "      bbox.append(box)\n",
        "      cls.append(obj['category_id']+1) #la prima classe non deve avere id 0.\n",
        "    boundy_box = torchvision.tv_tensors.BoundingBoxes(data=np.array(bbox),format = BoundingBoxFormat.XYWH, canvas_size=(700, 700))\n",
        "    converter = torchvision.transforms.v2.ConvertBoundingBoxFormat(\"XYXY\")\n",
        "    final_box = converter._transform(boundy_box, {})\n",
        "    print(image_id)\n",
        "    ret_dict[image_id] = {\n",
        "        'boxes': final_box,\n",
        "        'labels': torch.tensor(cls, dtype = torch.int64),\n",
        "        'image_id': obj['image_id'],\n",
        "        'area': torch.tensor(obj['area'], dtype=torch.float64),\n",
        "        'iscrowd': torch.tensor(obj['iscrowd'], dtype=torch.uint8),\n",
        "        'masks': Mask(seg)\n",
        "    }\n",
        "\n",
        "  return ret_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco adesso, come estensione della classe Dataset di torch.utils, la nuova classe che ci permette di leggere il dataset (sia le immagini che le labels associate). Si può notare come nel costruttore della classe venga richiamata la funzione precedentemente definita *createLabels* per creare un grande dizionario che mappa ogni id dell'immagine con le sue label corrispondenti. Nel metodo getitem viene preso il percorso corrispondente all'immagine, viene letta l'immagine e restituita insieme alla label corrispondente."
      ],
      "metadata": {
        "id": "ONmCvnAMm9Yk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYqSswzohUDh"
      },
      "outputs": [],
      "source": [
        "class TTPLADataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,img_dir,labels_dir):\n",
        "    self.img_dir = img_dir\n",
        "    self.labels_json = json.load(open(labels_dir)) #apertura del file json delle etichette\n",
        "    self.image_labels = createLabels(self.labels_json)\n",
        "    self.categories = {category['name']: category['id'] for category in self.labels_json['categories']}\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img_path = os.path.join(self.img_dir, self.labels_json['images'][idx]['file_name'])\n",
        "    image = (torchvision.io.read_image(img_path)/255)\n",
        "    label = self.image_labels[idx]\n",
        "    return (image,label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(os.listdir(self.img_dir))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ_Z70U9UOlr"
      },
      "source": [
        "Carico il dataset di training attraverso il montaggio con Google Drive e sfrutto il dataset custom che ho scritto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvyg8wpuVRjf",
        "outputId": "ca06bddd-17fb-44ef-a789-8581c8a55668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/tv_tensors/_tv_tensor.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  return torch.as_tensor(data, dtype=dtype, device=device).requires_grad_(requires_grad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "train_set = TTPLADataset('/content/drive/MyDrive/trainingset', '/content/drive/MyDrive/train.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMVCA_K0SAg2"
      },
      "source": [
        "# Definizione della Mask-RCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il modello che verrà preso come riferimento per il completamento del task di segmentazione e object detection è la Mask-RCNN, modello studiato anche durante le lezioni dedicate alla segmentazione. La ragione della scelta risiede nel fatto che la Mask-RCNN è un modello molto popolare nella risoluzione di tali task oltre a presentare una struttura flessibile composta da backbone, testa di region proposal, testa di classificazione, testa di predizione delle box, e testa di predizione delle maschere per la segmentazione. Inoltre data proprio la sua flessibilità ne esistono numerose varianti che modificano la backbone e le varie teste per ottenere sempre prestazioni migliori. Nel mio caso, ho voluto costruire un modello che sfruttasse come backbone il modello ResNet152, il cui compito è quello di estrarre poi le feature che saranno da input per le teste della Mask-RCNN."
      ],
      "metadata": {
        "id": "XSWImFAenDjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMz9dCFSSFYY"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0cfW9QuSbgy"
      },
      "source": [
        "Di seguito è mostrata la definizione del modello della MaskRCNN. Si è deciso di trattare separatamente backbone, rpn_head, fastrcc_head e mask_head per una maggiore flessibilità e per poter provare a ogni esperimento delle strutture diverse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kWHuA5BSLPl"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import mask_rcnn, backbone_utils\n",
        "from torchvision.models.detection.faster_rcnn import RPNHead\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "from torchvision.models.resnet import resnet50, resnet101, resnet152\n",
        "from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers\n",
        "from torchvision.models.detection.faster_rcnn import _default_anchorgen, FasterRCNN, FastRCNNConvFCHead, RPNHead\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNHeads\n",
        "\n",
        "backbone = resnet152(weights=torchvision.models.ResNet152_Weights.IMAGENET1K_V2)\n",
        "backbone = _resnet_fpn_extractor(backbone, 5)\n",
        "\n",
        "rpn_anchor_generator = _default_anchorgen()\n",
        "rpn_head = RPNHead(backbone.out_channels, rpn_anchor_generator.num_anchors_per_location()[0], conv_depth=2)\n",
        "box_head = FastRCNNConvFCHead(\n",
        "        (backbone.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=nn.BatchNorm2d\n",
        "    )\n",
        "mask_head = MaskRCNNHeads(backbone.out_channels, [256, 256, 256, 256], 1, norm_layer=nn.BatchNorm2d)\n",
        "\n",
        "#print(backbone)\n",
        "\n",
        "model = mask_rcnn.MaskRCNN(backbone=backbone,\n",
        "                           num_classes=5,\n",
        "                           rpn_anchor_generator=rpn_anchor_generator,\n",
        "                           rpn_head=rpn_head,\n",
        "                           box_head=box_head,\n",
        "                           mask_head=mask_head) #rpn_head=rpn_head #si potrebbe implementare come testa di segmentazione deeplab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mi0vS0fbyqo"
      },
      "outputs": [],
      "source": [
        "#caricamento del modello\n",
        "\n",
        "model = torch.load('/content/drive/MyDrive/mask_rcnn_res50v2_70ep')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predispongo adesso la funzione *train* per l'addestramento del modello. Imposto learning rate, ottimizzatore, batch size e preparo il dataloader del trainset. L'ottimizzatore scelto è AdamW, variante dell'ottimizzatore Adam, il quale è in grado di regolarizzare i pesi durante l'addestramento"
      ],
      "metadata": {
        "id": "ObEbIDrLoS36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEMGo2kUerPh"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "batch_size = 4\n",
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle=True, collate_fn= lambda batch: tuple(zip(*batch)))\n",
        "total_step = len(train_dataloader)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "\n",
        "def train(epoch,model,optimizer):\n",
        "    loss_per_epoch = 0\n",
        "    counter=0\n",
        "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
        "\n",
        "        counter = counter + 1\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{\n",
        "                    'boxes': target['boxes'].to(device),\n",
        "                    'labels': target['labels'].to(device),\n",
        "                    'image_id': target['image_id'],\n",
        "                    'area': target['area'].to(device),\n",
        "                    'iscrowd':target['boxes'].to(device),\n",
        "                    'masks': target['masks'].to(device)\n",
        "                    } for target in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        #prova cambio penalità su loss_mask\n",
        "        #loss_dict[\"loss_mask\"]=loss_dict[\"loss_mask\"]*1.5\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_per_epoch = loss_per_epoch + losses\n",
        "\n",
        "        # Backward pass\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        if (batch_idx+1) % 1 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch, num_epochs, batch_idx+1, total_step, losses))\n",
        "\n",
        "        if counter==total_step:\n",
        "          med_loss = loss_per_epoch/counter\n",
        "          print('Loss media: {:.4f}'.format(med_loss))\n",
        "          counter=0\n",
        "          loss_per_epoch=0\n",
        "\n",
        "        train_losses.append(losses)\n",
        "        train_counter.append(\n",
        "        (batch_idx*batch_size) + ((epoch-1)*len(train_dataloader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzbgjM3cS2BQ",
        "outputId": "f21951ec-762f-42a6-d637-a5b8af68293c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Epoch [28/100], Step [68/211], Loss: 0.4268\n",
            "Epoch [28/100], Step [69/211], Loss: 0.1748\n",
            "Epoch [28/100], Step [70/211], Loss: 0.2557\n",
            "Epoch [28/100], Step [71/211], Loss: 0.4764\n",
            "Epoch [28/100], Step [72/211], Loss: 0.2345\n",
            "Epoch [28/100], Step [73/211], Loss: 0.5238\n",
            "Epoch [28/100], Step [74/211], Loss: 0.3331\n",
            "Epoch [28/100], Step [75/211], Loss: 0.3701\n",
            "Epoch [28/100], Step [76/211], Loss: 0.5660\n",
            "Epoch [28/100], Step [77/211], Loss: 0.3653\n",
            "Epoch [28/100], Step [78/211], Loss: 0.4644\n",
            "Epoch [28/100], Step [79/211], Loss: 0.3783\n",
            "Epoch [28/100], Step [80/211], Loss: 0.2560\n",
            "Epoch [28/100], Step [81/211], Loss: 0.4617\n",
            "Epoch [28/100], Step [82/211], Loss: 0.2135\n",
            "Epoch [28/100], Step [83/211], Loss: 0.1032\n",
            "Epoch [28/100], Step [84/211], Loss: 0.2503\n",
            "Epoch [28/100], Step [85/211], Loss: 0.2585\n",
            "Epoch [28/100], Step [86/211], Loss: 0.4568\n",
            "Epoch [28/100], Step [87/211], Loss: 0.2728\n",
            "Epoch [28/100], Step [88/211], Loss: 0.3470\n",
            "Epoch [28/100], Step [89/211], Loss: 0.2085\n",
            "Epoch [28/100], Step [90/211], Loss: 0.2928\n",
            "Epoch [28/100], Step [91/211], Loss: 0.4076\n",
            "Epoch [28/100], Step [92/211], Loss: 0.3918\n",
            "Epoch [28/100], Step [93/211], Loss: 0.5929\n",
            "Epoch [28/100], Step [94/211], Loss: 0.3042\n",
            "Epoch [28/100], Step [95/211], Loss: 0.3880\n",
            "Epoch [28/100], Step [96/211], Loss: 0.4823\n",
            "Epoch [28/100], Step [97/211], Loss: 0.5195\n",
            "Epoch [28/100], Step [98/211], Loss: 0.3671\n",
            "Epoch [28/100], Step [99/211], Loss: 0.2780\n",
            "Epoch [28/100], Step [100/211], Loss: 0.1390\n",
            "Epoch [28/100], Step [101/211], Loss: 0.2229\n",
            "Epoch [28/100], Step [102/211], Loss: 0.4555\n",
            "Epoch [28/100], Step [103/211], Loss: 0.3392\n",
            "Epoch [28/100], Step [104/211], Loss: 0.5502\n",
            "Epoch [28/100], Step [105/211], Loss: 0.1484\n",
            "Epoch [28/100], Step [106/211], Loss: 0.3252\n",
            "Epoch [28/100], Step [107/211], Loss: 0.1636\n",
            "Epoch [28/100], Step [108/211], Loss: 0.3692\n",
            "Epoch [28/100], Step [109/211], Loss: 0.2323\n",
            "Epoch [28/100], Step [110/211], Loss: 0.1616\n",
            "Epoch [28/100], Step [111/211], Loss: 0.4170\n",
            "Epoch [28/100], Step [112/211], Loss: 0.4163\n",
            "Epoch [28/100], Step [113/211], Loss: 0.3579\n",
            "Epoch [28/100], Step [114/211], Loss: 0.2003\n",
            "Epoch [28/100], Step [115/211], Loss: 0.1169\n",
            "Epoch [28/100], Step [116/211], Loss: 0.1340\n",
            "Epoch [28/100], Step [117/211], Loss: 0.2471\n",
            "Epoch [28/100], Step [118/211], Loss: 0.4814\n",
            "Epoch [28/100], Step [119/211], Loss: 0.2083\n",
            "Epoch [28/100], Step [120/211], Loss: 0.3466\n",
            "Epoch [28/100], Step [121/211], Loss: 0.3513\n",
            "Epoch [28/100], Step [122/211], Loss: 0.2879\n",
            "Epoch [28/100], Step [123/211], Loss: 0.2956\n",
            "Epoch [28/100], Step [124/211], Loss: 0.1595\n",
            "Epoch [28/100], Step [125/211], Loss: 0.1883\n",
            "Epoch [28/100], Step [126/211], Loss: 0.3188\n",
            "Epoch [28/100], Step [127/211], Loss: 0.2802\n",
            "Epoch [28/100], Step [128/211], Loss: 0.5415\n",
            "Epoch [28/100], Step [129/211], Loss: 0.2634\n",
            "Epoch [28/100], Step [130/211], Loss: 0.2646\n",
            "Epoch [28/100], Step [131/211], Loss: 0.2727\n",
            "Epoch [28/100], Step [132/211], Loss: 0.2205\n",
            "Epoch [28/100], Step [133/211], Loss: 0.2288\n",
            "Epoch [28/100], Step [134/211], Loss: 0.1576\n",
            "Epoch [28/100], Step [135/211], Loss: 0.2778\n",
            "Epoch [28/100], Step [136/211], Loss: 0.5814\n",
            "Epoch [28/100], Step [137/211], Loss: 0.4279\n",
            "Epoch [28/100], Step [138/211], Loss: 0.4622\n",
            "Epoch [28/100], Step [139/211], Loss: 0.0933\n",
            "Epoch [28/100], Step [140/211], Loss: 0.1098\n",
            "Epoch [28/100], Step [141/211], Loss: 0.4517\n",
            "Epoch [28/100], Step [142/211], Loss: 0.1391\n",
            "Epoch [28/100], Step [143/211], Loss: 0.3742\n",
            "Epoch [28/100], Step [144/211], Loss: 0.2086\n",
            "Epoch [28/100], Step [145/211], Loss: 0.3079\n",
            "Epoch [28/100], Step [146/211], Loss: 0.1716\n",
            "Epoch [28/100], Step [147/211], Loss: 0.3228\n",
            "Epoch [28/100], Step [148/211], Loss: 0.2978\n",
            "Epoch [28/100], Step [149/211], Loss: 0.3783\n",
            "Epoch [28/100], Step [150/211], Loss: 0.2560\n",
            "Epoch [28/100], Step [151/211], Loss: 0.2896\n",
            "Epoch [28/100], Step [152/211], Loss: 0.1470\n",
            "Epoch [28/100], Step [153/211], Loss: 0.7040\n",
            "Epoch [28/100], Step [154/211], Loss: 0.3918\n",
            "Epoch [28/100], Step [155/211], Loss: 0.2991\n",
            "Epoch [28/100], Step [156/211], Loss: 0.2607\n",
            "Epoch [28/100], Step [157/211], Loss: 0.3319\n",
            "Epoch [28/100], Step [158/211], Loss: 0.3638\n",
            "Epoch [28/100], Step [159/211], Loss: 0.2255\n",
            "Epoch [28/100], Step [160/211], Loss: 0.2020\n",
            "Epoch [28/100], Step [161/211], Loss: 0.1981\n",
            "Epoch [28/100], Step [162/211], Loss: 0.2756\n",
            "Epoch [28/100], Step [163/211], Loss: 0.3120\n",
            "Epoch [28/100], Step [164/211], Loss: 0.2834\n",
            "Epoch [28/100], Step [165/211], Loss: 0.2200\n",
            "Epoch [28/100], Step [166/211], Loss: 0.2609\n",
            "Epoch [28/100], Step [167/211], Loss: 0.2712\n",
            "Epoch [28/100], Step [168/211], Loss: 0.2110\n",
            "Epoch [28/100], Step [169/211], Loss: 0.2061\n",
            "Epoch [28/100], Step [170/211], Loss: 0.2354\n",
            "Epoch [28/100], Step [171/211], Loss: 0.3295\n",
            "Epoch [28/100], Step [172/211], Loss: 0.2440\n",
            "Epoch [28/100], Step [173/211], Loss: 0.1767\n",
            "Epoch [28/100], Step [174/211], Loss: 0.3888\n",
            "Epoch [28/100], Step [175/211], Loss: 0.2436\n",
            "Epoch [28/100], Step [176/211], Loss: 0.3129\n",
            "Epoch [28/100], Step [177/211], Loss: 0.3125\n",
            "Epoch [28/100], Step [178/211], Loss: 0.3929\n",
            "Epoch [28/100], Step [179/211], Loss: 0.1338\n",
            "Epoch [28/100], Step [180/211], Loss: 0.2295\n",
            "Epoch [28/100], Step [181/211], Loss: 0.2891\n",
            "Epoch [28/100], Step [182/211], Loss: 0.3938\n",
            "Epoch [28/100], Step [183/211], Loss: 0.1572\n",
            "Epoch [28/100], Step [184/211], Loss: 0.2640\n",
            "Epoch [28/100], Step [185/211], Loss: 0.4620\n",
            "Epoch [28/100], Step [186/211], Loss: 0.4365\n",
            "Epoch [28/100], Step [187/211], Loss: 0.2041\n",
            "Epoch [28/100], Step [188/211], Loss: 0.2984\n",
            "Epoch [28/100], Step [189/211], Loss: 0.4052\n",
            "Epoch [28/100], Step [190/211], Loss: 0.1621\n",
            "Epoch [28/100], Step [191/211], Loss: 0.4747\n",
            "Epoch [28/100], Step [192/211], Loss: 0.4408\n",
            "Epoch [28/100], Step [193/211], Loss: 0.3700\n",
            "Epoch [28/100], Step [194/211], Loss: 0.1876\n",
            "Epoch [28/100], Step [195/211], Loss: 0.1345\n",
            "Epoch [28/100], Step [196/211], Loss: 0.2687\n",
            "Epoch [28/100], Step [197/211], Loss: 0.3062\n",
            "Epoch [28/100], Step [198/211], Loss: 0.3566\n",
            "Epoch [28/100], Step [199/211], Loss: 0.3316\n",
            "Epoch [28/100], Step [200/211], Loss: 0.2351\n",
            "Epoch [28/100], Step [201/211], Loss: 0.2893\n",
            "Epoch [28/100], Step [202/211], Loss: 0.2420\n",
            "Epoch [28/100], Step [203/211], Loss: 0.2826\n",
            "Epoch [28/100], Step [204/211], Loss: 0.3301\n",
            "Epoch [28/100], Step [205/211], Loss: 0.5183\n",
            "Epoch [28/100], Step [206/211], Loss: 0.3553\n",
            "Epoch [28/100], Step [207/211], Loss: 0.2865\n",
            "Epoch [28/100], Step [208/211], Loss: 0.1546\n",
            "Epoch [28/100], Step [209/211], Loss: 0.1222\n",
            "Epoch [28/100], Step [210/211], Loss: 0.3619\n",
            "Epoch [28/100], Step [211/211], Loss: 0.2781\n",
            "Loss media: 0.3119\n",
            "Epoch [29/100], Step [1/211], Loss: 0.3370\n",
            "Epoch [29/100], Step [2/211], Loss: 0.3211\n",
            "Epoch [29/100], Step [3/211], Loss: 0.1791\n",
            "Epoch [29/100], Step [4/211], Loss: 0.2164\n",
            "Epoch [29/100], Step [5/211], Loss: 0.2305\n",
            "Epoch [29/100], Step [6/211], Loss: 0.1949\n",
            "Epoch [29/100], Step [7/211], Loss: 0.3871\n",
            "Epoch [29/100], Step [8/211], Loss: 0.3488\n",
            "Epoch [29/100], Step [9/211], Loss: 0.1469\n",
            "Epoch [29/100], Step [10/211], Loss: 0.5217\n",
            "Epoch [29/100], Step [11/211], Loss: 0.3463\n",
            "Epoch [29/100], Step [12/211], Loss: 0.2539\n",
            "Epoch [29/100], Step [13/211], Loss: 0.1731\n",
            "Epoch [29/100], Step [14/211], Loss: 0.1750\n",
            "Epoch [29/100], Step [15/211], Loss: 0.4344\n",
            "Epoch [29/100], Step [16/211], Loss: 0.3783\n",
            "Epoch [29/100], Step [17/211], Loss: 0.4286\n",
            "Epoch [29/100], Step [18/211], Loss: 0.3964\n",
            "Epoch [29/100], Step [19/211], Loss: 0.2484\n",
            "Epoch [29/100], Step [20/211], Loss: 0.3937\n",
            "Epoch [29/100], Step [21/211], Loss: 0.2491\n",
            "Epoch [29/100], Step [22/211], Loss: 0.2534\n",
            "Epoch [29/100], Step [23/211], Loss: 0.3264\n",
            "Epoch [29/100], Step [24/211], Loss: 0.4171\n",
            "Epoch [29/100], Step [25/211], Loss: 0.1510\n",
            "Epoch [29/100], Step [26/211], Loss: 0.2457\n",
            "Epoch [29/100], Step [27/211], Loss: 0.3625\n",
            "Epoch [29/100], Step [28/211], Loss: 0.3553\n",
            "Epoch [29/100], Step [29/211], Loss: 0.3894\n",
            "Epoch [29/100], Step [30/211], Loss: 0.2624\n",
            "Epoch [29/100], Step [31/211], Loss: 0.3694\n",
            "Epoch [29/100], Step [32/211], Loss: 0.1767\n",
            "Epoch [29/100], Step [33/211], Loss: 0.1854\n",
            "Epoch [29/100], Step [34/211], Loss: 0.1169\n",
            "Epoch [29/100], Step [35/211], Loss: 0.3967\n",
            "Epoch [29/100], Step [36/211], Loss: 0.4172\n",
            "Epoch [29/100], Step [37/211], Loss: 0.4580\n",
            "Epoch [29/100], Step [38/211], Loss: 0.1866\n",
            "Epoch [29/100], Step [39/211], Loss: 0.5462\n",
            "Epoch [29/100], Step [40/211], Loss: 0.4116\n",
            "Epoch [29/100], Step [41/211], Loss: 0.4657\n",
            "Epoch [29/100], Step [42/211], Loss: 0.2334\n",
            "Epoch [29/100], Step [43/211], Loss: 0.1655\n",
            "Epoch [29/100], Step [44/211], Loss: 0.1487\n",
            "Epoch [29/100], Step [45/211], Loss: 0.4820\n",
            "Epoch [29/100], Step [46/211], Loss: 0.3894\n",
            "Epoch [29/100], Step [47/211], Loss: 0.3541\n",
            "Epoch [29/100], Step [48/211], Loss: 0.3121\n",
            "Epoch [29/100], Step [49/211], Loss: 0.3495\n",
            "Epoch [29/100], Step [50/211], Loss: 0.1953\n",
            "Epoch [29/100], Step [51/211], Loss: 0.1917\n",
            "Epoch [29/100], Step [52/211], Loss: 0.2293\n",
            "Epoch [29/100], Step [53/211], Loss: 0.2500\n",
            "Epoch [29/100], Step [54/211], Loss: 0.3744\n",
            "Epoch [29/100], Step [55/211], Loss: 0.1466\n",
            "Epoch [29/100], Step [56/211], Loss: 0.3576\n",
            "Epoch [29/100], Step [57/211], Loss: 0.3017\n",
            "Epoch [29/100], Step [58/211], Loss: 0.2621\n",
            "Epoch [29/100], Step [59/211], Loss: 0.1738\n",
            "Epoch [29/100], Step [60/211], Loss: 0.2061\n",
            "Epoch [29/100], Step [61/211], Loss: 0.5008\n",
            "Epoch [29/100], Step [62/211], Loss: 0.4359\n",
            "Epoch [29/100], Step [63/211], Loss: 0.1531\n",
            "Epoch [29/100], Step [64/211], Loss: 0.3578\n",
            "Epoch [29/100], Step [65/211], Loss: 0.2816\n",
            "Epoch [29/100], Step [66/211], Loss: 0.3563\n",
            "Epoch [29/100], Step [67/211], Loss: 0.4749\n",
            "Epoch [29/100], Step [68/211], Loss: 0.2216\n",
            "Epoch [29/100], Step [69/211], Loss: 0.2738\n",
            "Epoch [29/100], Step [70/211], Loss: 0.4290\n",
            "Epoch [29/100], Step [71/211], Loss: 0.2278\n",
            "Epoch [29/100], Step [72/211], Loss: 0.6579\n",
            "Epoch [29/100], Step [73/211], Loss: 0.3379\n",
            "Epoch [29/100], Step [74/211], Loss: 0.4948\n",
            "Epoch [29/100], Step [75/211], Loss: 0.2210\n",
            "Epoch [29/100], Step [76/211], Loss: 0.4711\n",
            "Epoch [29/100], Step [77/211], Loss: 0.1518\n",
            "Epoch [29/100], Step [78/211], Loss: 0.2413\n",
            "Epoch [29/100], Step [79/211], Loss: 0.2810\n",
            "Epoch [29/100], Step [80/211], Loss: 0.3974\n",
            "Epoch [29/100], Step [81/211], Loss: 0.2855\n",
            "Epoch [29/100], Step [82/211], Loss: 0.2180\n",
            "Epoch [29/100], Step [83/211], Loss: 0.1607\n",
            "Epoch [29/100], Step [84/211], Loss: 0.2902\n",
            "Epoch [29/100], Step [85/211], Loss: 0.1297\n",
            "Epoch [29/100], Step [86/211], Loss: 0.5868\n",
            "Epoch [29/100], Step [87/211], Loss: 0.5641\n",
            "Epoch [29/100], Step [88/211], Loss: 0.3856\n",
            "Epoch [29/100], Step [89/211], Loss: 0.5018\n",
            "Epoch [29/100], Step [90/211], Loss: 0.4874\n",
            "Epoch [29/100], Step [91/211], Loss: 0.3740\n",
            "Epoch [29/100], Step [92/211], Loss: 0.1518\n",
            "Epoch [29/100], Step [93/211], Loss: 0.4563\n",
            "Epoch [29/100], Step [94/211], Loss: 0.3024\n",
            "Epoch [29/100], Step [95/211], Loss: 0.2716\n",
            "Epoch [29/100], Step [96/211], Loss: 0.3100\n",
            "Epoch [29/100], Step [97/211], Loss: 0.1630\n",
            "Epoch [29/100], Step [98/211], Loss: 0.2681\n",
            "Epoch [29/100], Step [99/211], Loss: 0.2803\n",
            "Epoch [29/100], Step [100/211], Loss: 0.3399\n",
            "Epoch [29/100], Step [101/211], Loss: 0.6606\n",
            "Epoch [29/100], Step [102/211], Loss: 0.2472\n",
            "Epoch [29/100], Step [103/211], Loss: 0.1096\n",
            "Epoch [29/100], Step [104/211], Loss: 0.2465\n",
            "Epoch [29/100], Step [105/211], Loss: 0.1596\n",
            "Epoch [29/100], Step [106/211], Loss: 0.1866\n",
            "Epoch [29/100], Step [107/211], Loss: 0.2508\n",
            "Epoch [29/100], Step [108/211], Loss: 0.3183\n",
            "Epoch [29/100], Step [109/211], Loss: 0.3175\n",
            "Epoch [29/100], Step [110/211], Loss: 0.4685\n",
            "Epoch [29/100], Step [111/211], Loss: 0.8032\n",
            "Epoch [29/100], Step [112/211], Loss: 0.1928\n",
            "Epoch [29/100], Step [113/211], Loss: 0.2241\n",
            "Epoch [29/100], Step [114/211], Loss: 0.1960\n",
            "Epoch [29/100], Step [115/211], Loss: 0.4818\n",
            "Epoch [29/100], Step [116/211], Loss: 0.2545\n",
            "Epoch [29/100], Step [117/211], Loss: 0.3943\n",
            "Epoch [29/100], Step [118/211], Loss: 0.2264\n",
            "Epoch [29/100], Step [119/211], Loss: 0.1177\n",
            "Epoch [29/100], Step [120/211], Loss: 0.6232\n",
            "Epoch [29/100], Step [121/211], Loss: 0.4164\n",
            "Epoch [29/100], Step [122/211], Loss: 0.3487\n",
            "Epoch [29/100], Step [123/211], Loss: 0.2398\n",
            "Epoch [29/100], Step [124/211], Loss: 0.1041\n",
            "Epoch [29/100], Step [125/211], Loss: 0.2364\n",
            "Epoch [29/100], Step [126/211], Loss: 0.2694\n",
            "Epoch [29/100], Step [127/211], Loss: 0.2940\n",
            "Epoch [29/100], Step [128/211], Loss: 0.4581\n",
            "Epoch [29/100], Step [129/211], Loss: 0.2152\n",
            "Epoch [29/100], Step [130/211], Loss: 0.2947\n",
            "Epoch [29/100], Step [131/211], Loss: 0.4049\n",
            "Epoch [29/100], Step [132/211], Loss: 0.4832\n",
            "Epoch [29/100], Step [133/211], Loss: 0.2824\n",
            "Epoch [29/100], Step [134/211], Loss: 0.4662\n",
            "Epoch [29/100], Step [135/211], Loss: 0.2554\n",
            "Epoch [29/100], Step [136/211], Loss: 0.1606\n",
            "Epoch [29/100], Step [137/211], Loss: 0.3255\n",
            "Epoch [29/100], Step [138/211], Loss: 0.2748\n",
            "Epoch [29/100], Step [139/211], Loss: 0.4069\n",
            "Epoch [29/100], Step [140/211], Loss: 0.2251\n",
            "Epoch [29/100], Step [141/211], Loss: 0.3535\n",
            "Epoch [29/100], Step [142/211], Loss: 0.2975\n",
            "Epoch [29/100], Step [143/211], Loss: 0.4186\n",
            "Epoch [29/100], Step [144/211], Loss: 0.2189\n",
            "Epoch [29/100], Step [145/211], Loss: 0.1042\n",
            "Epoch [29/100], Step [146/211], Loss: 0.2662\n",
            "Epoch [29/100], Step [147/211], Loss: 0.1993\n",
            "Epoch [29/100], Step [148/211], Loss: 0.7494\n",
            "Epoch [29/100], Step [149/211], Loss: 0.5122\n",
            "Epoch [29/100], Step [150/211], Loss: 0.1751\n",
            "Epoch [29/100], Step [151/211], Loss: 0.1671\n",
            "Epoch [29/100], Step [152/211], Loss: 0.5208\n",
            "Epoch [29/100], Step [153/211], Loss: 0.1870\n",
            "Epoch [29/100], Step [154/211], Loss: 0.2114\n",
            "Epoch [29/100], Step [155/211], Loss: 0.4713\n",
            "Epoch [29/100], Step [156/211], Loss: 0.2365\n",
            "Epoch [29/100], Step [157/211], Loss: 0.2488\n",
            "Epoch [29/100], Step [158/211], Loss: 0.1732\n",
            "Epoch [29/100], Step [159/211], Loss: 0.6181\n",
            "Epoch [29/100], Step [160/211], Loss: 0.2203\n",
            "Epoch [29/100], Step [161/211], Loss: 0.3487\n",
            "Epoch [29/100], Step [162/211], Loss: 0.3737\n",
            "Epoch [29/100], Step [163/211], Loss: 0.2327\n",
            "Epoch [29/100], Step [164/211], Loss: 0.4661\n",
            "Epoch [29/100], Step [165/211], Loss: 0.1197\n",
            "Epoch [29/100], Step [166/211], Loss: 0.2133\n",
            "Epoch [29/100], Step [167/211], Loss: 0.2895\n",
            "Epoch [29/100], Step [168/211], Loss: 0.1963\n",
            "Epoch [29/100], Step [169/211], Loss: 0.1297\n",
            "Epoch [29/100], Step [170/211], Loss: 0.5016\n",
            "Epoch [29/100], Step [171/211], Loss: 0.2692\n",
            "Epoch [29/100], Step [172/211], Loss: 0.2978\n",
            "Epoch [29/100], Step [173/211], Loss: 0.2608\n",
            "Epoch [29/100], Step [174/211], Loss: 0.2374\n",
            "Epoch [29/100], Step [175/211], Loss: 0.3077\n",
            "Epoch [29/100], Step [176/211], Loss: 0.2585\n",
            "Epoch [29/100], Step [177/211], Loss: 0.1739\n",
            "Epoch [29/100], Step [178/211], Loss: 0.3273\n",
            "Epoch [29/100], Step [179/211], Loss: 0.1579\n",
            "Epoch [29/100], Step [180/211], Loss: 0.1346\n",
            "Epoch [29/100], Step [181/211], Loss: 0.4979\n",
            "Epoch [29/100], Step [182/211], Loss: 0.1697\n",
            "Epoch [29/100], Step [183/211], Loss: 0.2431\n",
            "Epoch [29/100], Step [184/211], Loss: 0.2439\n",
            "Epoch [29/100], Step [185/211], Loss: 0.2442\n",
            "Epoch [29/100], Step [186/211], Loss: 0.5953\n",
            "Epoch [29/100], Step [187/211], Loss: 0.3068\n",
            "Epoch [29/100], Step [188/211], Loss: 0.0954\n",
            "Epoch [29/100], Step [189/211], Loss: 0.3435\n",
            "Epoch [29/100], Step [190/211], Loss: 0.3218\n",
            "Epoch [29/100], Step [191/211], Loss: 0.2969\n",
            "Epoch [29/100], Step [192/211], Loss: 0.2452\n",
            "Epoch [29/100], Step [193/211], Loss: 0.2265\n",
            "Epoch [29/100], Step [194/211], Loss: 0.2946\n",
            "Epoch [29/100], Step [195/211], Loss: 0.4269\n",
            "Epoch [29/100], Step [196/211], Loss: 0.2193\n",
            "Epoch [29/100], Step [197/211], Loss: 0.3204\n",
            "Epoch [29/100], Step [198/211], Loss: 0.2211\n",
            "Epoch [29/100], Step [199/211], Loss: 0.2237\n",
            "Epoch [29/100], Step [200/211], Loss: 0.1096\n",
            "Epoch [29/100], Step [201/211], Loss: 0.2105\n",
            "Epoch [29/100], Step [202/211], Loss: 0.2468\n",
            "Epoch [29/100], Step [203/211], Loss: 0.2237\n",
            "Epoch [29/100], Step [204/211], Loss: 0.4606\n",
            "Epoch [29/100], Step [205/211], Loss: 0.3423\n",
            "Epoch [29/100], Step [206/211], Loss: 0.4243\n",
            "Epoch [29/100], Step [207/211], Loss: 0.1987\n",
            "Epoch [29/100], Step [208/211], Loss: 0.3500\n",
            "Epoch [29/100], Step [209/211], Loss: 0.2658\n",
            "Epoch [29/100], Step [210/211], Loss: 0.0783\n",
            "Epoch [29/100], Step [211/211], Loss: 0.3935\n",
            "Loss media: 0.3062\n",
            "Epoch [30/100], Step [1/211], Loss: 0.2573\n",
            "Epoch [30/100], Step [2/211], Loss: 0.3689\n",
            "Epoch [30/100], Step [3/211], Loss: 0.0805\n",
            "Epoch [30/100], Step [4/211], Loss: 0.3969\n",
            "Epoch [30/100], Step [5/211], Loss: 0.5186\n",
            "Epoch [30/100], Step [6/211], Loss: 0.3257\n",
            "Epoch [30/100], Step [7/211], Loss: 0.3834\n",
            "Epoch [30/100], Step [8/211], Loss: 0.4477\n",
            "Epoch [30/100], Step [9/211], Loss: 0.1844\n",
            "Epoch [30/100], Step [10/211], Loss: 0.2182\n",
            "Epoch [30/100], Step [11/211], Loss: 0.1748\n",
            "Epoch [30/100], Step [12/211], Loss: 0.2579\n",
            "Epoch [30/100], Step [13/211], Loss: 0.2641\n",
            "Epoch [30/100], Step [14/211], Loss: 0.6209\n",
            "Epoch [30/100], Step [15/211], Loss: 0.1689\n",
            "Epoch [30/100], Step [16/211], Loss: 0.3620\n",
            "Epoch [30/100], Step [17/211], Loss: 0.2824\n",
            "Epoch [30/100], Step [18/211], Loss: 0.4179\n",
            "Epoch [30/100], Step [19/211], Loss: 0.3426\n",
            "Epoch [30/100], Step [20/211], Loss: 0.3777\n",
            "Epoch [30/100], Step [21/211], Loss: 0.2319\n",
            "Epoch [30/100], Step [22/211], Loss: 0.3494\n",
            "Epoch [30/100], Step [23/211], Loss: 0.1469\n",
            "Epoch [30/100], Step [24/211], Loss: 0.3357\n",
            "Epoch [30/100], Step [25/211], Loss: 0.4088\n",
            "Epoch [30/100], Step [26/211], Loss: 0.3946\n",
            "Epoch [30/100], Step [27/211], Loss: 0.3129\n",
            "Epoch [30/100], Step [28/211], Loss: 0.1676\n",
            "Epoch [30/100], Step [29/211], Loss: 0.5485\n",
            "Epoch [30/100], Step [30/211], Loss: 0.5790\n",
            "Epoch [30/100], Step [31/211], Loss: 0.1461\n",
            "Epoch [30/100], Step [32/211], Loss: 0.1968\n",
            "Epoch [30/100], Step [33/211], Loss: 0.3202\n",
            "Epoch [30/100], Step [34/211], Loss: 0.2167\n",
            "Epoch [30/100], Step [35/211], Loss: 0.2603\n",
            "Epoch [30/100], Step [36/211], Loss: 0.4910\n",
            "Epoch [30/100], Step [37/211], Loss: 0.6387\n",
            "Epoch [30/100], Step [38/211], Loss: 0.1576\n",
            "Epoch [30/100], Step [39/211], Loss: 0.3001\n",
            "Epoch [30/100], Step [40/211], Loss: 0.3310\n",
            "Epoch [30/100], Step [41/211], Loss: 0.2446\n",
            "Epoch [30/100], Step [42/211], Loss: 0.2999\n",
            "Epoch [30/100], Step [43/211], Loss: 0.2155\n",
            "Epoch [30/100], Step [44/211], Loss: 0.1101\n",
            "Epoch [30/100], Step [45/211], Loss: 0.2763\n",
            "Epoch [30/100], Step [46/211], Loss: 0.4957\n",
            "Epoch [30/100], Step [47/211], Loss: 0.3006\n",
            "Epoch [30/100], Step [48/211], Loss: 0.3373\n",
            "Epoch [30/100], Step [49/211], Loss: 0.1278\n",
            "Epoch [30/100], Step [50/211], Loss: 0.1810\n",
            "Epoch [30/100], Step [51/211], Loss: 0.3835\n",
            "Epoch [30/100], Step [52/211], Loss: 0.1527\n",
            "Epoch [30/100], Step [53/211], Loss: 0.2653\n",
            "Epoch [30/100], Step [54/211], Loss: 0.2817\n",
            "Epoch [30/100], Step [55/211], Loss: 0.2296\n",
            "Epoch [30/100], Step [56/211], Loss: 0.1949\n",
            "Epoch [30/100], Step [57/211], Loss: 0.2050\n",
            "Epoch [30/100], Step [58/211], Loss: 0.2778\n",
            "Epoch [30/100], Step [59/211], Loss: 0.1170\n",
            "Epoch [30/100], Step [60/211], Loss: 0.5665\n",
            "Epoch [30/100], Step [61/211], Loss: 0.2731\n",
            "Epoch [30/100], Step [62/211], Loss: 0.2807\n",
            "Epoch [30/100], Step [63/211], Loss: 0.2105\n",
            "Epoch [30/100], Step [64/211], Loss: 0.3236\n",
            "Epoch [30/100], Step [65/211], Loss: 0.2604\n",
            "Epoch [30/100], Step [66/211], Loss: 0.1365\n",
            "Epoch [30/100], Step [67/211], Loss: 0.1834\n",
            "Epoch [30/100], Step [68/211], Loss: 0.2123\n",
            "Epoch [30/100], Step [69/211], Loss: 0.2964\n",
            "Epoch [30/100], Step [70/211], Loss: 0.4055\n",
            "Epoch [30/100], Step [71/211], Loss: 0.2349\n",
            "Epoch [30/100], Step [72/211], Loss: 0.3554\n",
            "Epoch [30/100], Step [73/211], Loss: 0.1672\n",
            "Epoch [30/100], Step [74/211], Loss: 0.2213\n",
            "Epoch [30/100], Step [75/211], Loss: 0.3252\n",
            "Epoch [30/100], Step [76/211], Loss: 0.1986\n",
            "Epoch [30/100], Step [77/211], Loss: 0.2118\n",
            "Epoch [30/100], Step [78/211], Loss: 0.4584\n",
            "Epoch [30/100], Step [79/211], Loss: 0.3187\n",
            "Epoch [30/100], Step [80/211], Loss: 0.3427\n",
            "Epoch [30/100], Step [81/211], Loss: 0.2845\n",
            "Epoch [30/100], Step [82/211], Loss: 0.3930\n",
            "Epoch [30/100], Step [83/211], Loss: 0.3789\n",
            "Epoch [30/100], Step [84/211], Loss: 0.1448\n",
            "Epoch [30/100], Step [85/211], Loss: 0.7722\n",
            "Epoch [30/100], Step [86/211], Loss: 0.3314\n",
            "Epoch [30/100], Step [87/211], Loss: 0.2090\n",
            "Epoch [30/100], Step [88/211], Loss: 0.4652\n",
            "Epoch [30/100], Step [89/211], Loss: 0.5860\n",
            "Epoch [30/100], Step [90/211], Loss: 0.2902\n",
            "Epoch [30/100], Step [91/211], Loss: 0.4657\n",
            "Epoch [30/100], Step [92/211], Loss: 0.2632\n",
            "Epoch [30/100], Step [93/211], Loss: 0.1421\n",
            "Epoch [30/100], Step [94/211], Loss: 0.4152\n",
            "Epoch [30/100], Step [95/211], Loss: 0.1656\n",
            "Epoch [30/100], Step [96/211], Loss: 0.2307\n",
            "Epoch [30/100], Step [97/211], Loss: 0.3514\n",
            "Epoch [30/100], Step [98/211], Loss: 0.2660\n",
            "Epoch [30/100], Step [99/211], Loss: 0.5039\n",
            "Epoch [30/100], Step [100/211], Loss: 0.2430\n",
            "Epoch [30/100], Step [101/211], Loss: 0.2712\n",
            "Epoch [30/100], Step [102/211], Loss: 0.1667\n",
            "Epoch [30/100], Step [103/211], Loss: 0.4266\n",
            "Epoch [30/100], Step [104/211], Loss: 0.4235\n",
            "Epoch [30/100], Step [105/211], Loss: 0.2565\n",
            "Epoch [30/100], Step [106/211], Loss: 0.2528\n",
            "Epoch [30/100], Step [107/211], Loss: 0.2067\n",
            "Epoch [30/100], Step [108/211], Loss: 0.1226\n",
            "Epoch [30/100], Step [109/211], Loss: 0.1297\n",
            "Epoch [30/100], Step [110/211], Loss: 0.6597\n",
            "Epoch [30/100], Step [111/211], Loss: 0.5636\n",
            "Epoch [30/100], Step [112/211], Loss: 0.1351\n",
            "Epoch [30/100], Step [113/211], Loss: 0.2920\n",
            "Epoch [30/100], Step [114/211], Loss: 0.2025\n",
            "Epoch [30/100], Step [115/211], Loss: 0.1660\n",
            "Epoch [30/100], Step [116/211], Loss: 0.5377\n",
            "Epoch [30/100], Step [117/211], Loss: 0.2607\n",
            "Epoch [30/100], Step [118/211], Loss: 0.3151\n",
            "Epoch [30/100], Step [119/211], Loss: 0.0944\n",
            "Epoch [30/100], Step [120/211], Loss: 0.2903\n",
            "Epoch [30/100], Step [121/211], Loss: 0.4557\n",
            "Epoch [30/100], Step [122/211], Loss: 0.3108\n",
            "Epoch [30/100], Step [123/211], Loss: 0.4707\n",
            "Epoch [30/100], Step [124/211], Loss: 0.4244\n",
            "Epoch [30/100], Step [125/211], Loss: 0.3336\n",
            "Epoch [30/100], Step [126/211], Loss: 0.2937\n",
            "Epoch [30/100], Step [127/211], Loss: 0.3555\n",
            "Epoch [30/100], Step [128/211], Loss: 0.1631\n",
            "Epoch [30/100], Step [129/211], Loss: 0.3938\n",
            "Epoch [30/100], Step [130/211], Loss: 0.2549\n",
            "Epoch [30/100], Step [131/211], Loss: 0.4572\n",
            "Epoch [30/100], Step [132/211], Loss: 0.1173\n",
            "Epoch [30/100], Step [133/211], Loss: 0.4089\n",
            "Epoch [30/100], Step [134/211], Loss: 0.2949\n",
            "Epoch [30/100], Step [135/211], Loss: 0.3794\n",
            "Epoch [30/100], Step [136/211], Loss: 0.2923\n",
            "Epoch [30/100], Step [137/211], Loss: 0.3661\n",
            "Epoch [30/100], Step [138/211], Loss: 0.3252\n",
            "Epoch [30/100], Step [139/211], Loss: 0.5072\n",
            "Epoch [30/100], Step [140/211], Loss: 0.5962\n",
            "Epoch [30/100], Step [141/211], Loss: 0.3147\n",
            "Epoch [30/100], Step [142/211], Loss: 0.1745\n",
            "Epoch [30/100], Step [143/211], Loss: 0.5337\n",
            "Epoch [30/100], Step [144/211], Loss: 0.2032\n",
            "Epoch [30/100], Step [145/211], Loss: 0.7585\n",
            "Epoch [30/100], Step [146/211], Loss: 0.3493\n",
            "Epoch [30/100], Step [147/211], Loss: 0.1357\n",
            "Epoch [30/100], Step [148/211], Loss: 0.2155\n",
            "Epoch [30/100], Step [149/211], Loss: 0.1995\n",
            "Epoch [30/100], Step [150/211], Loss: 0.2508\n",
            "Epoch [30/100], Step [151/211], Loss: 0.3662\n",
            "Epoch [30/100], Step [152/211], Loss: 0.3243\n",
            "Epoch [30/100], Step [153/211], Loss: 0.3634\n",
            "Epoch [30/100], Step [154/211], Loss: 0.2843\n",
            "Epoch [30/100], Step [155/211], Loss: 0.2725\n",
            "Epoch [30/100], Step [156/211], Loss: 0.2755\n",
            "Epoch [30/100], Step [157/211], Loss: 0.1690\n",
            "Epoch [30/100], Step [158/211], Loss: 0.1905\n",
            "Epoch [30/100], Step [159/211], Loss: 0.1510\n",
            "Epoch [30/100], Step [160/211], Loss: 0.2862\n",
            "Epoch [30/100], Step [161/211], Loss: 0.1278\n",
            "Epoch [30/100], Step [162/211], Loss: 0.5210\n",
            "Epoch [30/100], Step [163/211], Loss: 0.1682\n",
            "Epoch [30/100], Step [164/211], Loss: 0.3233\n",
            "Epoch [30/100], Step [165/211], Loss: 0.2414\n",
            "Epoch [30/100], Step [166/211], Loss: 0.5449\n",
            "Epoch [30/100], Step [167/211], Loss: 0.1906\n",
            "Epoch [30/100], Step [168/211], Loss: 0.3283\n",
            "Epoch [30/100], Step [169/211], Loss: 0.3129\n",
            "Epoch [30/100], Step [170/211], Loss: 0.2794\n",
            "Epoch [30/100], Step [171/211], Loss: 0.2188\n",
            "Epoch [30/100], Step [172/211], Loss: 0.3995\n",
            "Epoch [30/100], Step [173/211], Loss: 0.2122\n",
            "Epoch [30/100], Step [174/211], Loss: 0.1752\n",
            "Epoch [30/100], Step [175/211], Loss: 0.5280\n",
            "Epoch [30/100], Step [176/211], Loss: 0.1935\n",
            "Epoch [30/100], Step [177/211], Loss: 0.2701\n",
            "Epoch [30/100], Step [178/211], Loss: 0.1853\n",
            "Epoch [30/100], Step [179/211], Loss: 0.2337\n",
            "Epoch [30/100], Step [180/211], Loss: 0.2079\n",
            "Epoch [30/100], Step [181/211], Loss: 0.0962\n",
            "Epoch [30/100], Step [182/211], Loss: 0.3982\n",
            "Epoch [30/100], Step [183/211], Loss: 0.3305\n",
            "Epoch [30/100], Step [184/211], Loss: 0.2514\n",
            "Epoch [30/100], Step [185/211], Loss: 0.1458\n",
            "Epoch [30/100], Step [186/211], Loss: 0.3837\n",
            "Epoch [30/100], Step [187/211], Loss: 0.2721\n",
            "Epoch [30/100], Step [188/211], Loss: 0.3331\n",
            "Epoch [30/100], Step [189/211], Loss: 0.3857\n",
            "Epoch [30/100], Step [190/211], Loss: 0.2819\n",
            "Epoch [30/100], Step [191/211], Loss: 0.1806\n",
            "Epoch [30/100], Step [192/211], Loss: 0.3763\n",
            "Epoch [30/100], Step [193/211], Loss: 0.2154\n",
            "Epoch [30/100], Step [194/211], Loss: 0.4863\n",
            "Epoch [30/100], Step [195/211], Loss: 0.2172\n",
            "Epoch [30/100], Step [196/211], Loss: 0.3968\n",
            "Epoch [30/100], Step [197/211], Loss: 0.2161\n",
            "Epoch [30/100], Step [198/211], Loss: 0.2157\n",
            "Epoch [30/100], Step [199/211], Loss: 0.7134\n",
            "Epoch [30/100], Step [200/211], Loss: 0.4443\n",
            "Epoch [30/100], Step [201/211], Loss: 0.1998\n",
            "Epoch [30/100], Step [202/211], Loss: 0.2841\n",
            "Epoch [30/100], Step [203/211], Loss: 0.1607\n",
            "Epoch [30/100], Step [204/211], Loss: 0.3283\n",
            "Epoch [30/100], Step [205/211], Loss: 0.2845\n",
            "Epoch [30/100], Step [206/211], Loss: 0.3007\n",
            "Epoch [30/100], Step [207/211], Loss: 0.3505\n",
            "Epoch [30/100], Step [208/211], Loss: 0.3325\n",
            "Epoch [30/100], Step [209/211], Loss: 0.2270\n",
            "Epoch [30/100], Step [210/211], Loss: 0.3520\n",
            "Epoch [30/100], Step [211/211], Loss: 0.0686\n",
            "Loss media: 0.3045\n",
            "Epoch [31/100], Step [1/211], Loss: 0.1548\n",
            "Epoch [31/100], Step [2/211], Loss: 0.2383\n",
            "Epoch [31/100], Step [3/211], Loss: 0.3489\n",
            "Epoch [31/100], Step [4/211], Loss: 0.4978\n",
            "Epoch [31/100], Step [5/211], Loss: 0.2323\n",
            "Epoch [31/100], Step [6/211], Loss: 0.3217\n",
            "Epoch [31/100], Step [7/211], Loss: 0.3051\n",
            "Epoch [31/100], Step [8/211], Loss: 0.1936\n",
            "Epoch [31/100], Step [9/211], Loss: 0.2364\n",
            "Epoch [31/100], Step [10/211], Loss: 0.1300\n",
            "Epoch [31/100], Step [11/211], Loss: 0.2335\n",
            "Epoch [31/100], Step [12/211], Loss: 0.3651\n",
            "Epoch [31/100], Step [13/211], Loss: 0.3083\n",
            "Epoch [31/100], Step [14/211], Loss: 0.2225\n",
            "Epoch [31/100], Step [15/211], Loss: 0.1624\n",
            "Epoch [31/100], Step [16/211], Loss: 0.2180\n",
            "Epoch [31/100], Step [17/211], Loss: 0.2824\n",
            "Epoch [31/100], Step [18/211], Loss: 0.5331\n",
            "Epoch [31/100], Step [19/211], Loss: 0.2073\n",
            "Epoch [31/100], Step [20/211], Loss: 0.4716\n",
            "Epoch [31/100], Step [21/211], Loss: 0.1924\n",
            "Epoch [31/100], Step [22/211], Loss: 0.4456\n",
            "Epoch [31/100], Step [23/211], Loss: 0.3800\n",
            "Epoch [31/100], Step [24/211], Loss: 0.6356\n",
            "Epoch [31/100], Step [25/211], Loss: 0.3049\n",
            "Epoch [31/100], Step [26/211], Loss: 0.2715\n",
            "Epoch [31/100], Step [27/211], Loss: 0.3321\n",
            "Epoch [31/100], Step [28/211], Loss: 0.5726\n",
            "Epoch [31/100], Step [29/211], Loss: 0.3350\n",
            "Epoch [31/100], Step [30/211], Loss: 0.3101\n",
            "Epoch [31/100], Step [31/211], Loss: 0.3560\n",
            "Epoch [31/100], Step [32/211], Loss: 0.1673\n",
            "Epoch [31/100], Step [33/211], Loss: 0.2722\n",
            "Epoch [31/100], Step [34/211], Loss: 0.3544\n",
            "Epoch [31/100], Step [35/211], Loss: 0.5806\n",
            "Epoch [31/100], Step [36/211], Loss: 0.3136\n",
            "Epoch [31/100], Step [37/211], Loss: 0.2357\n",
            "Epoch [31/100], Step [38/211], Loss: 0.2697\n",
            "Epoch [31/100], Step [39/211], Loss: 0.4063\n",
            "Epoch [31/100], Step [40/211], Loss: 0.5141\n",
            "Epoch [31/100], Step [41/211], Loss: 0.3363\n",
            "Epoch [31/100], Step [42/211], Loss: 0.3759\n",
            "Epoch [31/100], Step [43/211], Loss: 0.2338\n",
            "Epoch [31/100], Step [44/211], Loss: 0.2008\n",
            "Epoch [31/100], Step [45/211], Loss: 0.2955\n",
            "Epoch [31/100], Step [46/211], Loss: 0.2038\n",
            "Epoch [31/100], Step [47/211], Loss: 0.1069\n",
            "Epoch [31/100], Step [48/211], Loss: 0.5736\n",
            "Epoch [31/100], Step [49/211], Loss: 0.2754\n",
            "Epoch [31/100], Step [50/211], Loss: 0.2670\n",
            "Epoch [31/100], Step [51/211], Loss: 0.3148\n",
            "Epoch [31/100], Step [52/211], Loss: 0.1795\n",
            "Epoch [31/100], Step [53/211], Loss: 0.2384\n",
            "Epoch [31/100], Step [54/211], Loss: 0.4702\n",
            "Epoch [31/100], Step [55/211], Loss: 0.3126\n",
            "Epoch [31/100], Step [56/211], Loss: 0.2361\n",
            "Epoch [31/100], Step [57/211], Loss: 0.4227\n",
            "Epoch [31/100], Step [58/211], Loss: 0.2845\n",
            "Epoch [31/100], Step [59/211], Loss: 0.2412\n",
            "Epoch [31/100], Step [60/211], Loss: 0.2170\n",
            "Epoch [31/100], Step [61/211], Loss: 0.2014\n",
            "Epoch [31/100], Step [62/211], Loss: 0.2099\n",
            "Epoch [31/100], Step [63/211], Loss: 0.2084\n",
            "Epoch [31/100], Step [64/211], Loss: 0.1823\n",
            "Epoch [31/100], Step [65/211], Loss: 0.2039\n",
            "Epoch [31/100], Step [66/211], Loss: 0.1235\n",
            "Epoch [31/100], Step [67/211], Loss: 0.1462\n",
            "Epoch [31/100], Step [68/211], Loss: 0.2576\n",
            "Epoch [31/100], Step [69/211], Loss: 0.3090\n",
            "Epoch [31/100], Step [70/211], Loss: 0.3624\n",
            "Epoch [31/100], Step [71/211], Loss: 0.3426\n",
            "Epoch [31/100], Step [72/211], Loss: 0.4359\n",
            "Epoch [31/100], Step [73/211], Loss: 0.1860\n",
            "Epoch [31/100], Step [74/211], Loss: 0.3132\n",
            "Epoch [31/100], Step [75/211], Loss: 0.5333\n",
            "Epoch [31/100], Step [76/211], Loss: 0.1904\n",
            "Epoch [31/100], Step [77/211], Loss: 0.2772\n",
            "Epoch [31/100], Step [78/211], Loss: 0.6961\n",
            "Epoch [31/100], Step [79/211], Loss: 0.3038\n",
            "Epoch [31/100], Step [80/211], Loss: 0.2662\n",
            "Epoch [31/100], Step [81/211], Loss: 0.3444\n",
            "Epoch [31/100], Step [82/211], Loss: 0.2456\n",
            "Epoch [31/100], Step [83/211], Loss: 0.2857\n",
            "Epoch [31/100], Step [84/211], Loss: 0.2818\n",
            "Epoch [31/100], Step [85/211], Loss: 0.1608\n",
            "Epoch [31/100], Step [86/211], Loss: 0.3894\n",
            "Epoch [31/100], Step [87/211], Loss: 0.2950\n",
            "Epoch [31/100], Step [88/211], Loss: 0.3801\n",
            "Epoch [31/100], Step [89/211], Loss: 0.3174\n",
            "Epoch [31/100], Step [90/211], Loss: 0.2603\n",
            "Epoch [31/100], Step [91/211], Loss: 0.3737\n",
            "Epoch [31/100], Step [92/211], Loss: 0.2798\n",
            "Epoch [31/100], Step [93/211], Loss: 0.1041\n",
            "Epoch [31/100], Step [94/211], Loss: 0.2084\n",
            "Epoch [31/100], Step [95/211], Loss: 0.1791\n",
            "Epoch [31/100], Step [96/211], Loss: 0.1248\n",
            "Epoch [31/100], Step [97/211], Loss: 0.2183\n",
            "Epoch [31/100], Step [98/211], Loss: 0.3205\n",
            "Epoch [31/100], Step [99/211], Loss: 0.2199\n",
            "Epoch [31/100], Step [100/211], Loss: 0.3187\n",
            "Epoch [31/100], Step [101/211], Loss: 0.4619\n",
            "Epoch [31/100], Step [102/211], Loss: 0.6623\n",
            "Epoch [31/100], Step [103/211], Loss: 0.1429\n",
            "Epoch [31/100], Step [104/211], Loss: 0.2702\n",
            "Epoch [31/100], Step [105/211], Loss: 0.4124\n",
            "Epoch [31/100], Step [106/211], Loss: 0.5401\n",
            "Epoch [31/100], Step [107/211], Loss: 0.1534\n",
            "Epoch [31/100], Step [108/211], Loss: 0.2377\n",
            "Epoch [31/100], Step [109/211], Loss: 0.1241\n",
            "Epoch [31/100], Step [110/211], Loss: 0.2280\n",
            "Epoch [31/100], Step [111/211], Loss: 0.2574\n",
            "Epoch [31/100], Step [112/211], Loss: 0.1761\n",
            "Epoch [31/100], Step [113/211], Loss: 0.4322\n",
            "Epoch [31/100], Step [114/211], Loss: 0.1556\n",
            "Epoch [31/100], Step [115/211], Loss: 0.3603\n",
            "Epoch [31/100], Step [116/211], Loss: 0.5630\n",
            "Epoch [31/100], Step [117/211], Loss: 0.3125\n",
            "Epoch [31/100], Step [118/211], Loss: 0.0855\n",
            "Epoch [31/100], Step [119/211], Loss: 0.2121\n",
            "Epoch [31/100], Step [120/211], Loss: 0.2403\n",
            "Epoch [31/100], Step [121/211], Loss: 0.1585\n",
            "Epoch [31/100], Step [122/211], Loss: 0.3590\n",
            "Epoch [31/100], Step [123/211], Loss: 0.3424\n",
            "Epoch [31/100], Step [124/211], Loss: 0.5886\n",
            "Epoch [31/100], Step [125/211], Loss: 0.2283\n",
            "Epoch [31/100], Step [126/211], Loss: 0.1362\n",
            "Epoch [31/100], Step [127/211], Loss: 0.1738\n",
            "Epoch [31/100], Step [128/211], Loss: 0.6258\n",
            "Epoch [31/100], Step [129/211], Loss: 0.2449\n",
            "Epoch [31/100], Step [130/211], Loss: 0.1827\n",
            "Epoch [31/100], Step [131/211], Loss: 0.1905\n",
            "Epoch [31/100], Step [132/211], Loss: 0.3940\n",
            "Epoch [31/100], Step [133/211], Loss: 0.6264\n",
            "Epoch [31/100], Step [134/211], Loss: 0.2757\n",
            "Epoch [31/100], Step [135/211], Loss: 0.3112\n",
            "Epoch [31/100], Step [136/211], Loss: 0.1693\n",
            "Epoch [31/100], Step [137/211], Loss: 0.1587\n",
            "Epoch [31/100], Step [138/211], Loss: 0.2130\n",
            "Epoch [31/100], Step [139/211], Loss: 0.1247\n",
            "Epoch [31/100], Step [140/211], Loss: 0.2266\n",
            "Epoch [31/100], Step [141/211], Loss: 0.2604\n",
            "Epoch [31/100], Step [142/211], Loss: 0.1469\n",
            "Epoch [31/100], Step [143/211], Loss: 0.2499\n",
            "Epoch [31/100], Step [144/211], Loss: 0.4160\n",
            "Epoch [31/100], Step [145/211], Loss: 0.2070\n",
            "Epoch [31/100], Step [146/211], Loss: 0.1625\n",
            "Epoch [31/100], Step [147/211], Loss: 0.3513\n",
            "Epoch [31/100], Step [148/211], Loss: 0.2069\n",
            "Epoch [31/100], Step [149/211], Loss: 0.1793\n",
            "Epoch [31/100], Step [150/211], Loss: 0.4943\n",
            "Epoch [31/100], Step [151/211], Loss: 0.2612\n",
            "Epoch [31/100], Step [152/211], Loss: 0.3827\n",
            "Epoch [31/100], Step [153/211], Loss: 0.4226\n",
            "Epoch [31/100], Step [154/211], Loss: 0.4581\n",
            "Epoch [31/100], Step [155/211], Loss: 0.1925\n",
            "Epoch [31/100], Step [156/211], Loss: 0.2832\n",
            "Epoch [31/100], Step [157/211], Loss: 0.3820\n",
            "Epoch [31/100], Step [158/211], Loss: 0.3996\n",
            "Epoch [31/100], Step [159/211], Loss: 0.3784\n",
            "Epoch [31/100], Step [160/211], Loss: 0.2646\n",
            "Epoch [31/100], Step [161/211], Loss: 0.3056\n",
            "Epoch [31/100], Step [162/211], Loss: 0.2719\n",
            "Epoch [31/100], Step [163/211], Loss: 0.4182\n",
            "Epoch [31/100], Step [164/211], Loss: 0.1913\n",
            "Epoch [31/100], Step [165/211], Loss: 0.2255\n",
            "Epoch [31/100], Step [166/211], Loss: 0.2587\n",
            "Epoch [31/100], Step [167/211], Loss: 0.3409\n",
            "Epoch [31/100], Step [168/211], Loss: 0.3507\n",
            "Epoch [31/100], Step [169/211], Loss: 0.1835\n",
            "Epoch [31/100], Step [170/211], Loss: 0.2100\n",
            "Epoch [31/100], Step [171/211], Loss: 0.3527\n",
            "Epoch [31/100], Step [172/211], Loss: 0.5844\n",
            "Epoch [31/100], Step [173/211], Loss: 0.1012\n",
            "Epoch [31/100], Step [174/211], Loss: 0.5478\n",
            "Epoch [31/100], Step [175/211], Loss: 0.2252\n",
            "Epoch [31/100], Step [176/211], Loss: 0.5108\n",
            "Epoch [31/100], Step [177/211], Loss: 0.2564\n",
            "Epoch [31/100], Step [178/211], Loss: 0.1690\n",
            "Epoch [31/100], Step [179/211], Loss: 0.3409\n",
            "Epoch [31/100], Step [180/211], Loss: 0.4961\n",
            "Epoch [31/100], Step [181/211], Loss: 0.1673\n",
            "Epoch [31/100], Step [182/211], Loss: 0.2349\n",
            "Epoch [31/100], Step [183/211], Loss: 0.1726\n",
            "Epoch [31/100], Step [184/211], Loss: 0.2301\n",
            "Epoch [31/100], Step [185/211], Loss: 0.2884\n",
            "Epoch [31/100], Step [186/211], Loss: 0.4725\n",
            "Epoch [31/100], Step [187/211], Loss: 0.3183\n",
            "Epoch [31/100], Step [188/211], Loss: 0.1711\n",
            "Epoch [31/100], Step [189/211], Loss: 0.2924\n",
            "Epoch [31/100], Step [190/211], Loss: 0.2838\n",
            "Epoch [31/100], Step [191/211], Loss: 0.1876\n",
            "Epoch [31/100], Step [192/211], Loss: 0.2006\n",
            "Epoch [31/100], Step [193/211], Loss: 0.3702\n",
            "Epoch [31/100], Step [194/211], Loss: 0.2817\n",
            "Epoch [31/100], Step [195/211], Loss: 0.2906\n",
            "Epoch [31/100], Step [196/211], Loss: 0.3991\n",
            "Epoch [31/100], Step [197/211], Loss: 0.3470\n",
            "Epoch [31/100], Step [198/211], Loss: 0.3786\n",
            "Epoch [31/100], Step [199/211], Loss: 0.3324\n",
            "Epoch [31/100], Step [200/211], Loss: 0.5925\n",
            "Epoch [31/100], Step [201/211], Loss: 0.3571\n",
            "Epoch [31/100], Step [202/211], Loss: 0.6486\n",
            "Epoch [31/100], Step [203/211], Loss: 0.4368\n",
            "Epoch [31/100], Step [204/211], Loss: 0.2730\n",
            "Epoch [31/100], Step [205/211], Loss: 0.4075\n",
            "Epoch [31/100], Step [206/211], Loss: 0.2451\n",
            "Epoch [31/100], Step [207/211], Loss: 0.1831\n",
            "Epoch [31/100], Step [208/211], Loss: 0.2688\n",
            "Epoch [31/100], Step [209/211], Loss: 0.3444\n",
            "Epoch [31/100], Step [210/211], Loss: 0.2982\n",
            "Epoch [31/100], Step [211/211], Loss: 0.2416\n",
            "Loss media: 0.3018\n",
            "Epoch [32/100], Step [1/211], Loss: 0.1523\n",
            "Epoch [32/100], Step [2/211], Loss: 0.3927\n",
            "Epoch [32/100], Step [3/211], Loss: 0.3091\n",
            "Epoch [32/100], Step [4/211], Loss: 0.4906\n",
            "Epoch [32/100], Step [5/211], Loss: 0.1341\n",
            "Epoch [32/100], Step [6/211], Loss: 0.3082\n",
            "Epoch [32/100], Step [7/211], Loss: 0.4090\n",
            "Epoch [32/100], Step [8/211], Loss: 0.1904\n",
            "Epoch [32/100], Step [9/211], Loss: 0.4781\n",
            "Epoch [32/100], Step [10/211], Loss: 0.2372\n",
            "Epoch [32/100], Step [11/211], Loss: 0.1881\n",
            "Epoch [32/100], Step [12/211], Loss: 0.3098\n",
            "Epoch [32/100], Step [13/211], Loss: 0.4511\n",
            "Epoch [32/100], Step [14/211], Loss: 0.1371\n",
            "Epoch [32/100], Step [15/211], Loss: 0.2793\n",
            "Epoch [32/100], Step [16/211], Loss: 0.2677\n",
            "Epoch [32/100], Step [17/211], Loss: 0.2876\n",
            "Epoch [32/100], Step [18/211], Loss: 0.2948\n",
            "Epoch [32/100], Step [19/211], Loss: 0.1502\n",
            "Epoch [32/100], Step [20/211], Loss: 0.2901\n",
            "Epoch [32/100], Step [21/211], Loss: 0.3277\n",
            "Epoch [32/100], Step [22/211], Loss: 0.4951\n",
            "Epoch [32/100], Step [23/211], Loss: 0.1453\n",
            "Epoch [32/100], Step [24/211], Loss: 0.2866\n",
            "Epoch [32/100], Step [25/211], Loss: 0.4929\n",
            "Epoch [32/100], Step [26/211], Loss: 0.2353\n",
            "Epoch [32/100], Step [27/211], Loss: 0.2277\n",
            "Epoch [32/100], Step [28/211], Loss: 0.2642\n",
            "Epoch [32/100], Step [29/211], Loss: 0.4470\n",
            "Epoch [32/100], Step [30/211], Loss: 0.5176\n",
            "Epoch [32/100], Step [31/211], Loss: 0.4638\n",
            "Epoch [32/100], Step [32/211], Loss: 0.2278\n",
            "Epoch [32/100], Step [33/211], Loss: 0.6848\n",
            "Epoch [32/100], Step [34/211], Loss: 0.2471\n",
            "Epoch [32/100], Step [35/211], Loss: 0.2299\n",
            "Epoch [32/100], Step [36/211], Loss: 0.1994\n",
            "Epoch [32/100], Step [37/211], Loss: 0.4242\n",
            "Epoch [32/100], Step [38/211], Loss: 0.3756\n",
            "Epoch [32/100], Step [39/211], Loss: 0.1781\n",
            "Epoch [32/100], Step [40/211], Loss: 0.1872\n",
            "Epoch [32/100], Step [41/211], Loss: 0.2672\n",
            "Epoch [32/100], Step [42/211], Loss: 0.1683\n",
            "Epoch [32/100], Step [43/211], Loss: 0.1963\n",
            "Epoch [32/100], Step [44/211], Loss: 0.2008\n",
            "Epoch [32/100], Step [45/211], Loss: 0.0780\n",
            "Epoch [32/100], Step [46/211], Loss: 0.2234\n",
            "Epoch [32/100], Step [47/211], Loss: 0.4467\n",
            "Epoch [32/100], Step [48/211], Loss: 0.3981\n",
            "Epoch [32/100], Step [49/211], Loss: 0.1719\n",
            "Epoch [32/100], Step [50/211], Loss: 0.3307\n",
            "Epoch [32/100], Step [51/211], Loss: 0.4115\n",
            "Epoch [32/100], Step [52/211], Loss: 0.3602\n",
            "Epoch [32/100], Step [53/211], Loss: 0.2093\n",
            "Epoch [32/100], Step [54/211], Loss: 0.2429\n",
            "Epoch [32/100], Step [55/211], Loss: 0.1272\n",
            "Epoch [32/100], Step [56/211], Loss: 0.6538\n",
            "Epoch [32/100], Step [57/211], Loss: 0.4286\n",
            "Epoch [32/100], Step [58/211], Loss: 0.1783\n",
            "Epoch [32/100], Step [59/211], Loss: 0.1873\n",
            "Epoch [32/100], Step [60/211], Loss: 0.2086\n",
            "Epoch [32/100], Step [61/211], Loss: 0.1555\n",
            "Epoch [32/100], Step [62/211], Loss: 0.1438\n",
            "Epoch [32/100], Step [63/211], Loss: 0.2287\n",
            "Epoch [32/100], Step [64/211], Loss: 0.2255\n",
            "Epoch [32/100], Step [65/211], Loss: 0.1029\n",
            "Epoch [32/100], Step [66/211], Loss: 0.1688\n",
            "Epoch [32/100], Step [67/211], Loss: 0.2524\n",
            "Epoch [32/100], Step [68/211], Loss: 0.3801\n",
            "Epoch [32/100], Step [69/211], Loss: 0.1287\n",
            "Epoch [32/100], Step [70/211], Loss: 0.2055\n",
            "Epoch [32/100], Step [71/211], Loss: 0.3414\n",
            "Epoch [32/100], Step [72/211], Loss: 0.1748\n",
            "Epoch [32/100], Step [73/211], Loss: 0.3173\n",
            "Epoch [32/100], Step [74/211], Loss: 0.2604\n",
            "Epoch [32/100], Step [75/211], Loss: 0.3336\n",
            "Epoch [32/100], Step [76/211], Loss: 0.2513\n",
            "Epoch [32/100], Step [77/211], Loss: 0.1987\n",
            "Epoch [32/100], Step [78/211], Loss: 0.2139\n",
            "Epoch [32/100], Step [79/211], Loss: 0.5221\n",
            "Epoch [32/100], Step [80/211], Loss: 0.2015\n",
            "Epoch [32/100], Step [81/211], Loss: 0.3946\n",
            "Epoch [32/100], Step [82/211], Loss: 0.4342\n",
            "Epoch [32/100], Step [83/211], Loss: 0.3530\n",
            "Epoch [32/100], Step [84/211], Loss: 0.1463\n",
            "Epoch [32/100], Step [85/211], Loss: 0.3489\n",
            "Epoch [32/100], Step [86/211], Loss: 0.2749\n",
            "Epoch [32/100], Step [87/211], Loss: 0.2032\n",
            "Epoch [32/100], Step [88/211], Loss: 0.4737\n",
            "Epoch [32/100], Step [89/211], Loss: 0.3253\n",
            "Epoch [32/100], Step [90/211], Loss: 0.2012\n",
            "Epoch [32/100], Step [91/211], Loss: 0.1323\n",
            "Epoch [32/100], Step [92/211], Loss: 0.2451\n",
            "Epoch [32/100], Step [93/211], Loss: 0.2705\n",
            "Epoch [32/100], Step [94/211], Loss: 0.4455\n",
            "Epoch [32/100], Step [95/211], Loss: 0.3576\n",
            "Epoch [32/100], Step [96/211], Loss: 0.2976\n",
            "Epoch [32/100], Step [97/211], Loss: 0.3484\n",
            "Epoch [32/100], Step [98/211], Loss: 0.4943\n",
            "Epoch [32/100], Step [99/211], Loss: 0.3322\n",
            "Epoch [32/100], Step [100/211], Loss: 0.2339\n",
            "Epoch [32/100], Step [101/211], Loss: 0.4841\n",
            "Epoch [32/100], Step [102/211], Loss: 0.1786\n",
            "Epoch [32/100], Step [103/211], Loss: 0.6370\n",
            "Epoch [32/100], Step [104/211], Loss: 0.4248\n",
            "Epoch [32/100], Step [105/211], Loss: 0.2376\n",
            "Epoch [32/100], Step [106/211], Loss: 0.2911\n",
            "Epoch [32/100], Step [107/211], Loss: 0.2015\n",
            "Epoch [32/100], Step [108/211], Loss: 0.4815\n",
            "Epoch [32/100], Step [109/211], Loss: 0.4801\n",
            "Epoch [32/100], Step [110/211], Loss: 0.2949\n",
            "Epoch [32/100], Step [111/211], Loss: 0.1426\n",
            "Epoch [32/100], Step [112/211], Loss: 0.2560\n",
            "Epoch [32/100], Step [113/211], Loss: 0.2248\n",
            "Epoch [32/100], Step [114/211], Loss: 0.4425\n",
            "Epoch [32/100], Step [115/211], Loss: 0.2224\n",
            "Epoch [32/100], Step [116/211], Loss: 0.2939\n",
            "Epoch [32/100], Step [117/211], Loss: 0.2433\n",
            "Epoch [32/100], Step [118/211], Loss: 0.3306\n",
            "Epoch [32/100], Step [119/211], Loss: 0.2297\n",
            "Epoch [32/100], Step [120/211], Loss: 0.3601\n",
            "Epoch [32/100], Step [121/211], Loss: 0.3413\n",
            "Epoch [32/100], Step [122/211], Loss: 0.1488\n",
            "Epoch [32/100], Step [123/211], Loss: 0.2404\n",
            "Epoch [32/100], Step [124/211], Loss: 0.0815\n",
            "Epoch [32/100], Step [125/211], Loss: 0.0925\n",
            "Epoch [32/100], Step [126/211], Loss: 0.2033\n",
            "Epoch [32/100], Step [127/211], Loss: 0.5648\n",
            "Epoch [32/100], Step [128/211], Loss: 0.2369\n",
            "Epoch [32/100], Step [129/211], Loss: 0.3274\n",
            "Epoch [32/100], Step [130/211], Loss: 0.2062\n",
            "Epoch [32/100], Step [131/211], Loss: 0.2735\n",
            "Epoch [32/100], Step [132/211], Loss: 0.2967\n",
            "Epoch [32/100], Step [133/211], Loss: 0.2190\n",
            "Epoch [32/100], Step [134/211], Loss: 0.4104\n",
            "Epoch [32/100], Step [135/211], Loss: 0.2860\n",
            "Epoch [32/100], Step [136/211], Loss: 0.2664\n",
            "Epoch [32/100], Step [137/211], Loss: 0.2770\n",
            "Epoch [32/100], Step [138/211], Loss: 0.4264\n",
            "Epoch [32/100], Step [139/211], Loss: 0.3282\n",
            "Epoch [32/100], Step [140/211], Loss: 0.3621\n",
            "Epoch [32/100], Step [141/211], Loss: 0.2295\n",
            "Epoch [32/100], Step [142/211], Loss: 0.4494\n",
            "Epoch [32/100], Step [143/211], Loss: 0.2698\n",
            "Epoch [32/100], Step [144/211], Loss: 0.3330\n",
            "Epoch [32/100], Step [145/211], Loss: 0.2202\n",
            "Epoch [32/100], Step [146/211], Loss: 0.2183\n",
            "Epoch [32/100], Step [147/211], Loss: 0.3628\n",
            "Epoch [32/100], Step [148/211], Loss: 0.2264\n",
            "Epoch [32/100], Step [149/211], Loss: 0.1433\n",
            "Epoch [32/100], Step [150/211], Loss: 0.2495\n",
            "Epoch [32/100], Step [151/211], Loss: 0.6999\n",
            "Epoch [32/100], Step [152/211], Loss: 0.4881\n",
            "Epoch [32/100], Step [153/211], Loss: 0.1274\n",
            "Epoch [32/100], Step [154/211], Loss: 0.2849\n",
            "Epoch [32/100], Step [155/211], Loss: 0.1134\n",
            "Epoch [32/100], Step [156/211], Loss: 0.2187\n",
            "Epoch [32/100], Step [157/211], Loss: 0.2711\n",
            "Epoch [32/100], Step [158/211], Loss: 0.3048\n",
            "Epoch [32/100], Step [159/211], Loss: 0.2555\n",
            "Epoch [32/100], Step [160/211], Loss: 0.2597\n",
            "Epoch [32/100], Step [161/211], Loss: 0.1775\n",
            "Epoch [32/100], Step [162/211], Loss: 0.2920\n",
            "Epoch [32/100], Step [163/211], Loss: 0.4091\n",
            "Epoch [32/100], Step [164/211], Loss: 0.5429\n",
            "Epoch [32/100], Step [165/211], Loss: 0.3167\n",
            "Epoch [32/100], Step [166/211], Loss: 0.1498\n",
            "Epoch [32/100], Step [167/211], Loss: 0.5109\n",
            "Epoch [32/100], Step [168/211], Loss: 0.2603\n",
            "Epoch [32/100], Step [169/211], Loss: 0.3245\n",
            "Epoch [32/100], Step [170/211], Loss: 0.2850\n",
            "Epoch [32/100], Step [171/211], Loss: 0.3679\n",
            "Epoch [32/100], Step [172/211], Loss: 0.2782\n",
            "Epoch [32/100], Step [173/211], Loss: 0.2203\n",
            "Epoch [32/100], Step [174/211], Loss: 0.2755\n",
            "Epoch [32/100], Step [175/211], Loss: 0.3330\n",
            "Epoch [32/100], Step [176/211], Loss: 0.1107\n",
            "Epoch [32/100], Step [177/211], Loss: 0.2374\n",
            "Epoch [32/100], Step [178/211], Loss: 0.3518\n",
            "Epoch [32/100], Step [179/211], Loss: 0.4019\n",
            "Epoch [32/100], Step [180/211], Loss: 0.4771\n",
            "Epoch [32/100], Step [181/211], Loss: 0.4715\n",
            "Epoch [32/100], Step [182/211], Loss: 0.5767\n",
            "Epoch [32/100], Step [183/211], Loss: 0.6164\n",
            "Epoch [32/100], Step [184/211], Loss: 0.2455\n",
            "Epoch [32/100], Step [185/211], Loss: 0.3704\n",
            "Epoch [32/100], Step [186/211], Loss: 0.2374\n",
            "Epoch [32/100], Step [187/211], Loss: 0.1575\n",
            "Epoch [32/100], Step [188/211], Loss: 0.3803\n",
            "Epoch [32/100], Step [189/211], Loss: 0.2318\n",
            "Epoch [32/100], Step [190/211], Loss: 0.1838\n",
            "Epoch [32/100], Step [191/211], Loss: 0.2444\n",
            "Epoch [32/100], Step [192/211], Loss: 0.4691\n",
            "Epoch [32/100], Step [193/211], Loss: 0.3572\n",
            "Epoch [32/100], Step [194/211], Loss: 0.3185\n",
            "Epoch [32/100], Step [195/211], Loss: 0.1677\n",
            "Epoch [32/100], Step [196/211], Loss: 0.3204\n",
            "Epoch [32/100], Step [197/211], Loss: 0.2305\n",
            "Epoch [32/100], Step [198/211], Loss: 0.2212\n",
            "Epoch [32/100], Step [199/211], Loss: 0.1963\n",
            "Epoch [32/100], Step [200/211], Loss: 0.1655\n",
            "Epoch [32/100], Step [201/211], Loss: 0.3041\n",
            "Epoch [32/100], Step [202/211], Loss: 0.6134\n",
            "Epoch [32/100], Step [203/211], Loss: 0.6083\n",
            "Epoch [32/100], Step [204/211], Loss: 0.2201\n",
            "Epoch [32/100], Step [205/211], Loss: 0.3417\n",
            "Epoch [32/100], Step [206/211], Loss: 0.2215\n",
            "Epoch [32/100], Step [207/211], Loss: 0.3674\n",
            "Epoch [32/100], Step [208/211], Loss: 0.2319\n",
            "Epoch [32/100], Step [209/211], Loss: 0.4369\n",
            "Epoch [32/100], Step [210/211], Loss: 0.4146\n",
            "Epoch [32/100], Step [211/211], Loss: 0.4190\n",
            "Loss media: 0.3007\n",
            "Epoch [33/100], Step [1/211], Loss: 0.1606\n",
            "Epoch [33/100], Step [2/211], Loss: 0.3239\n",
            "Epoch [33/100], Step [3/211], Loss: 0.2665\n",
            "Epoch [33/100], Step [4/211], Loss: 0.3662\n",
            "Epoch [33/100], Step [5/211], Loss: 0.1720\n",
            "Epoch [33/100], Step [6/211], Loss: 0.3828\n",
            "Epoch [33/100], Step [7/211], Loss: 0.3253\n",
            "Epoch [33/100], Step [8/211], Loss: 0.2468\n",
            "Epoch [33/100], Step [9/211], Loss: 0.1877\n",
            "Epoch [33/100], Step [10/211], Loss: 0.2247\n",
            "Epoch [33/100], Step [11/211], Loss: 0.2113\n",
            "Epoch [33/100], Step [12/211], Loss: 0.2006\n",
            "Epoch [33/100], Step [13/211], Loss: 0.2271\n",
            "Epoch [33/100], Step [14/211], Loss: 0.2996\n",
            "Epoch [33/100], Step [15/211], Loss: 0.2457\n",
            "Epoch [33/100], Step [16/211], Loss: 0.3544\n",
            "Epoch [33/100], Step [17/211], Loss: 0.4344\n",
            "Epoch [33/100], Step [18/211], Loss: 0.1556\n",
            "Epoch [33/100], Step [19/211], Loss: 0.3587\n",
            "Epoch [33/100], Step [20/211], Loss: 0.3097\n",
            "Epoch [33/100], Step [21/211], Loss: 0.3565\n",
            "Epoch [33/100], Step [22/211], Loss: 0.2839\n",
            "Epoch [33/100], Step [23/211], Loss: 0.2909\n",
            "Epoch [33/100], Step [24/211], Loss: 0.4289\n",
            "Epoch [33/100], Step [25/211], Loss: 0.5883\n",
            "Epoch [33/100], Step [26/211], Loss: 0.3596\n",
            "Epoch [33/100], Step [27/211], Loss: 0.1873\n",
            "Epoch [33/100], Step [28/211], Loss: 0.4463\n",
            "Epoch [33/100], Step [29/211], Loss: 0.2004\n",
            "Epoch [33/100], Step [30/211], Loss: 0.1339\n",
            "Epoch [33/100], Step [31/211], Loss: 0.3068\n",
            "Epoch [33/100], Step [32/211], Loss: 0.2734\n",
            "Epoch [33/100], Step [33/211], Loss: 0.3282\n",
            "Epoch [33/100], Step [34/211], Loss: 0.2712\n",
            "Epoch [33/100], Step [35/211], Loss: 0.2788\n",
            "Epoch [33/100], Step [36/211], Loss: 0.2699\n",
            "Epoch [33/100], Step [37/211], Loss: 0.3795\n",
            "Epoch [33/100], Step [38/211], Loss: 0.2457\n",
            "Epoch [33/100], Step [39/211], Loss: 0.3126\n",
            "Epoch [33/100], Step [40/211], Loss: 0.2417\n",
            "Epoch [33/100], Step [41/211], Loss: 0.3424\n",
            "Epoch [33/100], Step [42/211], Loss: 0.3789\n",
            "Epoch [33/100], Step [43/211], Loss: 0.3125\n",
            "Epoch [33/100], Step [44/211], Loss: 0.2934\n",
            "Epoch [33/100], Step [45/211], Loss: 0.1409\n",
            "Epoch [33/100], Step [46/211], Loss: 0.3319\n",
            "Epoch [33/100], Step [47/211], Loss: 0.2235\n",
            "Epoch [33/100], Step [48/211], Loss: 0.2163\n",
            "Epoch [33/100], Step [49/211], Loss: 0.1194\n",
            "Epoch [33/100], Step [50/211], Loss: 0.2420\n",
            "Epoch [33/100], Step [51/211], Loss: 0.1534\n",
            "Epoch [33/100], Step [52/211], Loss: 0.1503\n",
            "Epoch [33/100], Step [53/211], Loss: 0.3772\n",
            "Epoch [33/100], Step [54/211], Loss: 0.1471\n",
            "Epoch [33/100], Step [55/211], Loss: 0.2062\n",
            "Epoch [33/100], Step [56/211], Loss: 0.3609\n",
            "Epoch [33/100], Step [57/211], Loss: 0.2877\n",
            "Epoch [33/100], Step [58/211], Loss: 0.1063\n",
            "Epoch [33/100], Step [59/211], Loss: 0.3799\n",
            "Epoch [33/100], Step [60/211], Loss: 0.2038\n",
            "Epoch [33/100], Step [61/211], Loss: 0.1468\n",
            "Epoch [33/100], Step [62/211], Loss: 0.2057\n",
            "Epoch [33/100], Step [63/211], Loss: 0.3038\n",
            "Epoch [33/100], Step [64/211], Loss: 0.4336\n",
            "Epoch [33/100], Step [65/211], Loss: 0.3229\n",
            "Epoch [33/100], Step [66/211], Loss: 0.2852\n",
            "Epoch [33/100], Step [67/211], Loss: 0.3198\n",
            "Epoch [33/100], Step [68/211], Loss: 0.5216\n",
            "Epoch [33/100], Step [69/211], Loss: 0.4223\n",
            "Epoch [33/100], Step [70/211], Loss: 0.4035\n",
            "Epoch [33/100], Step [71/211], Loss: 0.2207\n",
            "Epoch [33/100], Step [72/211], Loss: 0.2160\n",
            "Epoch [33/100], Step [73/211], Loss: 0.3861\n",
            "Epoch [33/100], Step [74/211], Loss: 0.1816\n",
            "Epoch [33/100], Step [75/211], Loss: 0.2314\n",
            "Epoch [33/100], Step [76/211], Loss: 0.2346\n",
            "Epoch [33/100], Step [77/211], Loss: 0.2604\n",
            "Epoch [33/100], Step [78/211], Loss: 0.3875\n",
            "Epoch [33/100], Step [79/211], Loss: 0.4465\n",
            "Epoch [33/100], Step [80/211], Loss: 0.6807\n",
            "Epoch [33/100], Step [81/211], Loss: 0.2737\n",
            "Epoch [33/100], Step [82/211], Loss: 0.5217\n",
            "Epoch [33/100], Step [83/211], Loss: 0.2468\n",
            "Epoch [33/100], Step [84/211], Loss: 0.5615\n",
            "Epoch [33/100], Step [85/211], Loss: 0.4997\n",
            "Epoch [33/100], Step [86/211], Loss: 0.2005\n",
            "Epoch [33/100], Step [87/211], Loss: 0.3139\n",
            "Epoch [33/100], Step [88/211], Loss: 0.3171\n",
            "Epoch [33/100], Step [89/211], Loss: 0.1744\n",
            "Epoch [33/100], Step [90/211], Loss: 0.4307\n",
            "Epoch [33/100], Step [91/211], Loss: 0.2788\n",
            "Epoch [33/100], Step [92/211], Loss: 0.3414\n",
            "Epoch [33/100], Step [93/211], Loss: 0.2040\n",
            "Epoch [33/100], Step [94/211], Loss: 0.1862\n",
            "Epoch [33/100], Step [95/211], Loss: 0.2121\n",
            "Epoch [33/100], Step [96/211], Loss: 0.2652\n",
            "Epoch [33/100], Step [97/211], Loss: 0.1483\n",
            "Epoch [33/100], Step [98/211], Loss: 0.3471\n",
            "Epoch [33/100], Step [99/211], Loss: 0.4330\n",
            "Epoch [33/100], Step [100/211], Loss: 0.1928\n",
            "Epoch [33/100], Step [101/211], Loss: 0.2415\n",
            "Epoch [33/100], Step [102/211], Loss: 0.3876\n",
            "Epoch [33/100], Step [103/211], Loss: 0.3852\n",
            "Epoch [33/100], Step [104/211], Loss: 0.2270\n",
            "Epoch [33/100], Step [105/211], Loss: 0.3581\n",
            "Epoch [33/100], Step [106/211], Loss: 0.2781\n",
            "Epoch [33/100], Step [107/211], Loss: 0.2247\n",
            "Epoch [33/100], Step [108/211], Loss: 0.1137\n",
            "Epoch [33/100], Step [109/211], Loss: 0.1076\n",
            "Epoch [33/100], Step [110/211], Loss: 0.2366\n",
            "Epoch [33/100], Step [111/211], Loss: 0.2843\n",
            "Epoch [33/100], Step [112/211], Loss: 0.2316\n",
            "Epoch [33/100], Step [113/211], Loss: 0.1500\n",
            "Epoch [33/100], Step [114/211], Loss: 0.1400\n",
            "Epoch [33/100], Step [115/211], Loss: 0.2273\n",
            "Epoch [33/100], Step [116/211], Loss: 0.2600\n",
            "Epoch [33/100], Step [117/211], Loss: 0.1526\n",
            "Epoch [33/100], Step [118/211], Loss: 0.2225\n",
            "Epoch [33/100], Step [119/211], Loss: 0.4141\n",
            "Epoch [33/100], Step [120/211], Loss: 0.1072\n",
            "Epoch [33/100], Step [121/211], Loss: 0.4412\n",
            "Epoch [33/100], Step [122/211], Loss: 0.2171\n",
            "Epoch [33/100], Step [123/211], Loss: 0.2252\n",
            "Epoch [33/100], Step [124/211], Loss: 0.1335\n",
            "Epoch [33/100], Step [125/211], Loss: 0.1171\n",
            "Epoch [33/100], Step [126/211], Loss: 0.3870\n",
            "Epoch [33/100], Step [127/211], Loss: 0.3573\n",
            "Epoch [33/100], Step [128/211], Loss: 0.2103\n",
            "Epoch [33/100], Step [129/211], Loss: 0.2119\n",
            "Epoch [33/100], Step [130/211], Loss: 0.3517\n",
            "Epoch [33/100], Step [131/211], Loss: 0.2563\n",
            "Epoch [33/100], Step [132/211], Loss: 0.3389\n",
            "Epoch [33/100], Step [133/211], Loss: 0.3169\n",
            "Epoch [33/100], Step [134/211], Loss: 0.3490\n",
            "Epoch [33/100], Step [135/211], Loss: 0.2756\n",
            "Epoch [33/100], Step [136/211], Loss: 0.2771\n",
            "Epoch [33/100], Step [137/211], Loss: 0.3276\n",
            "Epoch [33/100], Step [138/211], Loss: 0.2613\n",
            "Epoch [33/100], Step [139/211], Loss: 0.2173\n",
            "Epoch [33/100], Step [140/211], Loss: 0.4509\n",
            "Epoch [33/100], Step [141/211], Loss: 0.4315\n",
            "Epoch [33/100], Step [142/211], Loss: 0.3332\n",
            "Epoch [33/100], Step [143/211], Loss: 0.2600\n",
            "Epoch [33/100], Step [144/211], Loss: 0.2861\n",
            "Epoch [33/100], Step [145/211], Loss: 0.3275\n",
            "Epoch [33/100], Step [146/211], Loss: 0.3804\n",
            "Epoch [33/100], Step [147/211], Loss: 0.1457\n",
            "Epoch [33/100], Step [148/211], Loss: 0.3429\n",
            "Epoch [33/100], Step [149/211], Loss: 0.1673\n",
            "Epoch [33/100], Step [150/211], Loss: 0.3821\n",
            "Epoch [33/100], Step [151/211], Loss: 0.3890\n",
            "Epoch [33/100], Step [152/211], Loss: 0.3383\n",
            "Epoch [33/100], Step [153/211], Loss: 0.3660\n",
            "Epoch [33/100], Step [154/211], Loss: 0.1846\n",
            "Epoch [33/100], Step [155/211], Loss: 0.2556\n",
            "Epoch [33/100], Step [156/211], Loss: 0.1295\n",
            "Epoch [33/100], Step [157/211], Loss: 0.3298\n",
            "Epoch [33/100], Step [158/211], Loss: 0.3847\n",
            "Epoch [33/100], Step [159/211], Loss: 0.4721\n",
            "Epoch [33/100], Step [160/211], Loss: 0.4285\n",
            "Epoch [33/100], Step [161/211], Loss: 0.6334\n",
            "Epoch [33/100], Step [162/211], Loss: 0.1445\n",
            "Epoch [33/100], Step [163/211], Loss: 0.1390\n",
            "Epoch [33/100], Step [164/211], Loss: 0.4159\n",
            "Epoch [33/100], Step [165/211], Loss: 0.1939\n",
            "Epoch [33/100], Step [166/211], Loss: 0.2403\n",
            "Epoch [33/100], Step [167/211], Loss: 0.3272\n",
            "Epoch [33/100], Step [168/211], Loss: 0.2088\n",
            "Epoch [33/100], Step [169/211], Loss: 0.3818\n",
            "Epoch [33/100], Step [170/211], Loss: 0.4191\n",
            "Epoch [33/100], Step [171/211], Loss: 0.3577\n",
            "Epoch [33/100], Step [172/211], Loss: 0.4354\n",
            "Epoch [33/100], Step [173/211], Loss: 0.3542\n",
            "Epoch [33/100], Step [174/211], Loss: 0.4185\n",
            "Epoch [33/100], Step [175/211], Loss: 0.2975\n",
            "Epoch [33/100], Step [176/211], Loss: 0.1169\n",
            "Epoch [33/100], Step [177/211], Loss: 0.2221\n",
            "Epoch [33/100], Step [178/211], Loss: 0.4435\n",
            "Epoch [33/100], Step [179/211], Loss: 0.2381\n",
            "Epoch [33/100], Step [180/211], Loss: 0.2683\n",
            "Epoch [33/100], Step [181/211], Loss: 0.2736\n",
            "Epoch [33/100], Step [182/211], Loss: 0.2697\n",
            "Epoch [33/100], Step [183/211], Loss: 0.1907\n",
            "Epoch [33/100], Step [184/211], Loss: 0.4332\n",
            "Epoch [33/100], Step [185/211], Loss: 0.5912\n",
            "Epoch [33/100], Step [186/211], Loss: 0.5092\n",
            "Epoch [33/100], Step [187/211], Loss: 0.2801\n",
            "Epoch [33/100], Step [188/211], Loss: 0.2081\n",
            "Epoch [33/100], Step [189/211], Loss: 0.4816\n",
            "Epoch [33/100], Step [190/211], Loss: 0.2089\n",
            "Epoch [33/100], Step [191/211], Loss: 0.1482\n",
            "Epoch [33/100], Step [192/211], Loss: 0.4280\n",
            "Epoch [33/100], Step [193/211], Loss: 0.2557\n",
            "Epoch [33/100], Step [194/211], Loss: 0.3720\n",
            "Epoch [33/100], Step [195/211], Loss: 0.5391\n",
            "Epoch [33/100], Step [196/211], Loss: 0.2109\n",
            "Epoch [33/100], Step [197/211], Loss: 0.4304\n",
            "Epoch [33/100], Step [198/211], Loss: 0.4654\n",
            "Epoch [33/100], Step [199/211], Loss: 0.1661\n",
            "Epoch [33/100], Step [200/211], Loss: 0.2965\n",
            "Epoch [33/100], Step [201/211], Loss: 0.2339\n",
            "Epoch [33/100], Step [202/211], Loss: 0.1937\n",
            "Epoch [33/100], Step [203/211], Loss: 0.2564\n",
            "Epoch [33/100], Step [204/211], Loss: 0.2776\n",
            "Epoch [33/100], Step [205/211], Loss: 0.1633\n",
            "Epoch [33/100], Step [206/211], Loss: 0.3371\n",
            "Epoch [33/100], Step [207/211], Loss: 0.1606\n",
            "Epoch [33/100], Step [208/211], Loss: 0.1374\n",
            "Epoch [33/100], Step [209/211], Loss: 0.2765\n",
            "Epoch [33/100], Step [210/211], Loss: 0.3214\n",
            "Epoch [33/100], Step [211/211], Loss: 0.1763\n",
            "Loss media: 0.2925\n",
            "Epoch [34/100], Step [1/211], Loss: 0.1645\n",
            "Epoch [34/100], Step [2/211], Loss: 0.2663\n",
            "Epoch [34/100], Step [3/211], Loss: 0.3389\n",
            "Epoch [34/100], Step [4/211], Loss: 0.5209\n",
            "Epoch [34/100], Step [5/211], Loss: 0.5320\n",
            "Epoch [34/100], Step [6/211], Loss: 0.2583\n",
            "Epoch [34/100], Step [7/211], Loss: 0.3732\n",
            "Epoch [34/100], Step [8/211], Loss: 0.1209\n",
            "Epoch [34/100], Step [9/211], Loss: 0.4881\n",
            "Epoch [34/100], Step [10/211], Loss: 0.3272\n",
            "Epoch [34/100], Step [11/211], Loss: 0.1409\n",
            "Epoch [34/100], Step [12/211], Loss: 0.3908\n",
            "Epoch [34/100], Step [13/211], Loss: 0.1940\n",
            "Epoch [34/100], Step [14/211], Loss: 0.4965\n",
            "Epoch [34/100], Step [15/211], Loss: 0.2537\n",
            "Epoch [34/100], Step [16/211], Loss: 0.4566\n",
            "Epoch [34/100], Step [17/211], Loss: 0.2500\n",
            "Epoch [34/100], Step [18/211], Loss: 0.3576\n",
            "Epoch [34/100], Step [19/211], Loss: 0.4549\n",
            "Epoch [34/100], Step [20/211], Loss: 0.3438\n",
            "Epoch [34/100], Step [21/211], Loss: 0.3781\n",
            "Epoch [34/100], Step [22/211], Loss: 0.1774\n",
            "Epoch [34/100], Step [23/211], Loss: 0.5456\n",
            "Epoch [34/100], Step [24/211], Loss: 0.6716\n",
            "Epoch [34/100], Step [25/211], Loss: 0.2463\n",
            "Epoch [34/100], Step [26/211], Loss: 0.1671\n",
            "Epoch [34/100], Step [27/211], Loss: 0.3291\n",
            "Epoch [34/100], Step [28/211], Loss: 0.3131\n",
            "Epoch [34/100], Step [29/211], Loss: 0.1262\n",
            "Epoch [34/100], Step [30/211], Loss: 0.1170\n",
            "Epoch [34/100], Step [31/211], Loss: 0.2531\n",
            "Epoch [34/100], Step [32/211], Loss: 0.3477\n",
            "Epoch [34/100], Step [33/211], Loss: 0.3724\n",
            "Epoch [34/100], Step [34/211], Loss: 0.1146\n",
            "Epoch [34/100], Step [35/211], Loss: 0.2012\n",
            "Epoch [34/100], Step [36/211], Loss: 0.2686\n",
            "Epoch [34/100], Step [37/211], Loss: 0.2803\n",
            "Epoch [34/100], Step [38/211], Loss: 0.3933\n",
            "Epoch [34/100], Step [39/211], Loss: 0.1629\n",
            "Epoch [34/100], Step [40/211], Loss: 0.2651\n",
            "Epoch [34/100], Step [41/211], Loss: 0.2184\n",
            "Epoch [34/100], Step [42/211], Loss: 0.1454\n",
            "Epoch [34/100], Step [43/211], Loss: 0.2094\n",
            "Epoch [34/100], Step [44/211], Loss: 0.2720\n",
            "Epoch [34/100], Step [45/211], Loss: 0.3523\n",
            "Epoch [34/100], Step [46/211], Loss: 0.2605\n",
            "Epoch [34/100], Step [47/211], Loss: 0.6623\n",
            "Epoch [34/100], Step [48/211], Loss: 0.1246\n",
            "Epoch [34/100], Step [49/211], Loss: 0.2351\n",
            "Epoch [34/100], Step [50/211], Loss: 0.1968\n",
            "Epoch [34/100], Step [51/211], Loss: 0.6140\n",
            "Epoch [34/100], Step [52/211], Loss: 0.4709\n",
            "Epoch [34/100], Step [53/211], Loss: 0.4002\n",
            "Epoch [34/100], Step [54/211], Loss: 0.1513\n",
            "Epoch [34/100], Step [55/211], Loss: 0.3148\n",
            "Epoch [34/100], Step [56/211], Loss: 0.2952\n",
            "Epoch [34/100], Step [57/211], Loss: 0.3459\n",
            "Epoch [34/100], Step [58/211], Loss: 0.2624\n",
            "Epoch [34/100], Step [59/211], Loss: 0.2544\n",
            "Epoch [34/100], Step [60/211], Loss: 0.2556\n",
            "Epoch [34/100], Step [61/211], Loss: 0.1462\n",
            "Epoch [34/100], Step [62/211], Loss: 0.2914\n",
            "Epoch [34/100], Step [63/211], Loss: 0.1449\n",
            "Epoch [34/100], Step [64/211], Loss: 0.2365\n",
            "Epoch [34/100], Step [65/211], Loss: 0.2138\n",
            "Epoch [34/100], Step [66/211], Loss: 0.2126\n",
            "Epoch [34/100], Step [67/211], Loss: 0.1724\n",
            "Epoch [34/100], Step [68/211], Loss: 0.3042\n",
            "Epoch [34/100], Step [69/211], Loss: 0.2747\n",
            "Epoch [34/100], Step [70/211], Loss: 0.3250\n",
            "Epoch [34/100], Step [71/211], Loss: 0.4025\n",
            "Epoch [34/100], Step [72/211], Loss: 0.2037\n",
            "Epoch [34/100], Step [73/211], Loss: 0.2841\n",
            "Epoch [34/100], Step [74/211], Loss: 0.1330\n",
            "Epoch [34/100], Step [75/211], Loss: 0.1562\n",
            "Epoch [34/100], Step [76/211], Loss: 0.1931\n",
            "Epoch [34/100], Step [77/211], Loss: 0.2458\n",
            "Epoch [34/100], Step [78/211], Loss: 0.3261\n",
            "Epoch [34/100], Step [79/211], Loss: 0.4698\n",
            "Epoch [34/100], Step [80/211], Loss: 0.2732\n",
            "Epoch [34/100], Step [81/211], Loss: 0.2998\n",
            "Epoch [34/100], Step [82/211], Loss: 0.1577\n",
            "Epoch [34/100], Step [83/211], Loss: 0.1259\n",
            "Epoch [34/100], Step [84/211], Loss: 0.1924\n",
            "Epoch [34/100], Step [85/211], Loss: 0.3433\n",
            "Epoch [34/100], Step [86/211], Loss: 0.4010\n",
            "Epoch [34/100], Step [87/211], Loss: 0.2533\n",
            "Epoch [34/100], Step [88/211], Loss: 0.2263\n",
            "Epoch [34/100], Step [89/211], Loss: 0.1931\n",
            "Epoch [34/100], Step [90/211], Loss: 0.2281\n",
            "Epoch [34/100], Step [91/211], Loss: 0.3483\n",
            "Epoch [34/100], Step [92/211], Loss: 0.3051\n",
            "Epoch [34/100], Step [93/211], Loss: 0.3385\n",
            "Epoch [34/100], Step [94/211], Loss: 0.1423\n",
            "Epoch [34/100], Step [95/211], Loss: 0.2723\n",
            "Epoch [34/100], Step [96/211], Loss: 0.3121\n",
            "Epoch [34/100], Step [97/211], Loss: 0.2307\n",
            "Epoch [34/100], Step [98/211], Loss: 0.5510\n",
            "Epoch [34/100], Step [99/211], Loss: 0.1716\n",
            "Epoch [34/100], Step [100/211], Loss: 0.1630\n",
            "Epoch [34/100], Step [101/211], Loss: 0.1482\n",
            "Epoch [34/100], Step [102/211], Loss: 0.1247\n",
            "Epoch [34/100], Step [103/211], Loss: 0.3906\n",
            "Epoch [34/100], Step [104/211], Loss: 0.2712\n",
            "Epoch [34/100], Step [105/211], Loss: 0.3571\n",
            "Epoch [34/100], Step [106/211], Loss: 0.6264\n",
            "Epoch [34/100], Step [107/211], Loss: 0.2913\n",
            "Epoch [34/100], Step [108/211], Loss: 0.3518\n",
            "Epoch [34/100], Step [109/211], Loss: 0.4645\n",
            "Epoch [34/100], Step [110/211], Loss: 0.3010\n",
            "Epoch [34/100], Step [111/211], Loss: 0.3293\n",
            "Epoch [34/100], Step [112/211], Loss: 0.4109\n",
            "Epoch [34/100], Step [113/211], Loss: 0.2069\n",
            "Epoch [34/100], Step [114/211], Loss: 0.2153\n",
            "Epoch [34/100], Step [115/211], Loss: 0.2141\n",
            "Epoch [34/100], Step [116/211], Loss: 0.3042\n",
            "Epoch [34/100], Step [117/211], Loss: 0.1406\n",
            "Epoch [34/100], Step [118/211], Loss: 0.2792\n",
            "Epoch [34/100], Step [119/211], Loss: 0.4760\n",
            "Epoch [34/100], Step [120/211], Loss: 0.5251\n",
            "Epoch [34/100], Step [121/211], Loss: 0.4320\n",
            "Epoch [34/100], Step [122/211], Loss: 0.4138\n",
            "Epoch [34/100], Step [123/211], Loss: 0.3651\n",
            "Epoch [34/100], Step [124/211], Loss: 0.3008\n",
            "Epoch [34/100], Step [125/211], Loss: 0.3442\n",
            "Epoch [34/100], Step [126/211], Loss: 0.2004\n",
            "Epoch [34/100], Step [127/211], Loss: 0.1346\n",
            "Epoch [34/100], Step [128/211], Loss: 0.3167\n",
            "Epoch [34/100], Step [129/211], Loss: 0.4520\n",
            "Epoch [34/100], Step [130/211], Loss: 0.2343\n",
            "Epoch [34/100], Step [131/211], Loss: 0.5046\n",
            "Epoch [34/100], Step [132/211], Loss: 0.5060\n",
            "Epoch [34/100], Step [133/211], Loss: 0.2284\n",
            "Epoch [34/100], Step [134/211], Loss: 0.4328\n",
            "Epoch [34/100], Step [135/211], Loss: 0.2108\n",
            "Epoch [34/100], Step [136/211], Loss: 0.3858\n",
            "Epoch [34/100], Step [137/211], Loss: 0.2837\n",
            "Epoch [34/100], Step [138/211], Loss: 0.2241\n",
            "Epoch [34/100], Step [139/211], Loss: 0.4895\n",
            "Epoch [34/100], Step [140/211], Loss: 0.6374\n",
            "Epoch [34/100], Step [141/211], Loss: 0.5642\n",
            "Epoch [34/100], Step [142/211], Loss: 0.3433\n",
            "Epoch [34/100], Step [143/211], Loss: 0.2953\n",
            "Epoch [34/100], Step [144/211], Loss: 0.4750\n",
            "Epoch [34/100], Step [145/211], Loss: 0.1685\n",
            "Epoch [34/100], Step [146/211], Loss: 0.3082\n",
            "Epoch [34/100], Step [147/211], Loss: 0.3042\n",
            "Epoch [34/100], Step [148/211], Loss: 0.3470\n",
            "Epoch [34/100], Step [149/211], Loss: 0.2196\n",
            "Epoch [34/100], Step [150/211], Loss: 0.3839\n",
            "Epoch [34/100], Step [151/211], Loss: 0.3581\n",
            "Epoch [34/100], Step [152/211], Loss: 0.2478\n",
            "Epoch [34/100], Step [153/211], Loss: 0.2435\n",
            "Epoch [34/100], Step [154/211], Loss: 0.7700\n",
            "Epoch [34/100], Step [155/211], Loss: 0.2562\n",
            "Epoch [34/100], Step [156/211], Loss: 0.2333\n",
            "Epoch [34/100], Step [157/211], Loss: 0.2322\n",
            "Epoch [34/100], Step [158/211], Loss: 0.2986\n",
            "Epoch [34/100], Step [159/211], Loss: 0.1730\n",
            "Epoch [34/100], Step [160/211], Loss: 0.3308\n",
            "Epoch [34/100], Step [161/211], Loss: 0.2751\n",
            "Epoch [34/100], Step [162/211], Loss: 0.3965\n",
            "Epoch [34/100], Step [163/211], Loss: 0.2961\n",
            "Epoch [34/100], Step [164/211], Loss: 0.5119\n",
            "Epoch [34/100], Step [165/211], Loss: 0.1528\n",
            "Epoch [34/100], Step [166/211], Loss: 0.1176\n",
            "Epoch [34/100], Step [167/211], Loss: 0.5081\n",
            "Epoch [34/100], Step [168/211], Loss: 0.3073\n",
            "Epoch [34/100], Step [169/211], Loss: 0.3249\n",
            "Epoch [34/100], Step [170/211], Loss: 0.6219\n",
            "Epoch [34/100], Step [171/211], Loss: 0.4959\n",
            "Epoch [34/100], Step [172/211], Loss: 0.2527\n",
            "Epoch [34/100], Step [173/211], Loss: 0.2099\n",
            "Epoch [34/100], Step [174/211], Loss: 0.1546\n",
            "Epoch [34/100], Step [175/211], Loss: 0.2441\n",
            "Epoch [34/100], Step [176/211], Loss: 0.3993\n",
            "Epoch [34/100], Step [177/211], Loss: 0.2294\n",
            "Epoch [34/100], Step [178/211], Loss: 0.2770\n",
            "Epoch [34/100], Step [179/211], Loss: 0.1490\n",
            "Epoch [34/100], Step [180/211], Loss: 0.5226\n",
            "Epoch [34/100], Step [181/211], Loss: 0.1568\n",
            "Epoch [34/100], Step [182/211], Loss: 0.3569\n",
            "Epoch [34/100], Step [183/211], Loss: 0.1438\n",
            "Epoch [34/100], Step [184/211], Loss: 0.2667\n",
            "Epoch [34/100], Step [185/211], Loss: 0.2228\n",
            "Epoch [34/100], Step [186/211], Loss: 0.2519\n",
            "Epoch [34/100], Step [187/211], Loss: 0.2949\n",
            "Epoch [34/100], Step [188/211], Loss: 0.2416\n",
            "Epoch [34/100], Step [189/211], Loss: 0.3958\n",
            "Epoch [34/100], Step [190/211], Loss: 0.2802\n",
            "Epoch [34/100], Step [191/211], Loss: 0.1801\n",
            "Epoch [34/100], Step [192/211], Loss: 0.1867\n",
            "Epoch [34/100], Step [193/211], Loss: 0.2396\n",
            "Epoch [34/100], Step [194/211], Loss: 0.1842\n",
            "Epoch [34/100], Step [195/211], Loss: 0.2275\n",
            "Epoch [34/100], Step [196/211], Loss: 0.3493\n",
            "Epoch [34/100], Step [197/211], Loss: 0.4810\n",
            "Epoch [34/100], Step [198/211], Loss: 0.1739\n",
            "Epoch [34/100], Step [199/211], Loss: 0.3488\n",
            "Epoch [34/100], Step [200/211], Loss: 0.1306\n",
            "Epoch [34/100], Step [201/211], Loss: 0.2378\n",
            "Epoch [34/100], Step [202/211], Loss: 0.2032\n",
            "Epoch [34/100], Step [203/211], Loss: 0.2060\n",
            "Epoch [34/100], Step [204/211], Loss: 0.2307\n",
            "Epoch [34/100], Step [205/211], Loss: 0.3454\n",
            "Epoch [34/100], Step [206/211], Loss: 0.1043\n",
            "Epoch [34/100], Step [207/211], Loss: 0.4875\n",
            "Epoch [34/100], Step [208/211], Loss: 0.1625\n",
            "Epoch [34/100], Step [209/211], Loss: 0.3457\n",
            "Epoch [34/100], Step [210/211], Loss: 0.3025\n",
            "Epoch [34/100], Step [211/211], Loss: 0.2140\n",
            "Loss media: 0.2997\n",
            "Epoch [35/100], Step [1/211], Loss: 0.1103\n",
            "Epoch [35/100], Step [2/211], Loss: 0.2279\n",
            "Epoch [35/100], Step [3/211], Loss: 0.2296\n",
            "Epoch [35/100], Step [4/211], Loss: 0.3849\n",
            "Epoch [35/100], Step [5/211], Loss: 0.2581\n",
            "Epoch [35/100], Step [6/211], Loss: 0.1782\n",
            "Epoch [35/100], Step [7/211], Loss: 0.2445\n",
            "Epoch [35/100], Step [8/211], Loss: 0.2748\n",
            "Epoch [35/100], Step [9/211], Loss: 0.3589\n",
            "Epoch [35/100], Step [10/211], Loss: 0.4560\n",
            "Epoch [35/100], Step [11/211], Loss: 0.2862\n",
            "Epoch [35/100], Step [12/211], Loss: 0.2415\n",
            "Epoch [35/100], Step [13/211], Loss: 0.2756\n",
            "Epoch [35/100], Step [14/211], Loss: 0.3279\n",
            "Epoch [35/100], Step [15/211], Loss: 0.4117\n",
            "Epoch [35/100], Step [16/211], Loss: 0.2435\n",
            "Epoch [35/100], Step [17/211], Loss: 0.1809\n",
            "Epoch [35/100], Step [18/211], Loss: 0.2976\n",
            "Epoch [35/100], Step [19/211], Loss: 0.4506\n",
            "Epoch [35/100], Step [20/211], Loss: 0.3550\n",
            "Epoch [35/100], Step [21/211], Loss: 0.2552\n",
            "Epoch [35/100], Step [22/211], Loss: 0.1522\n",
            "Epoch [35/100], Step [23/211], Loss: 0.2506\n",
            "Epoch [35/100], Step [24/211], Loss: 0.1928\n",
            "Epoch [35/100], Step [25/211], Loss: 0.3436\n",
            "Epoch [35/100], Step [26/211], Loss: 0.2618\n",
            "Epoch [35/100], Step [27/211], Loss: 0.5088\n",
            "Epoch [35/100], Step [28/211], Loss: 0.3079\n",
            "Epoch [35/100], Step [29/211], Loss: 0.3570\n",
            "Epoch [35/100], Step [30/211], Loss: 0.4079\n",
            "Epoch [35/100], Step [31/211], Loss: 0.3694\n",
            "Epoch [35/100], Step [32/211], Loss: 0.6097\n",
            "Epoch [35/100], Step [33/211], Loss: 0.3133\n",
            "Epoch [35/100], Step [34/211], Loss: 0.2181\n",
            "Epoch [35/100], Step [35/211], Loss: 0.2416\n",
            "Epoch [35/100], Step [36/211], Loss: 0.3486\n",
            "Epoch [35/100], Step [37/211], Loss: 0.3543\n",
            "Epoch [35/100], Step [38/211], Loss: 0.1410\n",
            "Epoch [35/100], Step [39/211], Loss: 0.4490\n",
            "Epoch [35/100], Step [40/211], Loss: 0.3772\n",
            "Epoch [35/100], Step [41/211], Loss: 0.2925\n",
            "Epoch [35/100], Step [42/211], Loss: 0.2655\n",
            "Epoch [35/100], Step [43/211], Loss: 0.6902\n",
            "Epoch [35/100], Step [44/211], Loss: 0.2470\n",
            "Epoch [35/100], Step [45/211], Loss: 0.1585\n",
            "Epoch [35/100], Step [46/211], Loss: 0.3364\n",
            "Epoch [35/100], Step [47/211], Loss: 0.2816\n",
            "Epoch [35/100], Step [48/211], Loss: 0.3340\n",
            "Epoch [35/100], Step [49/211], Loss: 0.3445\n",
            "Epoch [35/100], Step [50/211], Loss: 0.3204\n",
            "Epoch [35/100], Step [51/211], Loss: 0.4448\n",
            "Epoch [35/100], Step [52/211], Loss: 0.1074\n",
            "Epoch [35/100], Step [53/211], Loss: 0.6390\n",
            "Epoch [35/100], Step [54/211], Loss: 0.2261\n",
            "Epoch [35/100], Step [55/211], Loss: 0.3362\n",
            "Epoch [35/100], Step [56/211], Loss: 0.2735\n",
            "Epoch [35/100], Step [57/211], Loss: 0.3509\n",
            "Epoch [35/100], Step [58/211], Loss: 0.4213\n",
            "Epoch [35/100], Step [59/211], Loss: 0.2796\n",
            "Epoch [35/100], Step [60/211], Loss: 0.1524\n",
            "Epoch [35/100], Step [61/211], Loss: 0.2930\n",
            "Epoch [35/100], Step [62/211], Loss: 0.2987\n",
            "Epoch [35/100], Step [63/211], Loss: 0.3712\n",
            "Epoch [35/100], Step [64/211], Loss: 0.1665\n",
            "Epoch [35/100], Step [65/211], Loss: 0.1934\n",
            "Epoch [35/100], Step [66/211], Loss: 0.2546\n",
            "Epoch [35/100], Step [67/211], Loss: 0.4827\n",
            "Epoch [35/100], Step [68/211], Loss: 0.4534\n",
            "Epoch [35/100], Step [69/211], Loss: 0.1895\n",
            "Epoch [35/100], Step [70/211], Loss: 0.1631\n",
            "Epoch [35/100], Step [71/211], Loss: 0.2288\n",
            "Epoch [35/100], Step [72/211], Loss: 0.1351\n",
            "Epoch [35/100], Step [73/211], Loss: 0.1823\n",
            "Epoch [35/100], Step [74/211], Loss: 0.4413\n",
            "Epoch [35/100], Step [75/211], Loss: 0.3245\n",
            "Epoch [35/100], Step [76/211], Loss: 0.3935\n",
            "Epoch [35/100], Step [77/211], Loss: 0.3652\n",
            "Epoch [35/100], Step [78/211], Loss: 0.3016\n",
            "Epoch [35/100], Step [79/211], Loss: 0.1807\n",
            "Epoch [35/100], Step [80/211], Loss: 0.1535\n",
            "Epoch [35/100], Step [81/211], Loss: 0.1691\n",
            "Epoch [35/100], Step [82/211], Loss: 0.1898\n",
            "Epoch [35/100], Step [83/211], Loss: 0.2886\n",
            "Epoch [35/100], Step [84/211], Loss: 0.1397\n",
            "Epoch [35/100], Step [85/211], Loss: 0.2761\n",
            "Epoch [35/100], Step [86/211], Loss: 0.1647\n",
            "Epoch [35/100], Step [87/211], Loss: 0.2557\n",
            "Epoch [35/100], Step [88/211], Loss: 0.1824\n",
            "Epoch [35/100], Step [89/211], Loss: 0.1306\n",
            "Epoch [35/100], Step [90/211], Loss: 0.2624\n",
            "Epoch [35/100], Step [91/211], Loss: 0.3497\n",
            "Epoch [35/100], Step [92/211], Loss: 0.3892\n",
            "Epoch [35/100], Step [93/211], Loss: 0.2908\n",
            "Epoch [35/100], Step [94/211], Loss: 0.4553\n",
            "Epoch [35/100], Step [95/211], Loss: 0.2229\n",
            "Epoch [35/100], Step [96/211], Loss: 0.0725\n",
            "Epoch [35/100], Step [97/211], Loss: 0.4670\n",
            "Epoch [35/100], Step [98/211], Loss: 0.3109\n",
            "Epoch [35/100], Step [99/211], Loss: 0.1513\n",
            "Epoch [35/100], Step [100/211], Loss: 0.2454\n",
            "Epoch [35/100], Step [101/211], Loss: 0.1792\n",
            "Epoch [35/100], Step [102/211], Loss: 0.3040\n",
            "Epoch [35/100], Step [103/211], Loss: 0.3749\n",
            "Epoch [35/100], Step [104/211], Loss: 0.2540\n",
            "Epoch [35/100], Step [105/211], Loss: 0.3342\n",
            "Epoch [35/100], Step [106/211], Loss: 0.3166\n",
            "Epoch [35/100], Step [107/211], Loss: 0.2200\n",
            "Epoch [35/100], Step [108/211], Loss: 0.2733\n",
            "Epoch [35/100], Step [109/211], Loss: 0.4111\n",
            "Epoch [35/100], Step [110/211], Loss: 0.1787\n",
            "Epoch [35/100], Step [111/211], Loss: 0.1819\n",
            "Epoch [35/100], Step [112/211], Loss: 0.2077\n",
            "Epoch [35/100], Step [113/211], Loss: 0.3378\n",
            "Epoch [35/100], Step [114/211], Loss: 0.2783\n",
            "Epoch [35/100], Step [115/211], Loss: 0.2544\n",
            "Epoch [35/100], Step [116/211], Loss: 0.2047\n",
            "Epoch [35/100], Step [117/211], Loss: 0.2646\n",
            "Epoch [35/100], Step [118/211], Loss: 0.5515\n",
            "Epoch [35/100], Step [119/211], Loss: 0.3018\n",
            "Epoch [35/100], Step [120/211], Loss: 0.3604\n",
            "Epoch [35/100], Step [121/211], Loss: 0.3526\n",
            "Epoch [35/100], Step [122/211], Loss: 0.3876\n",
            "Epoch [35/100], Step [123/211], Loss: 0.2803\n",
            "Epoch [35/100], Step [124/211], Loss: 0.1862\n",
            "Epoch [35/100], Step [125/211], Loss: 0.3137\n",
            "Epoch [35/100], Step [126/211], Loss: 0.2016\n",
            "Epoch [35/100], Step [127/211], Loss: 0.6074\n",
            "Epoch [35/100], Step [128/211], Loss: 0.1339\n",
            "Epoch [35/100], Step [129/211], Loss: 0.1418\n",
            "Epoch [35/100], Step [130/211], Loss: 0.3446\n",
            "Epoch [35/100], Step [131/211], Loss: 0.6884\n",
            "Epoch [35/100], Step [132/211], Loss: 0.1416\n",
            "Epoch [35/100], Step [133/211], Loss: 0.2166\n",
            "Epoch [35/100], Step [134/211], Loss: 0.3352\n",
            "Epoch [35/100], Step [135/211], Loss: 0.3503\n",
            "Epoch [35/100], Step [136/211], Loss: 0.3951\n",
            "Epoch [35/100], Step [137/211], Loss: 0.2640\n",
            "Epoch [35/100], Step [138/211], Loss: 0.3118\n",
            "Epoch [35/100], Step [139/211], Loss: 0.1238\n",
            "Epoch [35/100], Step [140/211], Loss: 0.2867\n",
            "Epoch [35/100], Step [141/211], Loss: 0.2544\n",
            "Epoch [35/100], Step [142/211], Loss: 0.2724\n",
            "Epoch [35/100], Step [143/211], Loss: 0.2531\n",
            "Epoch [35/100], Step [144/211], Loss: 0.2138\n",
            "Epoch [35/100], Step [145/211], Loss: 0.2719\n",
            "Epoch [35/100], Step [146/211], Loss: 0.3745\n",
            "Epoch [35/100], Step [147/211], Loss: 0.2431\n",
            "Epoch [35/100], Step [148/211], Loss: 0.2441\n",
            "Epoch [35/100], Step [149/211], Loss: 0.1198\n",
            "Epoch [35/100], Step [150/211], Loss: 0.3074\n",
            "Epoch [35/100], Step [151/211], Loss: 0.1753\n",
            "Epoch [35/100], Step [152/211], Loss: 0.2956\n",
            "Epoch [35/100], Step [153/211], Loss: 0.2326\n",
            "Epoch [35/100], Step [154/211], Loss: 0.1681\n",
            "Epoch [35/100], Step [155/211], Loss: 0.2086\n",
            "Epoch [35/100], Step [156/211], Loss: 0.4984\n",
            "Epoch [35/100], Step [157/211], Loss: 0.2274\n",
            "Epoch [35/100], Step [158/211], Loss: 0.1142\n",
            "Epoch [35/100], Step [159/211], Loss: 0.2020\n",
            "Epoch [35/100], Step [160/211], Loss: 0.8804\n",
            "Epoch [35/100], Step [161/211], Loss: 0.4426\n",
            "Epoch [35/100], Step [162/211], Loss: 0.2134\n",
            "Epoch [35/100], Step [163/211], Loss: 0.1759\n",
            "Epoch [35/100], Step [164/211], Loss: 0.2400\n",
            "Epoch [35/100], Step [165/211], Loss: 0.1028\n",
            "Epoch [35/100], Step [166/211], Loss: 0.1613\n",
            "Epoch [35/100], Step [167/211], Loss: 0.2718\n",
            "Epoch [35/100], Step [168/211], Loss: 0.1679\n",
            "Epoch [35/100], Step [169/211], Loss: 0.1766\n",
            "Epoch [35/100], Step [170/211], Loss: 0.3563\n",
            "Epoch [35/100], Step [171/211], Loss: 0.1644\n",
            "Epoch [35/100], Step [172/211], Loss: 0.1144\n",
            "Epoch [35/100], Step [173/211], Loss: 0.4405\n",
            "Epoch [35/100], Step [174/211], Loss: 0.2489\n",
            "Epoch [35/100], Step [175/211], Loss: 0.1286\n",
            "Epoch [35/100], Step [176/211], Loss: 0.3503\n",
            "Epoch [35/100], Step [177/211], Loss: 0.4139\n",
            "Epoch [35/100], Step [178/211], Loss: 0.4218\n",
            "Epoch [35/100], Step [179/211], Loss: 0.2204\n",
            "Epoch [35/100], Step [180/211], Loss: 0.1530\n",
            "Epoch [35/100], Step [181/211], Loss: 0.4351\n",
            "Epoch [35/100], Step [182/211], Loss: 0.3422\n",
            "Epoch [35/100], Step [183/211], Loss: 0.1163\n",
            "Epoch [35/100], Step [184/211], Loss: 0.5668\n",
            "Epoch [35/100], Step [185/211], Loss: 0.1912\n",
            "Epoch [35/100], Step [186/211], Loss: 0.3994\n",
            "Epoch [35/100], Step [187/211], Loss: 0.2763\n",
            "Epoch [35/100], Step [188/211], Loss: 0.1141\n",
            "Epoch [35/100], Step [189/211], Loss: 0.3802\n",
            "Epoch [35/100], Step [190/211], Loss: 0.3704\n",
            "Epoch [35/100], Step [191/211], Loss: 0.3784\n",
            "Epoch [35/100], Step [192/211], Loss: 0.3695\n",
            "Epoch [35/100], Step [193/211], Loss: 0.2115\n",
            "Epoch [35/100], Step [194/211], Loss: 0.2956\n",
            "Epoch [35/100], Step [195/211], Loss: 0.1559\n",
            "Epoch [35/100], Step [196/211], Loss: 0.3489\n",
            "Epoch [35/100], Step [197/211], Loss: 0.3233\n",
            "Epoch [35/100], Step [198/211], Loss: 0.2414\n",
            "Epoch [35/100], Step [199/211], Loss: 0.2993\n",
            "Epoch [35/100], Step [200/211], Loss: 0.1976\n",
            "Epoch [35/100], Step [201/211], Loss: 0.1864\n",
            "Epoch [35/100], Step [202/211], Loss: 0.7870\n",
            "Epoch [35/100], Step [203/211], Loss: 0.3490\n",
            "Epoch [35/100], Step [204/211], Loss: 0.2407\n",
            "Epoch [35/100], Step [205/211], Loss: 0.2159\n",
            "Epoch [35/100], Step [206/211], Loss: 0.1721\n",
            "Epoch [35/100], Step [207/211], Loss: 0.3843\n",
            "Epoch [35/100], Step [208/211], Loss: 0.4024\n",
            "Epoch [35/100], Step [209/211], Loss: 0.2596\n",
            "Epoch [35/100], Step [210/211], Loss: 0.1642\n",
            "Epoch [35/100], Step [211/211], Loss: 0.2652\n",
            "Loss media: 0.2898\n",
            "Epoch [36/100], Step [1/211], Loss: 0.3378\n",
            "Epoch [36/100], Step [2/211], Loss: 0.2897\n",
            "Epoch [36/100], Step [3/211], Loss: 0.3239\n",
            "Epoch [36/100], Step [4/211], Loss: 0.4126\n",
            "Epoch [36/100], Step [5/211], Loss: 0.1604\n",
            "Epoch [36/100], Step [6/211], Loss: 0.3047\n",
            "Epoch [36/100], Step [7/211], Loss: 0.3299\n",
            "Epoch [36/100], Step [8/211], Loss: 0.2785\n",
            "Epoch [36/100], Step [9/211], Loss: 0.2303\n",
            "Epoch [36/100], Step [10/211], Loss: 0.4717\n",
            "Epoch [36/100], Step [11/211], Loss: 0.2780\n",
            "Epoch [36/100], Step [12/211], Loss: 0.2853\n",
            "Epoch [36/100], Step [13/211], Loss: 0.1945\n",
            "Epoch [36/100], Step [14/211], Loss: 0.1947\n",
            "Epoch [36/100], Step [15/211], Loss: 0.1028\n",
            "Epoch [36/100], Step [16/211], Loss: 0.2424\n",
            "Epoch [36/100], Step [17/211], Loss: 0.3504\n",
            "Epoch [36/100], Step [18/211], Loss: 0.0692\n",
            "Epoch [36/100], Step [19/211], Loss: 0.1682\n",
            "Epoch [36/100], Step [20/211], Loss: 0.2339\n",
            "Epoch [36/100], Step [21/211], Loss: 0.2371\n",
            "Epoch [36/100], Step [22/211], Loss: 0.1163\n",
            "Epoch [36/100], Step [23/211], Loss: 0.3357\n",
            "Epoch [36/100], Step [24/211], Loss: 0.0969\n",
            "Epoch [36/100], Step [25/211], Loss: 0.2727\n",
            "Epoch [36/100], Step [26/211], Loss: 0.3700\n",
            "Epoch [36/100], Step [27/211], Loss: 0.3571\n",
            "Epoch [36/100], Step [28/211], Loss: 0.4106\n",
            "Epoch [36/100], Step [29/211], Loss: 0.2049\n",
            "Epoch [36/100], Step [30/211], Loss: 0.1428\n",
            "Epoch [36/100], Step [31/211], Loss: 0.5546\n",
            "Epoch [36/100], Step [32/211], Loss: 0.2249\n",
            "Epoch [36/100], Step [33/211], Loss: 0.3305\n",
            "Epoch [36/100], Step [34/211], Loss: 0.3690\n",
            "Epoch [36/100], Step [35/211], Loss: 0.1561\n",
            "Epoch [36/100], Step [36/211], Loss: 0.3039\n",
            "Epoch [36/100], Step [37/211], Loss: 0.2488\n",
            "Epoch [36/100], Step [38/211], Loss: 0.2789\n",
            "Epoch [36/100], Step [39/211], Loss: 0.4282\n",
            "Epoch [36/100], Step [40/211], Loss: 0.3580\n",
            "Epoch [36/100], Step [41/211], Loss: 0.5194\n",
            "Epoch [36/100], Step [42/211], Loss: 0.1822\n",
            "Epoch [36/100], Step [43/211], Loss: 0.2975\n",
            "Epoch [36/100], Step [44/211], Loss: 0.1634\n",
            "Epoch [36/100], Step [45/211], Loss: 0.2980\n",
            "Epoch [36/100], Step [46/211], Loss: 0.1822\n",
            "Epoch [36/100], Step [47/211], Loss: 0.2260\n",
            "Epoch [36/100], Step [48/211], Loss: 0.3171\n",
            "Epoch [36/100], Step [49/211], Loss: 0.3772\n",
            "Epoch [36/100], Step [50/211], Loss: 0.2320\n",
            "Epoch [36/100], Step [51/211], Loss: 0.2707\n",
            "Epoch [36/100], Step [52/211], Loss: 0.4173\n",
            "Epoch [36/100], Step [53/211], Loss: 0.2574\n",
            "Epoch [36/100], Step [54/211], Loss: 0.4888\n",
            "Epoch [36/100], Step [55/211], Loss: 0.2809\n",
            "Epoch [36/100], Step [56/211], Loss: 0.1897\n",
            "Epoch [36/100], Step [57/211], Loss: 0.3023\n",
            "Epoch [36/100], Step [58/211], Loss: 0.2063\n",
            "Epoch [36/100], Step [59/211], Loss: 0.2085\n",
            "Epoch [36/100], Step [60/211], Loss: 0.5381\n",
            "Epoch [36/100], Step [61/211], Loss: 0.3564\n",
            "Epoch [36/100], Step [62/211], Loss: 0.1562\n",
            "Epoch [36/100], Step [63/211], Loss: 0.1419\n",
            "Epoch [36/100], Step [64/211], Loss: 0.5261\n",
            "Epoch [36/100], Step [65/211], Loss: 0.3380\n",
            "Epoch [36/100], Step [66/211], Loss: 0.0992\n",
            "Epoch [36/100], Step [67/211], Loss: 0.4269\n",
            "Epoch [36/100], Step [68/211], Loss: 0.3043\n",
            "Epoch [36/100], Step [69/211], Loss: 0.1458\n",
            "Epoch [36/100], Step [70/211], Loss: 0.1017\n",
            "Epoch [36/100], Step [71/211], Loss: 0.1062\n",
            "Epoch [36/100], Step [72/211], Loss: 0.3677\n",
            "Epoch [36/100], Step [73/211], Loss: 0.2475\n",
            "Epoch [36/100], Step [74/211], Loss: 0.3249\n",
            "Epoch [36/100], Step [75/211], Loss: 0.2465\n",
            "Epoch [36/100], Step [76/211], Loss: 0.4349\n",
            "Epoch [36/100], Step [77/211], Loss: 0.4007\n",
            "Epoch [36/100], Step [78/211], Loss: 0.3022\n",
            "Epoch [36/100], Step [79/211], Loss: 0.2515\n",
            "Epoch [36/100], Step [80/211], Loss: 0.3094\n",
            "Epoch [36/100], Step [81/211], Loss: 0.1178\n",
            "Epoch [36/100], Step [82/211], Loss: 0.1992\n",
            "Epoch [36/100], Step [83/211], Loss: 0.3360\n",
            "Epoch [36/100], Step [84/211], Loss: 0.3120\n",
            "Epoch [36/100], Step [85/211], Loss: 0.1583\n",
            "Epoch [36/100], Step [86/211], Loss: 0.1493\n",
            "Epoch [36/100], Step [87/211], Loss: 0.3199\n",
            "Epoch [36/100], Step [88/211], Loss: 0.3102\n",
            "Epoch [36/100], Step [89/211], Loss: 0.1916\n",
            "Epoch [36/100], Step [90/211], Loss: 0.4272\n",
            "Epoch [36/100], Step [91/211], Loss: 0.2676\n",
            "Epoch [36/100], Step [92/211], Loss: 0.3588\n",
            "Epoch [36/100], Step [93/211], Loss: 0.1252\n",
            "Epoch [36/100], Step [94/211], Loss: 0.5871\n",
            "Epoch [36/100], Step [95/211], Loss: 0.1382\n",
            "Epoch [36/100], Step [96/211], Loss: 0.2017\n",
            "Epoch [36/100], Step [97/211], Loss: 0.2182\n",
            "Epoch [36/100], Step [98/211], Loss: 0.2273\n",
            "Epoch [36/100], Step [99/211], Loss: 0.4665\n",
            "Epoch [36/100], Step [100/211], Loss: 0.3895\n",
            "Epoch [36/100], Step [101/211], Loss: 0.2251\n",
            "Epoch [36/100], Step [102/211], Loss: 0.3197\n",
            "Epoch [36/100], Step [103/211], Loss: 0.6064\n",
            "Epoch [36/100], Step [104/211], Loss: 0.3616\n",
            "Epoch [36/100], Step [105/211], Loss: 0.3540\n",
            "Epoch [36/100], Step [106/211], Loss: 0.2083\n",
            "Epoch [36/100], Step [107/211], Loss: 0.1755\n",
            "Epoch [36/100], Step [108/211], Loss: 0.1392\n",
            "Epoch [36/100], Step [109/211], Loss: 0.3420\n",
            "Epoch [36/100], Step [110/211], Loss: 0.3195\n",
            "Epoch [36/100], Step [111/211], Loss: 0.2207\n",
            "Epoch [36/100], Step [112/211], Loss: 0.1344\n",
            "Epoch [36/100], Step [113/211], Loss: 0.2006\n",
            "Epoch [36/100], Step [114/211], Loss: 0.1393\n",
            "Epoch [36/100], Step [115/211], Loss: 0.3269\n",
            "Epoch [36/100], Step [116/211], Loss: 0.1814\n",
            "Epoch [36/100], Step [117/211], Loss: 0.1577\n",
            "Epoch [36/100], Step [118/211], Loss: 0.1612\n",
            "Epoch [36/100], Step [119/211], Loss: 0.1777\n",
            "Epoch [36/100], Step [120/211], Loss: 0.2633\n",
            "Epoch [36/100], Step [121/211], Loss: 0.1665\n",
            "Epoch [36/100], Step [122/211], Loss: 0.2753\n",
            "Epoch [36/100], Step [123/211], Loss: 0.1114\n",
            "Epoch [36/100], Step [124/211], Loss: 0.3965\n",
            "Epoch [36/100], Step [125/211], Loss: 0.2288\n",
            "Epoch [36/100], Step [126/211], Loss: 0.1736\n",
            "Epoch [36/100], Step [127/211], Loss: 0.2169\n",
            "Epoch [36/100], Step [128/211], Loss: 0.2275\n",
            "Epoch [36/100], Step [129/211], Loss: 0.2847\n",
            "Epoch [36/100], Step [130/211], Loss: 0.1840\n",
            "Epoch [36/100], Step [131/211], Loss: 0.1794\n",
            "Epoch [36/100], Step [132/211], Loss: 0.5857\n",
            "Epoch [36/100], Step [133/211], Loss: 0.1109\n",
            "Epoch [36/100], Step [134/211], Loss: 0.4206\n",
            "Epoch [36/100], Step [135/211], Loss: 0.2974\n",
            "Epoch [36/100], Step [136/211], Loss: 0.2606\n",
            "Epoch [36/100], Step [137/211], Loss: 0.2103\n",
            "Epoch [36/100], Step [138/211], Loss: 0.2268\n",
            "Epoch [36/100], Step [139/211], Loss: 0.4395\n",
            "Epoch [36/100], Step [140/211], Loss: 0.1768\n",
            "Epoch [36/100], Step [141/211], Loss: 0.2907\n",
            "Epoch [36/100], Step [142/211], Loss: 0.2538\n",
            "Epoch [36/100], Step [143/211], Loss: 0.1888\n",
            "Epoch [36/100], Step [144/211], Loss: 0.1334\n",
            "Epoch [36/100], Step [145/211], Loss: 0.3040\n",
            "Epoch [36/100], Step [146/211], Loss: 0.2045\n",
            "Epoch [36/100], Step [147/211], Loss: 0.1507\n",
            "Epoch [36/100], Step [148/211], Loss: 0.3800\n",
            "Epoch [36/100], Step [149/211], Loss: 0.1300\n",
            "Epoch [36/100], Step [150/211], Loss: 0.5796\n",
            "Epoch [36/100], Step [151/211], Loss: 0.2162\n",
            "Epoch [36/100], Step [152/211], Loss: 0.1994\n",
            "Epoch [36/100], Step [153/211], Loss: 0.1924\n",
            "Epoch [36/100], Step [154/211], Loss: 0.3498\n",
            "Epoch [36/100], Step [155/211], Loss: 0.5030\n",
            "Epoch [36/100], Step [156/211], Loss: 0.2979\n",
            "Epoch [36/100], Step [157/211], Loss: 0.1518\n",
            "Epoch [36/100], Step [158/211], Loss: 0.2856\n",
            "Epoch [36/100], Step [159/211], Loss: 0.2113\n",
            "Epoch [36/100], Step [160/211], Loss: 0.1652\n",
            "Epoch [36/100], Step [161/211], Loss: 0.2334\n",
            "Epoch [36/100], Step [162/211], Loss: 0.2023\n",
            "Epoch [36/100], Step [163/211], Loss: 0.3446\n",
            "Epoch [36/100], Step [164/211], Loss: 0.6787\n",
            "Epoch [36/100], Step [165/211], Loss: 0.4083\n",
            "Epoch [36/100], Step [166/211], Loss: 0.2792\n",
            "Epoch [36/100], Step [167/211], Loss: 0.5129\n",
            "Epoch [36/100], Step [168/211], Loss: 0.1034\n",
            "Epoch [36/100], Step [169/211], Loss: 0.2339\n",
            "Epoch [36/100], Step [170/211], Loss: 0.5653\n",
            "Epoch [36/100], Step [171/211], Loss: 0.3138\n",
            "Epoch [36/100], Step [172/211], Loss: 0.2670\n",
            "Epoch [36/100], Step [173/211], Loss: 0.2687\n",
            "Epoch [36/100], Step [174/211], Loss: 0.3202\n",
            "Epoch [36/100], Step [175/211], Loss: 0.3042\n",
            "Epoch [36/100], Step [176/211], Loss: 0.2700\n",
            "Epoch [36/100], Step [177/211], Loss: 0.1300\n",
            "Epoch [36/100], Step [178/211], Loss: 0.4119\n",
            "Epoch [36/100], Step [179/211], Loss: 0.2015\n",
            "Epoch [36/100], Step [180/211], Loss: 0.2984\n",
            "Epoch [36/100], Step [181/211], Loss: 0.6591\n",
            "Epoch [36/100], Step [182/211], Loss: 0.3479\n",
            "Epoch [36/100], Step [183/211], Loss: 0.2968\n",
            "Epoch [36/100], Step [184/211], Loss: 0.3334\n",
            "Epoch [36/100], Step [185/211], Loss: 0.2115\n",
            "Epoch [36/100], Step [186/211], Loss: 0.1522\n",
            "Epoch [36/100], Step [187/211], Loss: 0.1158\n",
            "Epoch [36/100], Step [188/211], Loss: 0.3050\n",
            "Epoch [36/100], Step [189/211], Loss: 0.7314\n",
            "Epoch [36/100], Step [190/211], Loss: 0.5030\n",
            "Epoch [36/100], Step [191/211], Loss: 0.2036\n",
            "Epoch [36/100], Step [192/211], Loss: 0.2566\n",
            "Epoch [36/100], Step [193/211], Loss: 0.3448\n",
            "Epoch [36/100], Step [194/211], Loss: 0.2140\n",
            "Epoch [36/100], Step [195/211], Loss: 0.2045\n",
            "Epoch [36/100], Step [196/211], Loss: 0.2287\n",
            "Epoch [36/100], Step [197/211], Loss: 0.3403\n",
            "Epoch [36/100], Step [198/211], Loss: 0.2071\n",
            "Epoch [36/100], Step [199/211], Loss: 0.3043\n",
            "Epoch [36/100], Step [200/211], Loss: 0.2168\n",
            "Epoch [36/100], Step [201/211], Loss: 0.2920\n",
            "Epoch [36/100], Step [202/211], Loss: 0.3862\n",
            "Epoch [36/100], Step [203/211], Loss: 0.2056\n",
            "Epoch [36/100], Step [204/211], Loss: 0.2358\n",
            "Epoch [36/100], Step [205/211], Loss: 0.3417\n",
            "Epoch [36/100], Step [206/211], Loss: 0.4017\n",
            "Epoch [36/100], Step [207/211], Loss: 0.2398\n",
            "Epoch [36/100], Step [208/211], Loss: 0.2790\n",
            "Epoch [36/100], Step [209/211], Loss: 0.2089\n",
            "Epoch [36/100], Step [210/211], Loss: 0.2604\n",
            "Epoch [36/100], Step [211/211], Loss: 0.4791\n",
            "Loss media: 0.2812\n",
            "Epoch [37/100], Step [1/211], Loss: 0.4059\n",
            "Epoch [37/100], Step [2/211], Loss: 0.2006\n",
            "Epoch [37/100], Step [3/211], Loss: 0.3425\n",
            "Epoch [37/100], Step [4/211], Loss: 0.1984\n",
            "Epoch [37/100], Step [5/211], Loss: 0.4507\n",
            "Epoch [37/100], Step [6/211], Loss: 0.3532\n",
            "Epoch [37/100], Step [7/211], Loss: 0.2840\n",
            "Epoch [37/100], Step [8/211], Loss: 0.3811\n",
            "Epoch [37/100], Step [9/211], Loss: 0.2661\n",
            "Epoch [37/100], Step [10/211], Loss: 0.2642\n",
            "Epoch [37/100], Step [11/211], Loss: 0.2347\n",
            "Epoch [37/100], Step [12/211], Loss: 0.3303\n",
            "Epoch [37/100], Step [13/211], Loss: 0.2338\n",
            "Epoch [37/100], Step [14/211], Loss: 0.4208\n",
            "Epoch [37/100], Step [15/211], Loss: 0.1532\n",
            "Epoch [37/100], Step [16/211], Loss: 0.3133\n",
            "Epoch [37/100], Step [17/211], Loss: 0.3706\n",
            "Epoch [37/100], Step [18/211], Loss: 0.1802\n",
            "Epoch [37/100], Step [19/211], Loss: 0.1174\n",
            "Epoch [37/100], Step [20/211], Loss: 0.3359\n",
            "Epoch [37/100], Step [21/211], Loss: 0.4135\n",
            "Epoch [37/100], Step [22/211], Loss: 0.2091\n",
            "Epoch [37/100], Step [23/211], Loss: 0.1007\n",
            "Epoch [37/100], Step [24/211], Loss: 0.4182\n",
            "Epoch [37/100], Step [25/211], Loss: 0.1859\n",
            "Epoch [37/100], Step [26/211], Loss: 0.2405\n",
            "Epoch [37/100], Step [27/211], Loss: 0.2795\n",
            "Epoch [37/100], Step [28/211], Loss: 0.2831\n",
            "Epoch [37/100], Step [29/211], Loss: 0.3770\n",
            "Epoch [37/100], Step [30/211], Loss: 0.5176\n",
            "Epoch [37/100], Step [31/211], Loss: 0.1131\n",
            "Epoch [37/100], Step [32/211], Loss: 0.4646\n",
            "Epoch [37/100], Step [33/211], Loss: 0.2030\n",
            "Epoch [37/100], Step [34/211], Loss: 0.4001\n",
            "Epoch [37/100], Step [35/211], Loss: 0.1553\n",
            "Epoch [37/100], Step [36/211], Loss: 0.2214\n",
            "Epoch [37/100], Step [37/211], Loss: 0.5735\n",
            "Epoch [37/100], Step [38/211], Loss: 0.4278\n",
            "Epoch [37/100], Step [39/211], Loss: 0.2506\n",
            "Epoch [37/100], Step [40/211], Loss: 0.1882\n",
            "Epoch [37/100], Step [41/211], Loss: 0.1797\n",
            "Epoch [37/100], Step [42/211], Loss: 0.3120\n",
            "Epoch [37/100], Step [43/211], Loss: 0.5690\n",
            "Epoch [37/100], Step [44/211], Loss: 0.1734\n",
            "Epoch [37/100], Step [45/211], Loss: 0.3499\n",
            "Epoch [37/100], Step [46/211], Loss: 0.1867\n",
            "Epoch [37/100], Step [47/211], Loss: 0.2644\n",
            "Epoch [37/100], Step [48/211], Loss: 0.1072\n",
            "Epoch [37/100], Step [49/211], Loss: 0.2415\n",
            "Epoch [37/100], Step [50/211], Loss: 0.1826\n",
            "Epoch [37/100], Step [51/211], Loss: 0.4132\n",
            "Epoch [37/100], Step [52/211], Loss: 0.4962\n",
            "Epoch [37/100], Step [53/211], Loss: 0.3270\n",
            "Epoch [37/100], Step [54/211], Loss: 0.1896\n",
            "Epoch [37/100], Step [55/211], Loss: 0.2848\n",
            "Epoch [37/100], Step [56/211], Loss: 0.2300\n",
            "Epoch [37/100], Step [57/211], Loss: 0.2645\n",
            "Epoch [37/100], Step [58/211], Loss: 0.4757\n",
            "Epoch [37/100], Step [59/211], Loss: 0.2111\n",
            "Epoch [37/100], Step [60/211], Loss: 0.1545\n",
            "Epoch [37/100], Step [61/211], Loss: 0.1248\n",
            "Epoch [37/100], Step [62/211], Loss: 0.1693\n",
            "Epoch [37/100], Step [63/211], Loss: 0.4474\n",
            "Epoch [37/100], Step [64/211], Loss: 0.2962\n",
            "Epoch [37/100], Step [65/211], Loss: 0.2612\n",
            "Epoch [37/100], Step [66/211], Loss: 0.1643\n",
            "Epoch [37/100], Step [67/211], Loss: 0.2513\n",
            "Epoch [37/100], Step [68/211], Loss: 0.1795\n",
            "Epoch [37/100], Step [69/211], Loss: 0.5573\n",
            "Epoch [37/100], Step [70/211], Loss: 0.2643\n",
            "Epoch [37/100], Step [71/211], Loss: 0.3017\n",
            "Epoch [37/100], Step [72/211], Loss: 0.1685\n",
            "Epoch [37/100], Step [73/211], Loss: 0.3136\n",
            "Epoch [37/100], Step [74/211], Loss: 0.1249\n",
            "Epoch [37/100], Step [75/211], Loss: 0.2061\n",
            "Epoch [37/100], Step [76/211], Loss: 0.1833\n",
            "Epoch [37/100], Step [77/211], Loss: 0.2877\n",
            "Epoch [37/100], Step [78/211], Loss: 0.2965\n",
            "Epoch [37/100], Step [79/211], Loss: 0.4394\n",
            "Epoch [37/100], Step [80/211], Loss: 0.3413\n",
            "Epoch [37/100], Step [81/211], Loss: 0.3100\n",
            "Epoch [37/100], Step [82/211], Loss: 0.5561\n",
            "Epoch [37/100], Step [83/211], Loss: 0.4937\n",
            "Epoch [37/100], Step [84/211], Loss: 0.1701\n",
            "Epoch [37/100], Step [85/211], Loss: 0.1519\n",
            "Epoch [37/100], Step [86/211], Loss: 0.2977\n",
            "Epoch [37/100], Step [87/211], Loss: 0.1951\n",
            "Epoch [37/100], Step [88/211], Loss: 0.1474\n",
            "Epoch [37/100], Step [89/211], Loss: 0.1061\n",
            "Epoch [37/100], Step [90/211], Loss: 0.2170\n",
            "Epoch [37/100], Step [91/211], Loss: 0.0863\n",
            "Epoch [37/100], Step [92/211], Loss: 0.3151\n",
            "Epoch [37/100], Step [93/211], Loss: 0.4935\n",
            "Epoch [37/100], Step [94/211], Loss: 0.1489\n",
            "Epoch [37/100], Step [95/211], Loss: 0.3904\n",
            "Epoch [37/100], Step [96/211], Loss: 0.2953\n",
            "Epoch [37/100], Step [97/211], Loss: 0.3015\n",
            "Epoch [37/100], Step [98/211], Loss: 0.3627\n",
            "Epoch [37/100], Step [99/211], Loss: 0.3162\n",
            "Epoch [37/100], Step [100/211], Loss: 0.3133\n",
            "Epoch [37/100], Step [101/211], Loss: 0.4759\n",
            "Epoch [37/100], Step [102/211], Loss: 0.3183\n",
            "Epoch [37/100], Step [103/211], Loss: 0.2675\n",
            "Epoch [37/100], Step [104/211], Loss: 0.2895\n",
            "Epoch [37/100], Step [105/211], Loss: 0.2378\n",
            "Epoch [37/100], Step [106/211], Loss: 0.3937\n",
            "Epoch [37/100], Step [107/211], Loss: 0.1726\n",
            "Epoch [37/100], Step [108/211], Loss: 0.2401\n",
            "Epoch [37/100], Step [109/211], Loss: 0.2640\n",
            "Epoch [37/100], Step [110/211], Loss: 0.1748\n",
            "Epoch [37/100], Step [111/211], Loss: 0.1379\n",
            "Epoch [37/100], Step [112/211], Loss: 0.3295\n",
            "Epoch [37/100], Step [113/211], Loss: 0.1423\n",
            "Epoch [37/100], Step [114/211], Loss: 0.1221\n",
            "Epoch [37/100], Step [115/211], Loss: 0.2034\n",
            "Epoch [37/100], Step [116/211], Loss: 0.3589\n",
            "Epoch [37/100], Step [117/211], Loss: 0.3630\n",
            "Epoch [37/100], Step [118/211], Loss: 0.4536\n",
            "Epoch [37/100], Step [119/211], Loss: 0.2490\n",
            "Epoch [37/100], Step [120/211], Loss: 0.2413\n",
            "Epoch [37/100], Step [121/211], Loss: 0.3313\n",
            "Epoch [37/100], Step [122/211], Loss: 0.1813\n",
            "Epoch [37/100], Step [123/211], Loss: 0.3116\n",
            "Epoch [37/100], Step [124/211], Loss: 0.2530\n",
            "Epoch [37/100], Step [125/211], Loss: 0.2352\n",
            "Epoch [37/100], Step [126/211], Loss: 0.4611\n",
            "Epoch [37/100], Step [127/211], Loss: 0.2673\n",
            "Epoch [37/100], Step [128/211], Loss: 0.3960\n",
            "Epoch [37/100], Step [129/211], Loss: 0.4076\n",
            "Epoch [37/100], Step [130/211], Loss: 0.1978\n",
            "Epoch [37/100], Step [131/211], Loss: 0.2090\n",
            "Epoch [37/100], Step [132/211], Loss: 0.1602\n",
            "Epoch [37/100], Step [133/211], Loss: 0.2048\n",
            "Epoch [37/100], Step [134/211], Loss: 0.3229\n",
            "Epoch [37/100], Step [135/211], Loss: 0.1142\n",
            "Epoch [37/100], Step [136/211], Loss: 0.3461\n",
            "Epoch [37/100], Step [137/211], Loss: 0.2689\n",
            "Epoch [37/100], Step [138/211], Loss: 0.2409\n",
            "Epoch [37/100], Step [139/211], Loss: 0.2136\n",
            "Epoch [37/100], Step [140/211], Loss: 0.2202\n",
            "Epoch [37/100], Step [141/211], Loss: 0.3525\n",
            "Epoch [37/100], Step [142/211], Loss: 0.4017\n",
            "Epoch [37/100], Step [143/211], Loss: 0.4143\n",
            "Epoch [37/100], Step [144/211], Loss: 0.2750\n",
            "Epoch [37/100], Step [145/211], Loss: 0.1591\n",
            "Epoch [37/100], Step [146/211], Loss: 0.2348\n",
            "Epoch [37/100], Step [147/211], Loss: 0.1321\n",
            "Epoch [37/100], Step [148/211], Loss: 0.2386\n",
            "Epoch [37/100], Step [149/211], Loss: 0.4055\n",
            "Epoch [37/100], Step [150/211], Loss: 0.5047\n",
            "Epoch [37/100], Step [151/211], Loss: 0.5295\n",
            "Epoch [37/100], Step [152/211], Loss: 0.2653\n",
            "Epoch [37/100], Step [153/211], Loss: 0.2850\n",
            "Epoch [37/100], Step [154/211], Loss: 0.2817\n",
            "Epoch [37/100], Step [155/211], Loss: 0.6095\n",
            "Epoch [37/100], Step [156/211], Loss: 0.1107\n",
            "Epoch [37/100], Step [157/211], Loss: 0.2888\n",
            "Epoch [37/100], Step [158/211], Loss: 0.2995\n",
            "Epoch [37/100], Step [159/211], Loss: 0.1718\n",
            "Epoch [37/100], Step [160/211], Loss: 0.2493\n",
            "Epoch [37/100], Step [161/211], Loss: 0.2125\n",
            "Epoch [37/100], Step [162/211], Loss: 0.2474\n",
            "Epoch [37/100], Step [163/211], Loss: 0.3125\n",
            "Epoch [37/100], Step [164/211], Loss: 0.1849\n",
            "Epoch [37/100], Step [165/211], Loss: 0.1165\n",
            "Epoch [37/100], Step [166/211], Loss: 0.1742\n",
            "Epoch [37/100], Step [167/211], Loss: 0.1620\n",
            "Epoch [37/100], Step [168/211], Loss: 0.1686\n",
            "Epoch [37/100], Step [169/211], Loss: 0.3410\n",
            "Epoch [37/100], Step [170/211], Loss: 0.2620\n",
            "Epoch [37/100], Step [171/211], Loss: 0.3082\n",
            "Epoch [37/100], Step [172/211], Loss: 0.1235\n",
            "Epoch [37/100], Step [173/211], Loss: 0.2130\n",
            "Epoch [37/100], Step [174/211], Loss: 0.1889\n",
            "Epoch [37/100], Step [175/211], Loss: 0.2506\n",
            "Epoch [37/100], Step [176/211], Loss: 0.2837\n",
            "Epoch [37/100], Step [177/211], Loss: 0.1605\n",
            "Epoch [37/100], Step [178/211], Loss: 0.2214\n",
            "Epoch [37/100], Step [179/211], Loss: 0.3659\n",
            "Epoch [37/100], Step [180/211], Loss: 0.1918\n",
            "Epoch [37/100], Step [181/211], Loss: 0.3427\n",
            "Epoch [37/100], Step [182/211], Loss: 0.2085\n",
            "Epoch [37/100], Step [183/211], Loss: 0.5399\n",
            "Epoch [37/100], Step [184/211], Loss: 0.1596\n",
            "Epoch [37/100], Step [185/211], Loss: 0.2516\n",
            "Epoch [37/100], Step [186/211], Loss: 0.2496\n",
            "Epoch [37/100], Step [187/211], Loss: 0.1848\n",
            "Epoch [37/100], Step [188/211], Loss: 0.1504\n",
            "Epoch [37/100], Step [189/211], Loss: 0.3054\n",
            "Epoch [37/100], Step [190/211], Loss: 0.2735\n",
            "Epoch [37/100], Step [191/211], Loss: 0.2319\n",
            "Epoch [37/100], Step [192/211], Loss: 0.2014\n",
            "Epoch [37/100], Step [193/211], Loss: 0.1792\n",
            "Epoch [37/100], Step [194/211], Loss: 0.1898\n",
            "Epoch [37/100], Step [195/211], Loss: 0.1423\n",
            "Epoch [37/100], Step [196/211], Loss: 0.3261\n",
            "Epoch [37/100], Step [197/211], Loss: 0.3652\n",
            "Epoch [37/100], Step [198/211], Loss: 0.2131\n",
            "Epoch [37/100], Step [199/211], Loss: 0.3213\n",
            "Epoch [37/100], Step [200/211], Loss: 0.3909\n",
            "Epoch [37/100], Step [201/211], Loss: 0.1844\n",
            "Epoch [37/100], Step [202/211], Loss: 0.1849\n",
            "Epoch [37/100], Step [203/211], Loss: 0.2917\n",
            "Epoch [37/100], Step [204/211], Loss: 0.1927\n",
            "Epoch [37/100], Step [205/211], Loss: 0.3344\n",
            "Epoch [37/100], Step [206/211], Loss: 0.3511\n",
            "Epoch [37/100], Step [207/211], Loss: 0.3326\n",
            "Epoch [37/100], Step [208/211], Loss: 0.3467\n",
            "Epoch [37/100], Step [209/211], Loss: 0.3260\n",
            "Epoch [37/100], Step [210/211], Loss: 0.2183\n",
            "Epoch [37/100], Step [211/211], Loss: 0.3271\n",
            "Loss media: 0.2764\n",
            "Epoch [38/100], Step [1/211], Loss: 0.2531\n",
            "Epoch [38/100], Step [2/211], Loss: 0.1745\n",
            "Epoch [38/100], Step [3/211], Loss: 0.3116\n",
            "Epoch [38/100], Step [4/211], Loss: 0.2429\n",
            "Epoch [38/100], Step [5/211], Loss: 0.3677\n",
            "Epoch [38/100], Step [6/211], Loss: 0.1996\n",
            "Epoch [38/100], Step [7/211], Loss: 0.2222\n",
            "Epoch [38/100], Step [8/211], Loss: 0.3031\n",
            "Epoch [38/100], Step [9/211], Loss: 0.2907\n",
            "Epoch [38/100], Step [10/211], Loss: 0.1650\n",
            "Epoch [38/100], Step [11/211], Loss: 0.3394\n",
            "Epoch [38/100], Step [12/211], Loss: 0.2305\n",
            "Epoch [38/100], Step [13/211], Loss: 0.2656\n",
            "Epoch [38/100], Step [14/211], Loss: 0.2951\n",
            "Epoch [38/100], Step [15/211], Loss: 0.1814\n",
            "Epoch [38/100], Step [16/211], Loss: 0.2317\n",
            "Epoch [38/100], Step [17/211], Loss: 0.2645\n",
            "Epoch [38/100], Step [18/211], Loss: 0.3061\n",
            "Epoch [38/100], Step [19/211], Loss: 0.1681\n",
            "Epoch [38/100], Step [20/211], Loss: 0.3193\n",
            "Epoch [38/100], Step [21/211], Loss: 0.4780\n",
            "Epoch [38/100], Step [22/211], Loss: 0.1385\n",
            "Epoch [38/100], Step [23/211], Loss: 0.3008\n",
            "Epoch [38/100], Step [24/211], Loss: 0.4372\n",
            "Epoch [38/100], Step [25/211], Loss: 0.2126\n",
            "Epoch [38/100], Step [26/211], Loss: 0.1096\n",
            "Epoch [38/100], Step [27/211], Loss: 0.4022\n",
            "Epoch [38/100], Step [28/211], Loss: 0.3375\n",
            "Epoch [38/100], Step [29/211], Loss: 0.3191\n",
            "Epoch [38/100], Step [30/211], Loss: 0.2151\n",
            "Epoch [38/100], Step [31/211], Loss: 0.2073\n",
            "Epoch [38/100], Step [32/211], Loss: 0.2994\n",
            "Epoch [38/100], Step [33/211], Loss: 0.4685\n",
            "Epoch [38/100], Step [34/211], Loss: 0.2990\n",
            "Epoch [38/100], Step [35/211], Loss: 0.2432\n",
            "Epoch [38/100], Step [36/211], Loss: 0.1687\n",
            "Epoch [38/100], Step [37/211], Loss: 0.2531\n",
            "Epoch [38/100], Step [38/211], Loss: 0.4046\n",
            "Epoch [38/100], Step [39/211], Loss: 0.2868\n",
            "Epoch [38/100], Step [40/211], Loss: 0.3926\n",
            "Epoch [38/100], Step [41/211], Loss: 0.2783\n",
            "Epoch [38/100], Step [42/211], Loss: 0.1197\n",
            "Epoch [38/100], Step [43/211], Loss: 0.3494\n",
            "Epoch [38/100], Step [44/211], Loss: 0.3372\n",
            "Epoch [38/100], Step [45/211], Loss: 0.2804\n",
            "Epoch [38/100], Step [46/211], Loss: 0.2601\n",
            "Epoch [38/100], Step [47/211], Loss: 0.1930\n",
            "Epoch [38/100], Step [48/211], Loss: 0.0915\n",
            "Epoch [38/100], Step [49/211], Loss: 0.1039\n",
            "Epoch [38/100], Step [50/211], Loss: 0.1925\n",
            "Epoch [38/100], Step [51/211], Loss: 0.1766\n",
            "Epoch [38/100], Step [52/211], Loss: 0.1844\n",
            "Epoch [38/100], Step [53/211], Loss: 0.2698\n",
            "Epoch [38/100], Step [54/211], Loss: 0.1147\n",
            "Epoch [38/100], Step [55/211], Loss: 0.2370\n",
            "Epoch [38/100], Step [56/211], Loss: 0.2450\n",
            "Epoch [38/100], Step [57/211], Loss: 0.2312\n",
            "Epoch [38/100], Step [58/211], Loss: 0.1028\n",
            "Epoch [38/100], Step [59/211], Loss: 0.1326\n",
            "Epoch [38/100], Step [60/211], Loss: 0.2388\n",
            "Epoch [38/100], Step [61/211], Loss: 0.1978\n",
            "Epoch [38/100], Step [62/211], Loss: 0.2805\n",
            "Epoch [38/100], Step [63/211], Loss: 0.1640\n",
            "Epoch [38/100], Step [64/211], Loss: 0.2486\n",
            "Epoch [38/100], Step [65/211], Loss: 0.1612\n",
            "Epoch [38/100], Step [66/211], Loss: 0.2359\n",
            "Epoch [38/100], Step [67/211], Loss: 0.1374\n",
            "Epoch [38/100], Step [68/211], Loss: 0.0985\n",
            "Epoch [38/100], Step [69/211], Loss: 0.1347\n",
            "Epoch [38/100], Step [70/211], Loss: 0.1021\n",
            "Epoch [38/100], Step [71/211], Loss: 0.2933\n",
            "Epoch [38/100], Step [72/211], Loss: 0.4918\n",
            "Epoch [38/100], Step [73/211], Loss: 0.3307\n",
            "Epoch [38/100], Step [74/211], Loss: 0.4301\n",
            "Epoch [38/100], Step [75/211], Loss: 0.1942\n",
            "Epoch [38/100], Step [76/211], Loss: 0.2794\n",
            "Epoch [38/100], Step [77/211], Loss: 0.2361\n",
            "Epoch [38/100], Step [78/211], Loss: 0.1185\n",
            "Epoch [38/100], Step [79/211], Loss: 0.3980\n",
            "Epoch [38/100], Step [80/211], Loss: 0.1707\n",
            "Epoch [38/100], Step [81/211], Loss: 0.3698\n",
            "Epoch [38/100], Step [82/211], Loss: 0.3398\n",
            "Epoch [38/100], Step [83/211], Loss: 0.1673\n",
            "Epoch [38/100], Step [84/211], Loss: 0.2658\n",
            "Epoch [38/100], Step [85/211], Loss: 0.2633\n",
            "Epoch [38/100], Step [86/211], Loss: 0.3225\n",
            "Epoch [38/100], Step [87/211], Loss: 0.3971\n",
            "Epoch [38/100], Step [88/211], Loss: 0.3134\n",
            "Epoch [38/100], Step [89/211], Loss: 0.3344\n",
            "Epoch [38/100], Step [90/211], Loss: 0.3543\n",
            "Epoch [38/100], Step [91/211], Loss: 0.2758\n",
            "Epoch [38/100], Step [92/211], Loss: 0.3428\n",
            "Epoch [38/100], Step [93/211], Loss: 0.2973\n",
            "Epoch [38/100], Step [94/211], Loss: 0.3263\n",
            "Epoch [38/100], Step [95/211], Loss: 0.4164\n",
            "Epoch [38/100], Step [96/211], Loss: 0.1266\n",
            "Epoch [38/100], Step [97/211], Loss: 0.3896\n",
            "Epoch [38/100], Step [98/211], Loss: 0.5622\n",
            "Epoch [38/100], Step [99/211], Loss: 0.1980\n",
            "Epoch [38/100], Step [100/211], Loss: 0.4652\n",
            "Epoch [38/100], Step [101/211], Loss: 0.1787\n",
            "Epoch [38/100], Step [102/211], Loss: 0.3916\n",
            "Epoch [38/100], Step [103/211], Loss: 0.2223\n",
            "Epoch [38/100], Step [104/211], Loss: 0.3440\n",
            "Epoch [38/100], Step [105/211], Loss: 0.5410\n",
            "Epoch [38/100], Step [106/211], Loss: 0.1234\n",
            "Epoch [38/100], Step [107/211], Loss: 0.2257\n",
            "Epoch [38/100], Step [108/211], Loss: 0.3629\n",
            "Epoch [38/100], Step [109/211], Loss: 0.4534\n",
            "Epoch [38/100], Step [110/211], Loss: 0.2599\n",
            "Epoch [38/100], Step [111/211], Loss: 0.2191\n",
            "Epoch [38/100], Step [112/211], Loss: 0.1363\n",
            "Epoch [38/100], Step [113/211], Loss: 0.2113\n",
            "Epoch [38/100], Step [114/211], Loss: 0.3472\n",
            "Epoch [38/100], Step [115/211], Loss: 0.0975\n",
            "Epoch [38/100], Step [116/211], Loss: 0.1678\n",
            "Epoch [38/100], Step [117/211], Loss: 0.3178\n",
            "Epoch [38/100], Step [118/211], Loss: 0.1743\n",
            "Epoch [38/100], Step [119/211], Loss: 0.2576\n",
            "Epoch [38/100], Step [120/211], Loss: 0.3580\n",
            "Epoch [38/100], Step [121/211], Loss: 0.1823\n",
            "Epoch [38/100], Step [122/211], Loss: 0.1437\n",
            "Epoch [38/100], Step [123/211], Loss: 0.3653\n",
            "Epoch [38/100], Step [124/211], Loss: 0.2355\n",
            "Epoch [38/100], Step [125/211], Loss: 0.2648\n",
            "Epoch [38/100], Step [126/211], Loss: 0.4712\n",
            "Epoch [38/100], Step [127/211], Loss: 0.2761\n",
            "Epoch [38/100], Step [128/211], Loss: 0.2656\n",
            "Epoch [38/100], Step [129/211], Loss: 0.4151\n",
            "Epoch [38/100], Step [130/211], Loss: 0.2357\n",
            "Epoch [38/100], Step [131/211], Loss: 0.3822\n",
            "Epoch [38/100], Step [132/211], Loss: 0.3066\n",
            "Epoch [38/100], Step [133/211], Loss: 0.2556\n",
            "Epoch [38/100], Step [134/211], Loss: 0.3157\n",
            "Epoch [38/100], Step [135/211], Loss: 0.2480\n",
            "Epoch [38/100], Step [136/211], Loss: 0.1346\n",
            "Epoch [38/100], Step [137/211], Loss: 0.5161\n",
            "Epoch [38/100], Step [138/211], Loss: 0.1933\n",
            "Epoch [38/100], Step [139/211], Loss: 0.2489\n",
            "Epoch [38/100], Step [140/211], Loss: 0.2634\n",
            "Epoch [38/100], Step [141/211], Loss: 0.1870\n",
            "Epoch [38/100], Step [142/211], Loss: 0.2143\n",
            "Epoch [38/100], Step [143/211], Loss: 0.4363\n",
            "Epoch [38/100], Step [144/211], Loss: 0.0973\n",
            "Epoch [38/100], Step [145/211], Loss: 0.3599\n",
            "Epoch [38/100], Step [146/211], Loss: 0.1353\n",
            "Epoch [38/100], Step [147/211], Loss: 0.2025\n",
            "Epoch [38/100], Step [148/211], Loss: 0.1629\n",
            "Epoch [38/100], Step [149/211], Loss: 0.2885\n",
            "Epoch [38/100], Step [150/211], Loss: 0.5733\n",
            "Epoch [38/100], Step [151/211], Loss: 0.1711\n",
            "Epoch [38/100], Step [152/211], Loss: 0.4281\n",
            "Epoch [38/100], Step [153/211], Loss: 0.1526\n",
            "Epoch [38/100], Step [154/211], Loss: 0.3103\n",
            "Epoch [38/100], Step [155/211], Loss: 0.2833\n",
            "Epoch [38/100], Step [156/211], Loss: 0.2041\n",
            "Epoch [38/100], Step [157/211], Loss: 0.3305\n",
            "Epoch [38/100], Step [158/211], Loss: 0.2385\n",
            "Epoch [38/100], Step [159/211], Loss: 0.2145\n",
            "Epoch [38/100], Step [160/211], Loss: 0.2876\n",
            "Epoch [38/100], Step [161/211], Loss: 0.3208\n",
            "Epoch [38/100], Step [162/211], Loss: 0.7333\n",
            "Epoch [38/100], Step [163/211], Loss: 0.2142\n",
            "Epoch [38/100], Step [164/211], Loss: 0.1635\n",
            "Epoch [38/100], Step [165/211], Loss: 0.1030\n",
            "Epoch [38/100], Step [166/211], Loss: 0.5280\n",
            "Epoch [38/100], Step [167/211], Loss: 0.4048\n",
            "Epoch [38/100], Step [168/211], Loss: 0.2205\n",
            "Epoch [38/100], Step [169/211], Loss: 0.1553\n",
            "Epoch [38/100], Step [170/211], Loss: 0.1451\n",
            "Epoch [38/100], Step [171/211], Loss: 0.0991\n",
            "Epoch [38/100], Step [172/211], Loss: 0.2684\n",
            "Epoch [38/100], Step [173/211], Loss: 0.3468\n",
            "Epoch [38/100], Step [174/211], Loss: 0.1616\n",
            "Epoch [38/100], Step [175/211], Loss: 0.1615\n",
            "Epoch [38/100], Step [176/211], Loss: 0.5116\n",
            "Epoch [38/100], Step [177/211], Loss: 0.3294\n",
            "Epoch [38/100], Step [178/211], Loss: 0.2902\n",
            "Epoch [38/100], Step [179/211], Loss: 0.1866\n",
            "Epoch [38/100], Step [180/211], Loss: 0.2169\n",
            "Epoch [38/100], Step [181/211], Loss: 0.5326\n",
            "Epoch [38/100], Step [182/211], Loss: 0.3535\n",
            "Epoch [38/100], Step [183/211], Loss: 0.3150\n",
            "Epoch [38/100], Step [184/211], Loss: 0.1126\n",
            "Epoch [38/100], Step [185/211], Loss: 0.3035\n",
            "Epoch [38/100], Step [186/211], Loss: 0.1425\n",
            "Epoch [38/100], Step [187/211], Loss: 0.6196\n",
            "Epoch [38/100], Step [188/211], Loss: 0.3522\n",
            "Epoch [38/100], Step [189/211], Loss: 0.2568\n",
            "Epoch [38/100], Step [190/211], Loss: 0.3316\n",
            "Epoch [38/100], Step [191/211], Loss: 0.1927\n",
            "Epoch [38/100], Step [192/211], Loss: 0.5770\n",
            "Epoch [38/100], Step [193/211], Loss: 0.2740\n",
            "Epoch [38/100], Step [194/211], Loss: 0.4597\n",
            "Epoch [38/100], Step [195/211], Loss: 0.4012\n",
            "Epoch [38/100], Step [196/211], Loss: 0.2536\n",
            "Epoch [38/100], Step [197/211], Loss: 0.2752\n",
            "Epoch [38/100], Step [198/211], Loss: 0.1466\n",
            "Epoch [38/100], Step [199/211], Loss: 0.2915\n",
            "Epoch [38/100], Step [200/211], Loss: 0.1768\n",
            "Epoch [38/100], Step [201/211], Loss: 0.5310\n",
            "Epoch [38/100], Step [202/211], Loss: 0.2103\n",
            "Epoch [38/100], Step [203/211], Loss: 0.3430\n",
            "Epoch [38/100], Step [204/211], Loss: 0.2234\n",
            "Epoch [38/100], Step [205/211], Loss: 0.1689\n",
            "Epoch [38/100], Step [206/211], Loss: 0.2147\n",
            "Epoch [38/100], Step [207/211], Loss: 0.2057\n",
            "Epoch [38/100], Step [208/211], Loss: 0.1889\n",
            "Epoch [38/100], Step [209/211], Loss: 0.4827\n",
            "Epoch [38/100], Step [210/211], Loss: 0.0762\n",
            "Epoch [38/100], Step [211/211], Loss: 0.4966\n",
            "Loss media: 0.2746\n",
            "Epoch [39/100], Step [1/211], Loss: 0.4535\n",
            "Epoch [39/100], Step [2/211], Loss: 0.1593\n",
            "Epoch [39/100], Step [3/211], Loss: 0.1143\n",
            "Epoch [39/100], Step [4/211], Loss: 0.3255\n",
            "Epoch [39/100], Step [5/211], Loss: 0.2226\n",
            "Epoch [39/100], Step [6/211], Loss: 0.8940\n",
            "Epoch [39/100], Step [7/211], Loss: 0.3878\n",
            "Epoch [39/100], Step [8/211], Loss: 0.2710\n",
            "Epoch [39/100], Step [9/211], Loss: 0.3824\n",
            "Epoch [39/100], Step [10/211], Loss: 0.3261\n",
            "Epoch [39/100], Step [11/211], Loss: 0.1602\n",
            "Epoch [39/100], Step [12/211], Loss: 0.0931\n",
            "Epoch [39/100], Step [13/211], Loss: 0.3246\n",
            "Epoch [39/100], Step [14/211], Loss: 0.2721\n",
            "Epoch [39/100], Step [15/211], Loss: 0.4497\n",
            "Epoch [39/100], Step [16/211], Loss: 0.5628\n",
            "Epoch [39/100], Step [17/211], Loss: 0.1675\n",
            "Epoch [39/100], Step [18/211], Loss: 0.1055\n",
            "Epoch [39/100], Step [19/211], Loss: 0.2755\n",
            "Epoch [39/100], Step [20/211], Loss: 0.4091\n",
            "Epoch [39/100], Step [21/211], Loss: 0.0831\n",
            "Epoch [39/100], Step [22/211], Loss: 0.2755\n",
            "Epoch [39/100], Step [23/211], Loss: 0.3852\n",
            "Epoch [39/100], Step [24/211], Loss: 0.3287\n",
            "Epoch [39/100], Step [25/211], Loss: 0.1068\n",
            "Epoch [39/100], Step [26/211], Loss: 0.2621\n",
            "Epoch [39/100], Step [27/211], Loss: 0.4009\n",
            "Epoch [39/100], Step [28/211], Loss: 0.1474\n",
            "Epoch [39/100], Step [29/211], Loss: 0.3801\n",
            "Epoch [39/100], Step [30/211], Loss: 0.2802\n",
            "Epoch [39/100], Step [31/211], Loss: 0.2333\n",
            "Epoch [39/100], Step [32/211], Loss: 0.4760\n",
            "Epoch [39/100], Step [33/211], Loss: 0.2054\n",
            "Epoch [39/100], Step [34/211], Loss: 0.1379\n",
            "Epoch [39/100], Step [35/211], Loss: 0.2182\n",
            "Epoch [39/100], Step [36/211], Loss: 0.2683\n",
            "Epoch [39/100], Step [37/211], Loss: 0.1420\n",
            "Epoch [39/100], Step [38/211], Loss: 0.2364\n",
            "Epoch [39/100], Step [39/211], Loss: 0.2347\n",
            "Epoch [39/100], Step [40/211], Loss: 0.2929\n",
            "Epoch [39/100], Step [41/211], Loss: 0.3265\n",
            "Epoch [39/100], Step [42/211], Loss: 0.4121\n",
            "Epoch [39/100], Step [43/211], Loss: 0.1944\n",
            "Epoch [39/100], Step [44/211], Loss: 0.4113\n",
            "Epoch [39/100], Step [45/211], Loss: 0.1342\n",
            "Epoch [39/100], Step [46/211], Loss: 0.1664\n",
            "Epoch [39/100], Step [47/211], Loss: 0.3749\n",
            "Epoch [39/100], Step [48/211], Loss: 0.2524\n",
            "Epoch [39/100], Step [49/211], Loss: 0.2354\n",
            "Epoch [39/100], Step [50/211], Loss: 0.1668\n",
            "Epoch [39/100], Step [51/211], Loss: 0.1670\n",
            "Epoch [39/100], Step [52/211], Loss: 0.2537\n",
            "Epoch [39/100], Step [53/211], Loss: 0.2472\n",
            "Epoch [39/100], Step [54/211], Loss: 0.4577\n",
            "Epoch [39/100], Step [55/211], Loss: 0.3251\n",
            "Epoch [39/100], Step [56/211], Loss: 0.5250\n",
            "Epoch [39/100], Step [57/211], Loss: 0.2512\n",
            "Epoch [39/100], Step [58/211], Loss: 0.2063\n",
            "Epoch [39/100], Step [59/211], Loss: 0.2338\n",
            "Epoch [39/100], Step [60/211], Loss: 0.1893\n",
            "Epoch [39/100], Step [61/211], Loss: 0.1667\n",
            "Epoch [39/100], Step [62/211], Loss: 0.4841\n",
            "Epoch [39/100], Step [63/211], Loss: 0.4086\n",
            "Epoch [39/100], Step [64/211], Loss: 0.2047\n",
            "Epoch [39/100], Step [65/211], Loss: 0.1428\n",
            "Epoch [39/100], Step [66/211], Loss: 0.1119\n",
            "Epoch [39/100], Step [67/211], Loss: 0.2543\n",
            "Epoch [39/100], Step [68/211], Loss: 0.2252\n",
            "Epoch [39/100], Step [69/211], Loss: 0.6975\n",
            "Epoch [39/100], Step [70/211], Loss: 0.2576\n",
            "Epoch [39/100], Step [71/211], Loss: 0.2083\n",
            "Epoch [39/100], Step [72/211], Loss: 0.2067\n",
            "Epoch [39/100], Step [73/211], Loss: 0.2143\n",
            "Epoch [39/100], Step [74/211], Loss: 0.4878\n",
            "Epoch [39/100], Step [75/211], Loss: 0.2189\n",
            "Epoch [39/100], Step [76/211], Loss: 0.2995\n",
            "Epoch [39/100], Step [77/211], Loss: 0.2394\n",
            "Epoch [39/100], Step [78/211], Loss: 0.4773\n",
            "Epoch [39/100], Step [79/211], Loss: 0.2862\n",
            "Epoch [39/100], Step [80/211], Loss: 0.1889\n",
            "Epoch [39/100], Step [81/211], Loss: 0.3723\n",
            "Epoch [39/100], Step [82/211], Loss: 0.2374\n",
            "Epoch [39/100], Step [83/211], Loss: 0.2712\n",
            "Epoch [39/100], Step [84/211], Loss: 0.1515\n",
            "Epoch [39/100], Step [85/211], Loss: 0.2083\n",
            "Epoch [39/100], Step [86/211], Loss: 0.1810\n",
            "Epoch [39/100], Step [87/211], Loss: 0.2348\n",
            "Epoch [39/100], Step [88/211], Loss: 0.1630\n",
            "Epoch [39/100], Step [89/211], Loss: 0.2926\n",
            "Epoch [39/100], Step [90/211], Loss: 0.1089\n",
            "Epoch [39/100], Step [91/211], Loss: 0.3759\n",
            "Epoch [39/100], Step [92/211], Loss: 0.2126\n",
            "Epoch [39/100], Step [93/211], Loss: 0.1444\n",
            "Epoch [39/100], Step [94/211], Loss: 0.2740\n",
            "Epoch [39/100], Step [95/211], Loss: 0.4876\n",
            "Epoch [39/100], Step [96/211], Loss: 0.1614\n",
            "Epoch [39/100], Step [97/211], Loss: 0.1695\n",
            "Epoch [39/100], Step [98/211], Loss: 0.2672\n",
            "Epoch [39/100], Step [99/211], Loss: 0.1183\n",
            "Epoch [39/100], Step [100/211], Loss: 0.1185\n",
            "Epoch [39/100], Step [101/211], Loss: 0.1637\n",
            "Epoch [39/100], Step [102/211], Loss: 0.2621\n",
            "Epoch [39/100], Step [103/211], Loss: 0.1120\n",
            "Epoch [39/100], Step [104/211], Loss: 0.2309\n",
            "Epoch [39/100], Step [105/211], Loss: 0.3401\n",
            "Epoch [39/100], Step [106/211], Loss: 0.2201\n",
            "Epoch [39/100], Step [107/211], Loss: 0.3215\n",
            "Epoch [39/100], Step [108/211], Loss: 0.1429\n",
            "Epoch [39/100], Step [109/211], Loss: 0.1843\n",
            "Epoch [39/100], Step [110/211], Loss: 0.3047\n",
            "Epoch [39/100], Step [111/211], Loss: 0.4388\n",
            "Epoch [39/100], Step [112/211], Loss: 0.4530\n",
            "Epoch [39/100], Step [113/211], Loss: 0.3433\n",
            "Epoch [39/100], Step [114/211], Loss: 0.2461\n",
            "Epoch [39/100], Step [115/211], Loss: 0.2980\n",
            "Epoch [39/100], Step [116/211], Loss: 0.1652\n",
            "Epoch [39/100], Step [117/211], Loss: 0.2538\n",
            "Epoch [39/100], Step [118/211], Loss: 0.4119\n",
            "Epoch [39/100], Step [119/211], Loss: 0.2074\n",
            "Epoch [39/100], Step [120/211], Loss: 0.2439\n",
            "Epoch [39/100], Step [121/211], Loss: 0.2082\n",
            "Epoch [39/100], Step [122/211], Loss: 0.2208\n",
            "Epoch [39/100], Step [123/211], Loss: 0.2540\n",
            "Epoch [39/100], Step [124/211], Loss: 0.2730\n",
            "Epoch [39/100], Step [125/211], Loss: 0.5107\n",
            "Epoch [39/100], Step [126/211], Loss: 0.2316\n",
            "Epoch [39/100], Step [127/211], Loss: 0.1742\n",
            "Epoch [39/100], Step [128/211], Loss: 0.6070\n",
            "Epoch [39/100], Step [129/211], Loss: 0.2135\n",
            "Epoch [39/100], Step [130/211], Loss: 0.3262\n",
            "Epoch [39/100], Step [131/211], Loss: 0.1758\n",
            "Epoch [39/100], Step [132/211], Loss: 0.4552\n",
            "Epoch [39/100], Step [133/211], Loss: 0.2949\n",
            "Epoch [39/100], Step [134/211], Loss: 0.2649\n",
            "Epoch [39/100], Step [135/211], Loss: 0.2738\n",
            "Epoch [39/100], Step [136/211], Loss: 0.1264\n",
            "Epoch [39/100], Step [137/211], Loss: 0.2449\n",
            "Epoch [39/100], Step [138/211], Loss: 0.3613\n",
            "Epoch [39/100], Step [139/211], Loss: 0.1586\n",
            "Epoch [39/100], Step [140/211], Loss: 0.2750\n",
            "Epoch [39/100], Step [141/211], Loss: 0.2668\n",
            "Epoch [39/100], Step [142/211], Loss: 0.2181\n",
            "Epoch [39/100], Step [143/211], Loss: 0.2308\n",
            "Epoch [39/100], Step [144/211], Loss: 0.3578\n",
            "Epoch [39/100], Step [145/211], Loss: 0.1462\n",
            "Epoch [39/100], Step [146/211], Loss: 0.2074\n",
            "Epoch [39/100], Step [147/211], Loss: 0.3146\n",
            "Epoch [39/100], Step [148/211], Loss: 0.2982\n",
            "Epoch [39/100], Step [149/211], Loss: 0.3264\n",
            "Epoch [39/100], Step [150/211], Loss: 0.3696\n",
            "Epoch [39/100], Step [151/211], Loss: 0.2678\n",
            "Epoch [39/100], Step [152/211], Loss: 0.2468\n",
            "Epoch [39/100], Step [153/211], Loss: 0.1913\n",
            "Epoch [39/100], Step [154/211], Loss: 0.2790\n",
            "Epoch [39/100], Step [155/211], Loss: 0.2677\n",
            "Epoch [39/100], Step [156/211], Loss: 0.2567\n",
            "Epoch [39/100], Step [157/211], Loss: 0.3636\n",
            "Epoch [39/100], Step [158/211], Loss: 0.3546\n",
            "Epoch [39/100], Step [159/211], Loss: 0.1965\n",
            "Epoch [39/100], Step [160/211], Loss: 0.3074\n",
            "Epoch [39/100], Step [161/211], Loss: 0.3438\n",
            "Epoch [39/100], Step [162/211], Loss: 0.1348\n",
            "Epoch [39/100], Step [163/211], Loss: 0.5405\n",
            "Epoch [39/100], Step [164/211], Loss: 0.1893\n",
            "Epoch [39/100], Step [165/211], Loss: 0.3966\n",
            "Epoch [39/100], Step [166/211], Loss: 0.2571\n",
            "Epoch [39/100], Step [167/211], Loss: 0.3679\n",
            "Epoch [39/100], Step [168/211], Loss: 0.1827\n",
            "Epoch [39/100], Step [169/211], Loss: 0.3501\n",
            "Epoch [39/100], Step [170/211], Loss: 0.1503\n",
            "Epoch [39/100], Step [171/211], Loss: 0.3089\n",
            "Epoch [39/100], Step [172/211], Loss: 0.2909\n",
            "Epoch [39/100], Step [173/211], Loss: 0.1860\n",
            "Epoch [39/100], Step [174/211], Loss: 0.2964\n",
            "Epoch [39/100], Step [175/211], Loss: 0.2841\n",
            "Epoch [39/100], Step [176/211], Loss: 0.1493\n",
            "Epoch [39/100], Step [177/211], Loss: 0.1890\n",
            "Epoch [39/100], Step [178/211], Loss: 0.4205\n",
            "Epoch [39/100], Step [179/211], Loss: 0.4225\n",
            "Epoch [39/100], Step [180/211], Loss: 0.2069\n",
            "Epoch [39/100], Step [181/211], Loss: 0.3759\n",
            "Epoch [39/100], Step [182/211], Loss: 0.0930\n",
            "Epoch [39/100], Step [183/211], Loss: 0.3567\n",
            "Epoch [39/100], Step [184/211], Loss: 0.2570\n",
            "Epoch [39/100], Step [185/211], Loss: 0.4666\n",
            "Epoch [39/100], Step [186/211], Loss: 0.1366\n",
            "Epoch [39/100], Step [187/211], Loss: 0.1848\n",
            "Epoch [39/100], Step [188/211], Loss: 0.4207\n",
            "Epoch [39/100], Step [189/211], Loss: 0.1515\n",
            "Epoch [39/100], Step [190/211], Loss: 0.3016\n",
            "Epoch [39/100], Step [191/211], Loss: 0.2527\n",
            "Epoch [39/100], Step [192/211], Loss: 0.1680\n",
            "Epoch [39/100], Step [193/211], Loss: 0.4667\n",
            "Epoch [39/100], Step [194/211], Loss: 0.1854\n",
            "Epoch [39/100], Step [195/211], Loss: 0.1674\n",
            "Epoch [39/100], Step [196/211], Loss: 0.3208\n",
            "Epoch [39/100], Step [197/211], Loss: 0.2582\n",
            "Epoch [39/100], Step [198/211], Loss: 0.3354\n",
            "Epoch [39/100], Step [199/211], Loss: 0.1959\n",
            "Epoch [39/100], Step [200/211], Loss: 0.3586\n",
            "Epoch [39/100], Step [201/211], Loss: 0.4504\n",
            "Epoch [39/100], Step [202/211], Loss: 0.2068\n",
            "Epoch [39/100], Step [203/211], Loss: 0.5306\n",
            "Epoch [39/100], Step [204/211], Loss: 0.1233\n",
            "Epoch [39/100], Step [205/211], Loss: 0.0820\n",
            "Epoch [39/100], Step [206/211], Loss: 0.1820\n",
            "Epoch [39/100], Step [207/211], Loss: 0.2447\n",
            "Epoch [39/100], Step [208/211], Loss: 0.2359\n",
            "Epoch [39/100], Step [209/211], Loss: 0.1891\n",
            "Epoch [39/100], Step [210/211], Loss: 0.2220\n",
            "Epoch [39/100], Step [211/211], Loss: 0.2315\n",
            "Loss media: 0.2730\n",
            "Epoch [40/100], Step [1/211], Loss: 0.1267\n",
            "Epoch [40/100], Step [2/211], Loss: 0.5025\n",
            "Epoch [40/100], Step [3/211], Loss: 0.2169\n",
            "Epoch [40/100], Step [4/211], Loss: 0.2905\n",
            "Epoch [40/100], Step [5/211], Loss: 0.5657\n",
            "Epoch [40/100], Step [6/211], Loss: 0.2886\n",
            "Epoch [40/100], Step [7/211], Loss: 0.1597\n",
            "Epoch [40/100], Step [8/211], Loss: 0.2006\n",
            "Epoch [40/100], Step [9/211], Loss: 0.1157\n",
            "Epoch [40/100], Step [10/211], Loss: 0.3650\n",
            "Epoch [40/100], Step [11/211], Loss: 0.1866\n",
            "Epoch [40/100], Step [12/211], Loss: 0.2749\n",
            "Epoch [40/100], Step [13/211], Loss: 0.1533\n",
            "Epoch [40/100], Step [14/211], Loss: 0.2513\n",
            "Epoch [40/100], Step [15/211], Loss: 0.5727\n",
            "Epoch [40/100], Step [16/211], Loss: 0.2336\n",
            "Epoch [40/100], Step [17/211], Loss: 0.2888\n",
            "Epoch [40/100], Step [18/211], Loss: 0.3522\n",
            "Epoch [40/100], Step [19/211], Loss: 0.3861\n",
            "Epoch [40/100], Step [20/211], Loss: 0.1556\n",
            "Epoch [40/100], Step [21/211], Loss: 0.1970\n",
            "Epoch [40/100], Step [22/211], Loss: 0.3333\n",
            "Epoch [40/100], Step [23/211], Loss: 0.3754\n",
            "Epoch [40/100], Step [24/211], Loss: 0.3646\n",
            "Epoch [40/100], Step [25/211], Loss: 0.3109\n",
            "Epoch [40/100], Step [26/211], Loss: 0.2853\n",
            "Epoch [40/100], Step [27/211], Loss: 0.3185\n",
            "Epoch [40/100], Step [28/211], Loss: 0.4707\n",
            "Epoch [40/100], Step [29/211], Loss: 0.0819\n",
            "Epoch [40/100], Step [30/211], Loss: 0.4305\n",
            "Epoch [40/100], Step [31/211], Loss: 0.4286\n",
            "Epoch [40/100], Step [32/211], Loss: 0.2741\n",
            "Epoch [40/100], Step [33/211], Loss: 0.2322\n",
            "Epoch [40/100], Step [34/211], Loss: 0.1460\n",
            "Epoch [40/100], Step [35/211], Loss: 0.2658\n",
            "Epoch [40/100], Step [36/211], Loss: 0.2807\n",
            "Epoch [40/100], Step [37/211], Loss: 0.2555\n",
            "Epoch [40/100], Step [38/211], Loss: 0.2404\n",
            "Epoch [40/100], Step [39/211], Loss: 0.2436\n",
            "Epoch [40/100], Step [40/211], Loss: 0.6195\n",
            "Epoch [40/100], Step [41/211], Loss: 0.2466\n",
            "Epoch [40/100], Step [42/211], Loss: 0.3234\n",
            "Epoch [40/100], Step [43/211], Loss: 0.2740\n",
            "Epoch [40/100], Step [44/211], Loss: 0.2310\n",
            "Epoch [40/100], Step [45/211], Loss: 0.1677\n",
            "Epoch [40/100], Step [46/211], Loss: 0.2632\n",
            "Epoch [40/100], Step [47/211], Loss: 0.0955\n",
            "Epoch [40/100], Step [48/211], Loss: 0.4572\n",
            "Epoch [40/100], Step [49/211], Loss: 0.2867\n",
            "Epoch [40/100], Step [50/211], Loss: 0.2531\n",
            "Epoch [40/100], Step [51/211], Loss: 0.3403\n",
            "Epoch [40/100], Step [52/211], Loss: 0.1496\n",
            "Epoch [40/100], Step [53/211], Loss: 0.2421\n",
            "Epoch [40/100], Step [54/211], Loss: 0.2677\n",
            "Epoch [40/100], Step [55/211], Loss: 0.2968\n",
            "Epoch [40/100], Step [56/211], Loss: 0.1711\n",
            "Epoch [40/100], Step [57/211], Loss: 0.5512\n",
            "Epoch [40/100], Step [58/211], Loss: 0.2804\n",
            "Epoch [40/100], Step [59/211], Loss: 0.2824\n",
            "Epoch [40/100], Step [60/211], Loss: 0.4421\n",
            "Epoch [40/100], Step [61/211], Loss: 0.1629\n",
            "Epoch [40/100], Step [62/211], Loss: 0.2217\n",
            "Epoch [40/100], Step [63/211], Loss: 0.1734\n",
            "Epoch [40/100], Step [64/211], Loss: 0.1647\n",
            "Epoch [40/100], Step [65/211], Loss: 0.2598\n",
            "Epoch [40/100], Step [66/211], Loss: 0.1257\n",
            "Epoch [40/100], Step [67/211], Loss: 0.2237\n",
            "Epoch [40/100], Step [68/211], Loss: 0.1739\n",
            "Epoch [40/100], Step [69/211], Loss: 0.1298\n",
            "Epoch [40/100], Step [70/211], Loss: 0.1606\n",
            "Epoch [40/100], Step [71/211], Loss: 0.2220\n",
            "Epoch [40/100], Step [72/211], Loss: 0.1188\n",
            "Epoch [40/100], Step [73/211], Loss: 0.1751\n",
            "Epoch [40/100], Step [74/211], Loss: 0.2201\n",
            "Epoch [40/100], Step [75/211], Loss: 0.2704\n",
            "Epoch [40/100], Step [76/211], Loss: 0.3043\n",
            "Epoch [40/100], Step [77/211], Loss: 0.1727\n",
            "Epoch [40/100], Step [78/211], Loss: 0.2820\n",
            "Epoch [40/100], Step [79/211], Loss: 0.1590\n",
            "Epoch [40/100], Step [80/211], Loss: 0.2480\n",
            "Epoch [40/100], Step [81/211], Loss: 0.3048\n",
            "Epoch [40/100], Step [82/211], Loss: 0.2227\n",
            "Epoch [40/100], Step [83/211], Loss: 0.2557\n",
            "Epoch [40/100], Step [84/211], Loss: 0.2593\n",
            "Epoch [40/100], Step [85/211], Loss: 0.2468\n",
            "Epoch [40/100], Step [86/211], Loss: 0.1520\n",
            "Epoch [40/100], Step [87/211], Loss: 0.3773\n",
            "Epoch [40/100], Step [88/211], Loss: 0.1249\n",
            "Epoch [40/100], Step [89/211], Loss: 0.2374\n",
            "Epoch [40/100], Step [90/211], Loss: 0.2496\n",
            "Epoch [40/100], Step [91/211], Loss: 0.5623\n",
            "Epoch [40/100], Step [92/211], Loss: 0.3542\n",
            "Epoch [40/100], Step [93/211], Loss: 0.4709\n",
            "Epoch [40/100], Step [94/211], Loss: 0.3371\n",
            "Epoch [40/100], Step [95/211], Loss: 0.1824\n",
            "Epoch [40/100], Step [96/211], Loss: 0.2221\n",
            "Epoch [40/100], Step [97/211], Loss: 0.2156\n",
            "Epoch [40/100], Step [98/211], Loss: 0.5751\n",
            "Epoch [40/100], Step [99/211], Loss: 0.2473\n",
            "Epoch [40/100], Step [100/211], Loss: 0.3926\n",
            "Epoch [40/100], Step [101/211], Loss: 0.3206\n",
            "Epoch [40/100], Step [102/211], Loss: 0.3978\n",
            "Epoch [40/100], Step [103/211], Loss: 0.2283\n",
            "Epoch [40/100], Step [104/211], Loss: 0.2010\n",
            "Epoch [40/100], Step [105/211], Loss: 0.2620\n",
            "Epoch [40/100], Step [106/211], Loss: 0.2320\n",
            "Epoch [40/100], Step [107/211], Loss: 0.5197\n",
            "Epoch [40/100], Step [108/211], Loss: 0.1228\n",
            "Epoch [40/100], Step [109/211], Loss: 0.2824\n",
            "Epoch [40/100], Step [110/211], Loss: 0.2946\n",
            "Epoch [40/100], Step [111/211], Loss: 0.2153\n",
            "Epoch [40/100], Step [112/211], Loss: 0.3916\n",
            "Epoch [40/100], Step [113/211], Loss: 0.3652\n",
            "Epoch [40/100], Step [114/211], Loss: 0.3666\n",
            "Epoch [40/100], Step [115/211], Loss: 0.3061\n",
            "Epoch [40/100], Step [116/211], Loss: 0.2791\n",
            "Epoch [40/100], Step [117/211], Loss: 0.1908\n",
            "Epoch [40/100], Step [118/211], Loss: 0.2691\n",
            "Epoch [40/100], Step [119/211], Loss: 0.4833\n",
            "Epoch [40/100], Step [120/211], Loss: 0.1835\n",
            "Epoch [40/100], Step [121/211], Loss: 0.1736\n",
            "Epoch [40/100], Step [122/211], Loss: 0.2502\n",
            "Epoch [40/100], Step [123/211], Loss: 0.2140\n",
            "Epoch [40/100], Step [124/211], Loss: 0.2592\n",
            "Epoch [40/100], Step [125/211], Loss: 0.2329\n",
            "Epoch [40/100], Step [126/211], Loss: 0.1502\n",
            "Epoch [40/100], Step [127/211], Loss: 0.2863\n",
            "Epoch [40/100], Step [128/211], Loss: 0.2210\n",
            "Epoch [40/100], Step [129/211], Loss: 0.1342\n",
            "Epoch [40/100], Step [130/211], Loss: 0.1434\n",
            "Epoch [40/100], Step [131/211], Loss: 0.3933\n",
            "Epoch [40/100], Step [132/211], Loss: 0.4640\n",
            "Epoch [40/100], Step [133/211], Loss: 0.2150\n",
            "Epoch [40/100], Step [134/211], Loss: 0.1527\n",
            "Epoch [40/100], Step [135/211], Loss: 0.2050\n",
            "Epoch [40/100], Step [136/211], Loss: 0.3882\n",
            "Epoch [40/100], Step [137/211], Loss: 0.2608\n",
            "Epoch [40/100], Step [138/211], Loss: 0.4461\n",
            "Epoch [40/100], Step [139/211], Loss: 0.2341\n",
            "Epoch [40/100], Step [140/211], Loss: 0.2500\n",
            "Epoch [40/100], Step [141/211], Loss: 0.1562\n",
            "Epoch [40/100], Step [142/211], Loss: 0.3762\n",
            "Epoch [40/100], Step [143/211], Loss: 0.6297\n",
            "Epoch [40/100], Step [144/211], Loss: 0.2908\n",
            "Epoch [40/100], Step [145/211], Loss: 0.3272\n",
            "Epoch [40/100], Step [146/211], Loss: 0.2157\n",
            "Epoch [40/100], Step [147/211], Loss: 0.1280\n",
            "Epoch [40/100], Step [148/211], Loss: 0.0911\n",
            "Epoch [40/100], Step [149/211], Loss: 0.1189\n",
            "Epoch [40/100], Step [150/211], Loss: 0.2394\n",
            "Epoch [40/100], Step [151/211], Loss: 0.3025\n",
            "Epoch [40/100], Step [152/211], Loss: 0.3542\n",
            "Epoch [40/100], Step [153/211], Loss: 0.3369\n",
            "Epoch [40/100], Step [154/211], Loss: 0.2244\n",
            "Epoch [40/100], Step [155/211], Loss: 0.3354\n",
            "Epoch [40/100], Step [156/211], Loss: 0.1072\n",
            "Epoch [40/100], Step [157/211], Loss: 0.2194\n",
            "Epoch [40/100], Step [158/211], Loss: 0.2227\n",
            "Epoch [40/100], Step [159/211], Loss: 0.2391\n",
            "Epoch [40/100], Step [160/211], Loss: 0.1438\n",
            "Epoch [40/100], Step [161/211], Loss: 0.5407\n",
            "Epoch [40/100], Step [162/211], Loss: 0.3302\n",
            "Epoch [40/100], Step [163/211], Loss: 0.1978\n",
            "Epoch [40/100], Step [164/211], Loss: 0.5750\n",
            "Epoch [40/100], Step [165/211], Loss: 0.2771\n",
            "Epoch [40/100], Step [166/211], Loss: 0.2554\n",
            "Epoch [40/100], Step [167/211], Loss: 0.2008\n",
            "Epoch [40/100], Step [168/211], Loss: 0.2514\n",
            "Epoch [40/100], Step [169/211], Loss: 0.1658\n",
            "Epoch [40/100], Step [170/211], Loss: 0.4989\n",
            "Epoch [40/100], Step [171/211], Loss: 0.1070\n",
            "Epoch [40/100], Step [172/211], Loss: 0.3312\n",
            "Epoch [40/100], Step [173/211], Loss: 0.2021\n",
            "Epoch [40/100], Step [174/211], Loss: 0.2377\n",
            "Epoch [40/100], Step [175/211], Loss: 0.4198\n",
            "Epoch [40/100], Step [176/211], Loss: 0.3031\n",
            "Epoch [40/100], Step [177/211], Loss: 0.2835\n",
            "Epoch [40/100], Step [178/211], Loss: 0.2428\n",
            "Epoch [40/100], Step [179/211], Loss: 0.1655\n",
            "Epoch [40/100], Step [180/211], Loss: 0.3380\n",
            "Epoch [40/100], Step [181/211], Loss: 0.2076\n",
            "Epoch [40/100], Step [182/211], Loss: 0.0841\n",
            "Epoch [40/100], Step [183/211], Loss: 0.1985\n",
            "Epoch [40/100], Step [184/211], Loss: 0.2719\n",
            "Epoch [40/100], Step [185/211], Loss: 0.1605\n",
            "Epoch [40/100], Step [186/211], Loss: 0.2458\n",
            "Epoch [40/100], Step [187/211], Loss: 0.1801\n",
            "Epoch [40/100], Step [188/211], Loss: 0.4231\n",
            "Epoch [40/100], Step [189/211], Loss: 0.3047\n",
            "Epoch [40/100], Step [190/211], Loss: 0.1330\n",
            "Epoch [40/100], Step [191/211], Loss: 0.1812\n",
            "Epoch [40/100], Step [192/211], Loss: 0.4772\n",
            "Epoch [40/100], Step [193/211], Loss: 0.2895\n",
            "Epoch [40/100], Step [194/211], Loss: 0.1911\n",
            "Epoch [40/100], Step [195/211], Loss: 0.3855\n",
            "Epoch [40/100], Step [196/211], Loss: 0.2309\n",
            "Epoch [40/100], Step [197/211], Loss: 0.2084\n",
            "Epoch [40/100], Step [198/211], Loss: 0.3165\n",
            "Epoch [40/100], Step [199/211], Loss: 0.1167\n",
            "Epoch [40/100], Step [200/211], Loss: 0.1784\n",
            "Epoch [40/100], Step [201/211], Loss: 0.2121\n",
            "Epoch [40/100], Step [202/211], Loss: 0.2880\n",
            "Epoch [40/100], Step [203/211], Loss: 0.1117\n",
            "Epoch [40/100], Step [204/211], Loss: 0.4190\n",
            "Epoch [40/100], Step [205/211], Loss: 0.2595\n",
            "Epoch [40/100], Step [206/211], Loss: 0.2922\n",
            "Epoch [40/100], Step [207/211], Loss: 0.1859\n",
            "Epoch [40/100], Step [208/211], Loss: 0.1045\n",
            "Epoch [40/100], Step [209/211], Loss: 0.2912\n",
            "Epoch [40/100], Step [210/211], Loss: 0.2565\n",
            "Epoch [40/100], Step [211/211], Loss: 0.1311\n",
            "Loss media: 0.2689\n",
            "Epoch [41/100], Step [1/211], Loss: 0.1881\n",
            "Epoch [41/100], Step [2/211], Loss: 0.2935\n",
            "Epoch [41/100], Step [3/211], Loss: 0.1588\n",
            "Epoch [41/100], Step [4/211], Loss: 0.4128\n",
            "Epoch [41/100], Step [5/211], Loss: 0.1577\n",
            "Epoch [41/100], Step [6/211], Loss: 0.1586\n",
            "Epoch [41/100], Step [7/211], Loss: 0.3955\n",
            "Epoch [41/100], Step [8/211], Loss: 0.1960\n",
            "Epoch [41/100], Step [9/211], Loss: 0.2660\n",
            "Epoch [41/100], Step [10/211], Loss: 0.2965\n",
            "Epoch [41/100], Step [11/211], Loss: 0.1709\n",
            "Epoch [41/100], Step [12/211], Loss: 0.2486\n",
            "Epoch [41/100], Step [13/211], Loss: 0.2458\n",
            "Epoch [41/100], Step [14/211], Loss: 0.2573\n",
            "Epoch [41/100], Step [15/211], Loss: 0.2484\n",
            "Epoch [41/100], Step [16/211], Loss: 0.1242\n",
            "Epoch [41/100], Step [17/211], Loss: 0.3077\n",
            "Epoch [41/100], Step [18/211], Loss: 0.5183\n",
            "Epoch [41/100], Step [19/211], Loss: 0.2834\n",
            "Epoch [41/100], Step [20/211], Loss: 0.1941\n",
            "Epoch [41/100], Step [21/211], Loss: 0.1859\n",
            "Epoch [41/100], Step [22/211], Loss: 0.1194\n",
            "Epoch [41/100], Step [23/211], Loss: 0.2073\n",
            "Epoch [41/100], Step [24/211], Loss: 0.2920\n",
            "Epoch [41/100], Step [25/211], Loss: 0.2255\n",
            "Epoch [41/100], Step [26/211], Loss: 0.4704\n",
            "Epoch [41/100], Step [27/211], Loss: 0.1025\n",
            "Epoch [41/100], Step [28/211], Loss: 0.3202\n",
            "Epoch [41/100], Step [29/211], Loss: 0.2288\n",
            "Epoch [41/100], Step [30/211], Loss: 0.2886\n",
            "Epoch [41/100], Step [31/211], Loss: 0.3268\n",
            "Epoch [41/100], Step [32/211], Loss: 0.2495\n",
            "Epoch [41/100], Step [33/211], Loss: 0.5416\n",
            "Epoch [41/100], Step [34/211], Loss: 0.3024\n",
            "Epoch [41/100], Step [35/211], Loss: 0.2088\n",
            "Epoch [41/100], Step [36/211], Loss: 0.2966\n",
            "Epoch [41/100], Step [37/211], Loss: 0.2875\n",
            "Epoch [41/100], Step [38/211], Loss: 0.1761\n",
            "Epoch [41/100], Step [39/211], Loss: 0.1928\n",
            "Epoch [41/100], Step [40/211], Loss: 0.1098\n",
            "Epoch [41/100], Step [41/211], Loss: 0.3719\n",
            "Epoch [41/100], Step [42/211], Loss: 0.1962\n",
            "Epoch [41/100], Step [43/211], Loss: 0.4250\n",
            "Epoch [41/100], Step [44/211], Loss: 0.3626\n",
            "Epoch [41/100], Step [45/211], Loss: 0.3582\n",
            "Epoch [41/100], Step [46/211], Loss: 0.1475\n",
            "Epoch [41/100], Step [47/211], Loss: 0.3948\n",
            "Epoch [41/100], Step [48/211], Loss: 0.3073\n",
            "Epoch [41/100], Step [49/211], Loss: 0.2146\n",
            "Epoch [41/100], Step [50/211], Loss: 0.1830\n",
            "Epoch [41/100], Step [51/211], Loss: 0.3557\n",
            "Epoch [41/100], Step [52/211], Loss: 0.1085\n",
            "Epoch [41/100], Step [53/211], Loss: 0.2390\n",
            "Epoch [41/100], Step [54/211], Loss: 0.4131\n",
            "Epoch [41/100], Step [55/211], Loss: 0.1974\n",
            "Epoch [41/100], Step [56/211], Loss: 0.4015\n",
            "Epoch [41/100], Step [57/211], Loss: 0.2569\n",
            "Epoch [41/100], Step [58/211], Loss: 0.4484\n",
            "Epoch [41/100], Step [59/211], Loss: 0.3737\n",
            "Epoch [41/100], Step [60/211], Loss: 0.2175\n",
            "Epoch [41/100], Step [61/211], Loss: 0.3589\n",
            "Epoch [41/100], Step [62/211], Loss: 0.2182\n",
            "Epoch [41/100], Step [63/211], Loss: 0.2630\n",
            "Epoch [41/100], Step [64/211], Loss: 0.2468\n",
            "Epoch [41/100], Step [65/211], Loss: 0.5113\n",
            "Epoch [41/100], Step [66/211], Loss: 0.1923\n",
            "Epoch [41/100], Step [67/211], Loss: 0.1757\n",
            "Epoch [41/100], Step [68/211], Loss: 0.2440\n",
            "Epoch [41/100], Step [69/211], Loss: 0.2435\n",
            "Epoch [41/100], Step [70/211], Loss: 0.3858\n",
            "Epoch [41/100], Step [71/211], Loss: 0.2224\n",
            "Epoch [41/100], Step [72/211], Loss: 0.3487\n",
            "Epoch [41/100], Step [73/211], Loss: 0.2199\n",
            "Epoch [41/100], Step [74/211], Loss: 0.1381\n",
            "Epoch [41/100], Step [75/211], Loss: 0.2106\n",
            "Epoch [41/100], Step [76/211], Loss: 0.2999\n",
            "Epoch [41/100], Step [77/211], Loss: 0.1640\n",
            "Epoch [41/100], Step [78/211], Loss: 0.0854\n",
            "Epoch [41/100], Step [79/211], Loss: 0.2182\n",
            "Epoch [41/100], Step [80/211], Loss: 0.2284\n",
            "Epoch [41/100], Step [81/211], Loss: 0.2338\n",
            "Epoch [41/100], Step [82/211], Loss: 0.4448\n",
            "Epoch [41/100], Step [83/211], Loss: 0.1729\n",
            "Epoch [41/100], Step [84/211], Loss: 0.3654\n",
            "Epoch [41/100], Step [85/211], Loss: 0.3700\n",
            "Epoch [41/100], Step [86/211], Loss: 0.1419\n",
            "Epoch [41/100], Step [87/211], Loss: 0.3599\n",
            "Epoch [41/100], Step [88/211], Loss: 0.5293\n",
            "Epoch [41/100], Step [89/211], Loss: 0.2731\n",
            "Epoch [41/100], Step [90/211], Loss: 0.1661\n",
            "Epoch [41/100], Step [91/211], Loss: 0.3085\n",
            "Epoch [41/100], Step [92/211], Loss: 0.4216\n",
            "Epoch [41/100], Step [93/211], Loss: 0.3250\n",
            "Epoch [41/100], Step [94/211], Loss: 0.4606\n",
            "Epoch [41/100], Step [95/211], Loss: 0.2855\n",
            "Epoch [41/100], Step [96/211], Loss: 0.4445\n",
            "Epoch [41/100], Step [97/211], Loss: 0.4552\n",
            "Epoch [41/100], Step [98/211], Loss: 0.2567\n",
            "Epoch [41/100], Step [99/211], Loss: 0.2899\n",
            "Epoch [41/100], Step [100/211], Loss: 0.3674\n",
            "Epoch [41/100], Step [101/211], Loss: 0.2744\n",
            "Epoch [41/100], Step [102/211], Loss: 0.4322\n",
            "Epoch [41/100], Step [103/211], Loss: 0.2758\n",
            "Epoch [41/100], Step [104/211], Loss: 0.1680\n",
            "Epoch [41/100], Step [105/211], Loss: 0.1476\n",
            "Epoch [41/100], Step [106/211], Loss: 0.2996\n",
            "Epoch [41/100], Step [107/211], Loss: 0.2446\n",
            "Epoch [41/100], Step [108/211], Loss: 0.1137\n",
            "Epoch [41/100], Step [109/211], Loss: 0.3842\n",
            "Epoch [41/100], Step [110/211], Loss: 0.2842\n",
            "Epoch [41/100], Step [111/211], Loss: 0.2821\n",
            "Epoch [41/100], Step [112/211], Loss: 0.2607\n",
            "Epoch [41/100], Step [113/211], Loss: 0.3343\n",
            "Epoch [41/100], Step [114/211], Loss: 0.3936\n",
            "Epoch [41/100], Step [115/211], Loss: 0.2604\n",
            "Epoch [41/100], Step [116/211], Loss: 0.1621\n",
            "Epoch [41/100], Step [117/211], Loss: 0.1484\n",
            "Epoch [41/100], Step [118/211], Loss: 0.2621\n",
            "Epoch [41/100], Step [119/211], Loss: 0.2615\n",
            "Epoch [41/100], Step [120/211], Loss: 0.3644\n",
            "Epoch [41/100], Step [121/211], Loss: 0.1916\n",
            "Epoch [41/100], Step [122/211], Loss: 0.1231\n",
            "Epoch [41/100], Step [123/211], Loss: 0.2141\n",
            "Epoch [41/100], Step [124/211], Loss: 0.3287\n",
            "Epoch [41/100], Step [125/211], Loss: 0.2160\n",
            "Epoch [41/100], Step [126/211], Loss: 0.2663\n",
            "Epoch [41/100], Step [127/211], Loss: 0.1873\n",
            "Epoch [41/100], Step [128/211], Loss: 0.2202\n",
            "Epoch [41/100], Step [129/211], Loss: 0.1175\n",
            "Epoch [41/100], Step [130/211], Loss: 0.1372\n",
            "Epoch [41/100], Step [131/211], Loss: 0.2293\n",
            "Epoch [41/100], Step [132/211], Loss: 0.2199\n",
            "Epoch [41/100], Step [133/211], Loss: 0.1170\n",
            "Epoch [41/100], Step [134/211], Loss: 0.2093\n",
            "Epoch [41/100], Step [135/211], Loss: 0.1833\n",
            "Epoch [41/100], Step [136/211], Loss: 0.1675\n",
            "Epoch [41/100], Step [137/211], Loss: 0.2260\n",
            "Epoch [41/100], Step [138/211], Loss: 0.1968\n",
            "Epoch [41/100], Step [139/211], Loss: 0.1580\n",
            "Epoch [41/100], Step [140/211], Loss: 0.2386\n",
            "Epoch [41/100], Step [141/211], Loss: 0.1898\n",
            "Epoch [41/100], Step [142/211], Loss: 0.3899\n",
            "Epoch [41/100], Step [143/211], Loss: 0.3258\n",
            "Epoch [41/100], Step [144/211], Loss: 0.1006\n",
            "Epoch [41/100], Step [145/211], Loss: 0.1499\n",
            "Epoch [41/100], Step [146/211], Loss: 0.1412\n",
            "Epoch [41/100], Step [147/211], Loss: 0.2766\n",
            "Epoch [41/100], Step [148/211], Loss: 0.1471\n",
            "Epoch [41/100], Step [149/211], Loss: 0.3993\n",
            "Epoch [41/100], Step [150/211], Loss: 0.1391\n",
            "Epoch [41/100], Step [151/211], Loss: 0.2796\n",
            "Epoch [41/100], Step [152/211], Loss: 0.3175\n",
            "Epoch [41/100], Step [153/211], Loss: 0.1675\n",
            "Epoch [41/100], Step [154/211], Loss: 0.2450\n",
            "Epoch [41/100], Step [155/211], Loss: 0.1029\n",
            "Epoch [41/100], Step [156/211], Loss: 0.1239\n",
            "Epoch [41/100], Step [157/211], Loss: 0.3217\n",
            "Epoch [41/100], Step [158/211], Loss: 0.1676\n",
            "Epoch [41/100], Step [159/211], Loss: 0.1556\n",
            "Epoch [41/100], Step [160/211], Loss: 0.5043\n",
            "Epoch [41/100], Step [161/211], Loss: 0.3707\n",
            "Epoch [41/100], Step [162/211], Loss: 0.3214\n",
            "Epoch [41/100], Step [163/211], Loss: 0.3128\n",
            "Epoch [41/100], Step [164/211], Loss: 0.2732\n",
            "Epoch [41/100], Step [165/211], Loss: 0.5176\n",
            "Epoch [41/100], Step [166/211], Loss: 0.2610\n",
            "Epoch [41/100], Step [167/211], Loss: 0.4113\n",
            "Epoch [41/100], Step [168/211], Loss: 0.2630\n",
            "Epoch [41/100], Step [169/211], Loss: 0.3291\n",
            "Epoch [41/100], Step [170/211], Loss: 0.1975\n",
            "Epoch [41/100], Step [171/211], Loss: 0.3010\n",
            "Epoch [41/100], Step [172/211], Loss: 0.2125\n",
            "Epoch [41/100], Step [173/211], Loss: 0.3116\n",
            "Epoch [41/100], Step [174/211], Loss: 0.4608\n",
            "Epoch [41/100], Step [175/211], Loss: 0.0862\n",
            "Epoch [41/100], Step [176/211], Loss: 0.2747\n",
            "Epoch [41/100], Step [177/211], Loss: 0.1706\n",
            "Epoch [41/100], Step [178/211], Loss: 0.3124\n",
            "Epoch [41/100], Step [179/211], Loss: 0.2913\n",
            "Epoch [41/100], Step [180/211], Loss: 0.2899\n",
            "Epoch [41/100], Step [181/211], Loss: 0.5076\n",
            "Epoch [41/100], Step [182/211], Loss: 0.2308\n",
            "Epoch [41/100], Step [183/211], Loss: 0.0827\n",
            "Epoch [41/100], Step [184/211], Loss: 0.1324\n",
            "Epoch [41/100], Step [185/211], Loss: 0.3464\n",
            "Epoch [41/100], Step [186/211], Loss: 0.1213\n",
            "Epoch [41/100], Step [187/211], Loss: 0.5663\n",
            "Epoch [41/100], Step [188/211], Loss: 0.2799\n",
            "Epoch [41/100], Step [189/211], Loss: 0.3466\n",
            "Epoch [41/100], Step [190/211], Loss: 0.3289\n",
            "Epoch [41/100], Step [191/211], Loss: 0.3209\n",
            "Epoch [41/100], Step [192/211], Loss: 0.2047\n",
            "Epoch [41/100], Step [193/211], Loss: 0.2190\n",
            "Epoch [41/100], Step [194/211], Loss: 0.1675\n",
            "Epoch [41/100], Step [195/211], Loss: 0.4472\n",
            "Epoch [41/100], Step [196/211], Loss: 0.3378\n",
            "Epoch [41/100], Step [197/211], Loss: 0.3256\n",
            "Epoch [41/100], Step [198/211], Loss: 0.3794\n",
            "Epoch [41/100], Step [199/211], Loss: 0.2854\n",
            "Epoch [41/100], Step [200/211], Loss: 0.5131\n",
            "Epoch [41/100], Step [201/211], Loss: 0.1713\n",
            "Epoch [41/100], Step [202/211], Loss: 0.4350\n",
            "Epoch [41/100], Step [203/211], Loss: 0.2363\n",
            "Epoch [41/100], Step [204/211], Loss: 0.1139\n",
            "Epoch [41/100], Step [205/211], Loss: 0.3352\n",
            "Epoch [41/100], Step [206/211], Loss: 0.2399\n",
            "Epoch [41/100], Step [207/211], Loss: 0.2545\n",
            "Epoch [41/100], Step [208/211], Loss: 0.2303\n",
            "Epoch [41/100], Step [209/211], Loss: 0.2027\n",
            "Epoch [41/100], Step [210/211], Loss: 0.2774\n",
            "Epoch [41/100], Step [211/211], Loss: 0.5665\n",
            "Loss media: 0.2701\n",
            "Epoch [42/100], Step [1/211], Loss: 0.2130\n",
            "Epoch [42/100], Step [2/211], Loss: 0.2724\n",
            "Epoch [42/100], Step [3/211], Loss: 0.2145\n",
            "Epoch [42/100], Step [4/211], Loss: 0.2382\n",
            "Epoch [42/100], Step [5/211], Loss: 0.1981\n",
            "Epoch [42/100], Step [6/211], Loss: 0.1603\n",
            "Epoch [42/100], Step [7/211], Loss: 0.2260\n",
            "Epoch [42/100], Step [8/211], Loss: 0.3052\n",
            "Epoch [42/100], Step [9/211], Loss: 0.2054\n",
            "Epoch [42/100], Step [10/211], Loss: 0.6012\n",
            "Epoch [42/100], Step [11/211], Loss: 0.3103\n",
            "Epoch [42/100], Step [12/211], Loss: 0.1612\n",
            "Epoch [42/100], Step [13/211], Loss: 0.1167\n",
            "Epoch [42/100], Step [14/211], Loss: 0.3816\n",
            "Epoch [42/100], Step [15/211], Loss: 0.3075\n",
            "Epoch [42/100], Step [16/211], Loss: 0.3632\n",
            "Epoch [42/100], Step [17/211], Loss: 0.4246\n",
            "Epoch [42/100], Step [18/211], Loss: 0.4403\n",
            "Epoch [42/100], Step [19/211], Loss: 0.2330\n",
            "Epoch [42/100], Step [20/211], Loss: 0.2741\n",
            "Epoch [42/100], Step [21/211], Loss: 0.1886\n",
            "Epoch [42/100], Step [22/211], Loss: 0.5807\n",
            "Epoch [42/100], Step [23/211], Loss: 0.0702\n",
            "Epoch [42/100], Step [24/211], Loss: 0.3567\n",
            "Epoch [42/100], Step [25/211], Loss: 0.4445\n",
            "Epoch [42/100], Step [26/211], Loss: 0.2853\n",
            "Epoch [42/100], Step [27/211], Loss: 0.2115\n",
            "Epoch [42/100], Step [28/211], Loss: 0.3424\n",
            "Epoch [42/100], Step [29/211], Loss: 0.2284\n",
            "Epoch [42/100], Step [30/211], Loss: 0.1220\n",
            "Epoch [42/100], Step [31/211], Loss: 0.3584\n",
            "Epoch [42/100], Step [32/211], Loss: 0.2762\n",
            "Epoch [42/100], Step [33/211], Loss: 0.1259\n",
            "Epoch [42/100], Step [34/211], Loss: 0.2857\n",
            "Epoch [42/100], Step [35/211], Loss: 0.2355\n",
            "Epoch [42/100], Step [36/211], Loss: 0.1565\n",
            "Epoch [42/100], Step [37/211], Loss: 0.3025\n",
            "Epoch [42/100], Step [38/211], Loss: 0.3198\n",
            "Epoch [42/100], Step [39/211], Loss: 0.2524\n",
            "Epoch [42/100], Step [40/211], Loss: 0.2193\n",
            "Epoch [42/100], Step [41/211], Loss: 0.4150\n",
            "Epoch [42/100], Step [42/211], Loss: 0.2278\n",
            "Epoch [42/100], Step [43/211], Loss: 0.1314\n",
            "Epoch [42/100], Step [44/211], Loss: 0.1016\n",
            "Epoch [42/100], Step [45/211], Loss: 0.1574\n",
            "Epoch [42/100], Step [46/211], Loss: 0.1957\n",
            "Epoch [42/100], Step [47/211], Loss: 0.3077\n",
            "Epoch [42/100], Step [48/211], Loss: 0.3402\n",
            "Epoch [42/100], Step [49/211], Loss: 0.2528\n",
            "Epoch [42/100], Step [50/211], Loss: 0.2781\n",
            "Epoch [42/100], Step [51/211], Loss: 0.1087\n",
            "Epoch [42/100], Step [52/211], Loss: 0.1816\n",
            "Epoch [42/100], Step [53/211], Loss: 0.1608\n",
            "Epoch [42/100], Step [54/211], Loss: 0.3212\n",
            "Epoch [42/100], Step [55/211], Loss: 0.2497\n",
            "Epoch [42/100], Step [56/211], Loss: 0.2579\n",
            "Epoch [42/100], Step [57/211], Loss: 0.3089\n",
            "Epoch [42/100], Step [58/211], Loss: 0.1895\n",
            "Epoch [42/100], Step [59/211], Loss: 0.2297\n",
            "Epoch [42/100], Step [60/211], Loss: 0.3442\n",
            "Epoch [42/100], Step [61/211], Loss: 0.2104\n",
            "Epoch [42/100], Step [62/211], Loss: 0.4513\n",
            "Epoch [42/100], Step [63/211], Loss: 0.1496\n",
            "Epoch [42/100], Step [64/211], Loss: 0.2029\n",
            "Epoch [42/100], Step [65/211], Loss: 0.3254\n",
            "Epoch [42/100], Step [66/211], Loss: 0.2745\n",
            "Epoch [42/100], Step [67/211], Loss: 0.3125\n",
            "Epoch [42/100], Step [68/211], Loss: 0.2215\n",
            "Epoch [42/100], Step [69/211], Loss: 0.3303\n",
            "Epoch [42/100], Step [70/211], Loss: 0.0848\n",
            "Epoch [42/100], Step [71/211], Loss: 0.1781\n",
            "Epoch [42/100], Step [72/211], Loss: 0.2745\n",
            "Epoch [42/100], Step [73/211], Loss: 0.2405\n",
            "Epoch [42/100], Step [74/211], Loss: 0.2051\n",
            "Epoch [42/100], Step [75/211], Loss: 0.2187\n",
            "Epoch [42/100], Step [76/211], Loss: 0.2619\n",
            "Epoch [42/100], Step [77/211], Loss: 0.4645\n",
            "Epoch [42/100], Step [78/211], Loss: 0.2239\n",
            "Epoch [42/100], Step [79/211], Loss: 0.2782\n",
            "Epoch [42/100], Step [80/211], Loss: 0.1972\n",
            "Epoch [42/100], Step [81/211], Loss: 0.2771\n",
            "Epoch [42/100], Step [82/211], Loss: 0.1906\n",
            "Epoch [42/100], Step [83/211], Loss: 0.2734\n",
            "Epoch [42/100], Step [84/211], Loss: 0.3213\n",
            "Epoch [42/100], Step [85/211], Loss: 0.1852\n",
            "Epoch [42/100], Step [86/211], Loss: 0.2445\n",
            "Epoch [42/100], Step [87/211], Loss: 0.1054\n",
            "Epoch [42/100], Step [88/211], Loss: 0.4262\n",
            "Epoch [42/100], Step [89/211], Loss: 0.3760\n",
            "Epoch [42/100], Step [90/211], Loss: 0.2347\n",
            "Epoch [42/100], Step [91/211], Loss: 0.3968\n",
            "Epoch [42/100], Step [92/211], Loss: 0.2304\n",
            "Epoch [42/100], Step [93/211], Loss: 0.2402\n",
            "Epoch [42/100], Step [94/211], Loss: 0.2154\n",
            "Epoch [42/100], Step [95/211], Loss: 0.1410\n",
            "Epoch [42/100], Step [96/211], Loss: 0.1516\n",
            "Epoch [42/100], Step [97/211], Loss: 0.2680\n",
            "Epoch [42/100], Step [98/211], Loss: 0.1771\n",
            "Epoch [42/100], Step [99/211], Loss: 0.5514\n",
            "Epoch [42/100], Step [100/211], Loss: 0.2372\n",
            "Epoch [42/100], Step [101/211], Loss: 0.4767\n",
            "Epoch [42/100], Step [102/211], Loss: 0.2585\n",
            "Epoch [42/100], Step [103/211], Loss: 0.1744\n",
            "Epoch [42/100], Step [104/211], Loss: 0.4202\n",
            "Epoch [42/100], Step [105/211], Loss: 0.2633\n",
            "Epoch [42/100], Step [106/211], Loss: 0.2774\n",
            "Epoch [42/100], Step [107/211], Loss: 0.2192\n",
            "Epoch [42/100], Step [108/211], Loss: 0.2942\n",
            "Epoch [42/100], Step [109/211], Loss: 0.1396\n",
            "Epoch [42/100], Step [110/211], Loss: 0.3561\n",
            "Epoch [42/100], Step [111/211], Loss: 0.3296\n",
            "Epoch [42/100], Step [112/211], Loss: 0.1789\n",
            "Epoch [42/100], Step [113/211], Loss: 0.1617\n",
            "Epoch [42/100], Step [114/211], Loss: 0.1489\n",
            "Epoch [42/100], Step [115/211], Loss: 0.3339\n",
            "Epoch [42/100], Step [116/211], Loss: 0.4223\n",
            "Epoch [42/100], Step [117/211], Loss: 0.5650\n",
            "Epoch [42/100], Step [118/211], Loss: 0.1176\n",
            "Epoch [42/100], Step [119/211], Loss: 0.1783\n",
            "Epoch [42/100], Step [120/211], Loss: 0.1883\n",
            "Epoch [42/100], Step [121/211], Loss: 0.2913\n",
            "Epoch [42/100], Step [122/211], Loss: 0.1814\n",
            "Epoch [42/100], Step [123/211], Loss: 0.2671\n",
            "Epoch [42/100], Step [124/211], Loss: 0.2297\n",
            "Epoch [42/100], Step [125/211], Loss: 0.1492\n",
            "Epoch [42/100], Step [126/211], Loss: 0.2477\n",
            "Epoch [42/100], Step [127/211], Loss: 0.2977\n",
            "Epoch [42/100], Step [128/211], Loss: 0.3400\n",
            "Epoch [42/100], Step [129/211], Loss: 0.3313\n",
            "Epoch [42/100], Step [130/211], Loss: 0.1522\n",
            "Epoch [42/100], Step [131/211], Loss: 0.3453\n",
            "Epoch [42/100], Step [132/211], Loss: 0.1459\n",
            "Epoch [42/100], Step [133/211], Loss: 0.3212\n",
            "Epoch [42/100], Step [134/211], Loss: 0.1560\n",
            "Epoch [42/100], Step [135/211], Loss: 0.3042\n",
            "Epoch [42/100], Step [136/211], Loss: 0.1204\n",
            "Epoch [42/100], Step [137/211], Loss: 0.3210\n",
            "Epoch [42/100], Step [138/211], Loss: 0.0953\n",
            "Epoch [42/100], Step [139/211], Loss: 0.2114\n",
            "Epoch [42/100], Step [140/211], Loss: 0.4721\n",
            "Epoch [42/100], Step [141/211], Loss: 0.6708\n",
            "Epoch [42/100], Step [142/211], Loss: 0.2073\n",
            "Epoch [42/100], Step [143/211], Loss: 0.2562\n",
            "Epoch [42/100], Step [144/211], Loss: 0.2944\n",
            "Epoch [42/100], Step [145/211], Loss: 0.1890\n",
            "Epoch [42/100], Step [146/211], Loss: 0.2376\n",
            "Epoch [42/100], Step [147/211], Loss: 0.2626\n",
            "Epoch [42/100], Step [148/211], Loss: 0.2587\n",
            "Epoch [42/100], Step [149/211], Loss: 0.3159\n",
            "Epoch [42/100], Step [150/211], Loss: 0.4526\n",
            "Epoch [42/100], Step [151/211], Loss: 0.3169\n",
            "Epoch [42/100], Step [152/211], Loss: 0.1917\n",
            "Epoch [42/100], Step [153/211], Loss: 0.2759\n",
            "Epoch [42/100], Step [154/211], Loss: 0.4717\n",
            "Epoch [42/100], Step [155/211], Loss: 0.3250\n",
            "Epoch [42/100], Step [156/211], Loss: 0.1748\n",
            "Epoch [42/100], Step [157/211], Loss: 0.1698\n",
            "Epoch [42/100], Step [158/211], Loss: 0.3602\n",
            "Epoch [42/100], Step [159/211], Loss: 0.3084\n",
            "Epoch [42/100], Step [160/211], Loss: 0.1399\n",
            "Epoch [42/100], Step [161/211], Loss: 0.3079\n",
            "Epoch [42/100], Step [162/211], Loss: 0.2443\n",
            "Epoch [42/100], Step [163/211], Loss: 0.3704\n",
            "Epoch [42/100], Step [164/211], Loss: 0.2587\n",
            "Epoch [42/100], Step [165/211], Loss: 0.3134\n",
            "Epoch [42/100], Step [166/211], Loss: 0.2142\n",
            "Epoch [42/100], Step [167/211], Loss: 0.4273\n",
            "Epoch [42/100], Step [168/211], Loss: 0.2311\n",
            "Epoch [42/100], Step [169/211], Loss: 0.4714\n",
            "Epoch [42/100], Step [170/211], Loss: 0.2208\n",
            "Epoch [42/100], Step [171/211], Loss: 0.3011\n",
            "Epoch [42/100], Step [172/211], Loss: 0.2576\n",
            "Epoch [42/100], Step [173/211], Loss: 0.2523\n",
            "Epoch [42/100], Step [174/211], Loss: 0.4188\n",
            "Epoch [42/100], Step [175/211], Loss: 0.0937\n",
            "Epoch [42/100], Step [176/211], Loss: 0.2593\n",
            "Epoch [42/100], Step [177/211], Loss: 0.3488\n",
            "Epoch [42/100], Step [178/211], Loss: 0.2559\n",
            "Epoch [42/100], Step [179/211], Loss: 0.1606\n",
            "Epoch [42/100], Step [180/211], Loss: 0.2875\n",
            "Epoch [42/100], Step [181/211], Loss: 0.3780\n",
            "Epoch [42/100], Step [182/211], Loss: 0.1481\n",
            "Epoch [42/100], Step [183/211], Loss: 0.2207\n",
            "Epoch [42/100], Step [184/211], Loss: 0.1411\n",
            "Epoch [42/100], Step [185/211], Loss: 0.2409\n",
            "Epoch [42/100], Step [186/211], Loss: 0.2193\n",
            "Epoch [42/100], Step [187/211], Loss: 0.2706\n",
            "Epoch [42/100], Step [188/211], Loss: 0.2736\n",
            "Epoch [42/100], Step [189/211], Loss: 0.5316\n",
            "Epoch [42/100], Step [190/211], Loss: 0.4330\n",
            "Epoch [42/100], Step [191/211], Loss: 0.2559\n",
            "Epoch [42/100], Step [192/211], Loss: 0.3004\n",
            "Epoch [42/100], Step [193/211], Loss: 0.3046\n",
            "Epoch [42/100], Step [194/211], Loss: 0.2524\n",
            "Epoch [42/100], Step [195/211], Loss: 0.2837\n",
            "Epoch [42/100], Step [196/211], Loss: 0.3446\n",
            "Epoch [42/100], Step [197/211], Loss: 0.2946\n",
            "Epoch [42/100], Step [198/211], Loss: 0.2433\n",
            "Epoch [42/100], Step [199/211], Loss: 0.0940\n",
            "Epoch [42/100], Step [200/211], Loss: 0.1291\n",
            "Epoch [42/100], Step [201/211], Loss: 0.2723\n",
            "Epoch [42/100], Step [202/211], Loss: 0.1008\n",
            "Epoch [42/100], Step [203/211], Loss: 0.2457\n",
            "Epoch [42/100], Step [204/211], Loss: 0.2900\n",
            "Epoch [42/100], Step [205/211], Loss: 0.1593\n",
            "Epoch [42/100], Step [206/211], Loss: 0.2016\n",
            "Epoch [42/100], Step [207/211], Loss: 0.2672\n",
            "Epoch [42/100], Step [208/211], Loss: 0.1713\n",
            "Epoch [42/100], Step [209/211], Loss: 0.1934\n",
            "Epoch [42/100], Step [210/211], Loss: 0.4652\n",
            "Epoch [42/100], Step [211/211], Loss: 0.1474\n",
            "Loss media: 0.2656\n",
            "Epoch [43/100], Step [1/211], Loss: 0.3398\n",
            "Epoch [43/100], Step [2/211], Loss: 0.2380\n",
            "Epoch [43/100], Step [3/211], Loss: 0.2456\n",
            "Epoch [43/100], Step [4/211], Loss: 0.2341\n",
            "Epoch [43/100], Step [5/211], Loss: 0.2122\n",
            "Epoch [43/100], Step [6/211], Loss: 0.1258\n",
            "Epoch [43/100], Step [7/211], Loss: 0.1535\n",
            "Epoch [43/100], Step [8/211], Loss: 0.3478\n",
            "Epoch [43/100], Step [9/211], Loss: 0.1660\n",
            "Epoch [43/100], Step [10/211], Loss: 0.3594\n",
            "Epoch [43/100], Step [11/211], Loss: 0.3221\n",
            "Epoch [43/100], Step [12/211], Loss: 0.2043\n",
            "Epoch [43/100], Step [13/211], Loss: 0.1180\n",
            "Epoch [43/100], Step [14/211], Loss: 0.3488\n",
            "Epoch [43/100], Step [15/211], Loss: 0.0988\n",
            "Epoch [43/100], Step [16/211], Loss: 0.2462\n",
            "Epoch [43/100], Step [17/211], Loss: 0.3118\n",
            "Epoch [43/100], Step [18/211], Loss: 0.2764\n",
            "Epoch [43/100], Step [19/211], Loss: 0.2216\n",
            "Epoch [43/100], Step [20/211], Loss: 0.5631\n",
            "Epoch [43/100], Step [21/211], Loss: 0.2358\n",
            "Epoch [43/100], Step [22/211], Loss: 0.1458\n",
            "Epoch [43/100], Step [23/211], Loss: 0.2820\n",
            "Epoch [43/100], Step [24/211], Loss: 0.1596\n",
            "Epoch [43/100], Step [25/211], Loss: 0.2294\n",
            "Epoch [43/100], Step [26/211], Loss: 0.2142\n",
            "Epoch [43/100], Step [27/211], Loss: 0.2272\n",
            "Epoch [43/100], Step [28/211], Loss: 0.2038\n",
            "Epoch [43/100], Step [29/211], Loss: 0.1974\n",
            "Epoch [43/100], Step [30/211], Loss: 0.3643\n",
            "Epoch [43/100], Step [31/211], Loss: 0.3916\n",
            "Epoch [43/100], Step [32/211], Loss: 0.2917\n",
            "Epoch [43/100], Step [33/211], Loss: 0.1829\n",
            "Epoch [43/100], Step [34/211], Loss: 0.4712\n",
            "Epoch [43/100], Step [35/211], Loss: 0.2093\n",
            "Epoch [43/100], Step [36/211], Loss: 0.4270\n",
            "Epoch [43/100], Step [37/211], Loss: 0.2476\n",
            "Epoch [43/100], Step [38/211], Loss: 0.1622\n",
            "Epoch [43/100], Step [39/211], Loss: 0.3544\n",
            "Epoch [43/100], Step [40/211], Loss: 0.4088\n",
            "Epoch [43/100], Step [41/211], Loss: 0.1749\n",
            "Epoch [43/100], Step [42/211], Loss: 0.3276\n",
            "Epoch [43/100], Step [43/211], Loss: 0.2338\n",
            "Epoch [43/100], Step [44/211], Loss: 0.2497\n",
            "Epoch [43/100], Step [45/211], Loss: 0.6044\n",
            "Epoch [43/100], Step [46/211], Loss: 0.1921\n",
            "Epoch [43/100], Step [47/211], Loss: 0.1009\n",
            "Epoch [43/100], Step [48/211], Loss: 0.2231\n",
            "Epoch [43/100], Step [49/211], Loss: 0.2413\n",
            "Epoch [43/100], Step [50/211], Loss: 0.1581\n",
            "Epoch [43/100], Step [51/211], Loss: 0.3434\n",
            "Epoch [43/100], Step [52/211], Loss: 0.4815\n",
            "Epoch [43/100], Step [53/211], Loss: 0.5440\n",
            "Epoch [43/100], Step [54/211], Loss: 0.3496\n",
            "Epoch [43/100], Step [55/211], Loss: 0.1795\n",
            "Epoch [43/100], Step [56/211], Loss: 0.3348\n",
            "Epoch [43/100], Step [57/211], Loss: 0.1667\n",
            "Epoch [43/100], Step [58/211], Loss: 0.3655\n",
            "Epoch [43/100], Step [59/211], Loss: 0.1178\n",
            "Epoch [43/100], Step [60/211], Loss: 0.2049\n",
            "Epoch [43/100], Step [61/211], Loss: 0.1029\n",
            "Epoch [43/100], Step [62/211], Loss: 0.2337\n",
            "Epoch [43/100], Step [63/211], Loss: 0.3773\n",
            "Epoch [43/100], Step [64/211], Loss: 0.3021\n",
            "Epoch [43/100], Step [65/211], Loss: 0.1048\n",
            "Epoch [43/100], Step [66/211], Loss: 0.2262\n",
            "Epoch [43/100], Step [67/211], Loss: 0.2345\n",
            "Epoch [43/100], Step [68/211], Loss: 0.1853\n",
            "Epoch [43/100], Step [69/211], Loss: 0.2088\n",
            "Epoch [43/100], Step [70/211], Loss: 0.1447\n",
            "Epoch [43/100], Step [71/211], Loss: 0.3149\n",
            "Epoch [43/100], Step [72/211], Loss: 0.2214\n",
            "Epoch [43/100], Step [73/211], Loss: 0.3385\n",
            "Epoch [43/100], Step [74/211], Loss: 0.3135\n",
            "Epoch [43/100], Step [75/211], Loss: 0.1337\n",
            "Epoch [43/100], Step [76/211], Loss: 0.5076\n",
            "Epoch [43/100], Step [77/211], Loss: 0.2520\n",
            "Epoch [43/100], Step [78/211], Loss: 0.3411\n",
            "Epoch [43/100], Step [79/211], Loss: 0.2141\n",
            "Epoch [43/100], Step [80/211], Loss: 0.5392\n",
            "Epoch [43/100], Step [81/211], Loss: 0.2337\n",
            "Epoch [43/100], Step [82/211], Loss: 0.4022\n",
            "Epoch [43/100], Step [83/211], Loss: 0.2621\n",
            "Epoch [43/100], Step [84/211], Loss: 0.1759\n",
            "Epoch [43/100], Step [85/211], Loss: 0.2400\n",
            "Epoch [43/100], Step [86/211], Loss: 0.2487\n",
            "Epoch [43/100], Step [87/211], Loss: 0.4275\n",
            "Epoch [43/100], Step [88/211], Loss: 0.3619\n",
            "Epoch [43/100], Step [89/211], Loss: 0.4792\n",
            "Epoch [43/100], Step [90/211], Loss: 0.1018\n",
            "Epoch [43/100], Step [91/211], Loss: 0.3797\n",
            "Epoch [43/100], Step [92/211], Loss: 0.2492\n",
            "Epoch [43/100], Step [93/211], Loss: 0.1185\n",
            "Epoch [43/100], Step [94/211], Loss: 0.2978\n",
            "Epoch [43/100], Step [95/211], Loss: 0.4103\n",
            "Epoch [43/100], Step [96/211], Loss: 0.2719\n",
            "Epoch [43/100], Step [97/211], Loss: 0.2970\n",
            "Epoch [43/100], Step [98/211], Loss: 0.1254\n",
            "Epoch [43/100], Step [99/211], Loss: 0.4303\n",
            "Epoch [43/100], Step [100/211], Loss: 0.1459\n",
            "Epoch [43/100], Step [101/211], Loss: 0.7716\n",
            "Epoch [43/100], Step [102/211], Loss: 0.3415\n",
            "Epoch [43/100], Step [103/211], Loss: 0.5164\n",
            "Epoch [43/100], Step [104/211], Loss: 0.1177\n",
            "Epoch [43/100], Step [105/211], Loss: 0.2864\n",
            "Epoch [43/100], Step [106/211], Loss: 0.2754\n",
            "Epoch [43/100], Step [107/211], Loss: 0.1552\n",
            "Epoch [43/100], Step [108/211], Loss: 0.1884\n",
            "Epoch [43/100], Step [109/211], Loss: 0.1939\n",
            "Epoch [43/100], Step [110/211], Loss: 0.1737\n",
            "Epoch [43/100], Step [111/211], Loss: 0.2130\n",
            "Epoch [43/100], Step [112/211], Loss: 0.2273\n",
            "Epoch [43/100], Step [113/211], Loss: 0.2994\n",
            "Epoch [43/100], Step [114/211], Loss: 0.2238\n",
            "Epoch [43/100], Step [115/211], Loss: 0.0879\n",
            "Epoch [43/100], Step [116/211], Loss: 0.1745\n",
            "Epoch [43/100], Step [117/211], Loss: 0.3424\n",
            "Epoch [43/100], Step [118/211], Loss: 0.4436\n",
            "Epoch [43/100], Step [119/211], Loss: 0.2653\n",
            "Epoch [43/100], Step [120/211], Loss: 0.2704\n",
            "Epoch [43/100], Step [121/211], Loss: 0.2342\n",
            "Epoch [43/100], Step [122/211], Loss: 0.2270\n",
            "Epoch [43/100], Step [123/211], Loss: 0.1316\n",
            "Epoch [43/100], Step [124/211], Loss: 0.3717\n",
            "Epoch [43/100], Step [125/211], Loss: 0.1959\n",
            "Epoch [43/100], Step [126/211], Loss: 0.1157\n",
            "Epoch [43/100], Step [127/211], Loss: 0.3452\n",
            "Epoch [43/100], Step [128/211], Loss: 0.3619\n",
            "Epoch [43/100], Step [129/211], Loss: 0.2993\n",
            "Epoch [43/100], Step [130/211], Loss: 0.1676\n",
            "Epoch [43/100], Step [131/211], Loss: 0.2031\n",
            "Epoch [43/100], Step [132/211], Loss: 0.2744\n",
            "Epoch [43/100], Step [133/211], Loss: 0.2918\n",
            "Epoch [43/100], Step [134/211], Loss: 0.1910\n",
            "Epoch [43/100], Step [135/211], Loss: 0.1841\n",
            "Epoch [43/100], Step [136/211], Loss: 0.1259\n",
            "Epoch [43/100], Step [137/211], Loss: 0.2323\n",
            "Epoch [43/100], Step [138/211], Loss: 0.2105\n",
            "Epoch [43/100], Step [139/211], Loss: 0.2054\n",
            "Epoch [43/100], Step [140/211], Loss: 0.2704\n",
            "Epoch [43/100], Step [141/211], Loss: 0.5108\n",
            "Epoch [43/100], Step [142/211], Loss: 0.2001\n",
            "Epoch [43/100], Step [143/211], Loss: 0.3402\n",
            "Epoch [43/100], Step [144/211], Loss: 0.2066\n",
            "Epoch [43/100], Step [145/211], Loss: 0.3149\n",
            "Epoch [43/100], Step [146/211], Loss: 0.2662\n",
            "Epoch [43/100], Step [147/211], Loss: 0.1930\n",
            "Epoch [43/100], Step [148/211], Loss: 0.1484\n",
            "Epoch [43/100], Step [149/211], Loss: 0.1928\n",
            "Epoch [43/100], Step [150/211], Loss: 0.1160\n",
            "Epoch [43/100], Step [151/211], Loss: 0.1317\n",
            "Epoch [43/100], Step [152/211], Loss: 0.1470\n",
            "Epoch [43/100], Step [153/211], Loss: 0.3433\n",
            "Epoch [43/100], Step [154/211], Loss: 0.1218\n",
            "Epoch [43/100], Step [155/211], Loss: 0.2407\n",
            "Epoch [43/100], Step [156/211], Loss: 0.1534\n",
            "Epoch [43/100], Step [157/211], Loss: 0.3056\n",
            "Epoch [43/100], Step [158/211], Loss: 0.2295\n",
            "Epoch [43/100], Step [159/211], Loss: 0.4280\n",
            "Epoch [43/100], Step [160/211], Loss: 0.2762\n",
            "Epoch [43/100], Step [161/211], Loss: 0.3721\n",
            "Epoch [43/100], Step [162/211], Loss: 0.1497\n",
            "Epoch [43/100], Step [163/211], Loss: 0.2002\n",
            "Epoch [43/100], Step [164/211], Loss: 0.1864\n",
            "Epoch [43/100], Step [165/211], Loss: 0.2479\n",
            "Epoch [43/100], Step [166/211], Loss: 0.3222\n",
            "Epoch [43/100], Step [167/211], Loss: 0.2418\n",
            "Epoch [43/100], Step [168/211], Loss: 0.3095\n",
            "Epoch [43/100], Step [169/211], Loss: 0.1685\n",
            "Epoch [43/100], Step [170/211], Loss: 0.3951\n",
            "Epoch [43/100], Step [171/211], Loss: 0.1332\n",
            "Epoch [43/100], Step [172/211], Loss: 0.1904\n",
            "Epoch [43/100], Step [173/211], Loss: 0.1791\n",
            "Epoch [43/100], Step [174/211], Loss: 0.1067\n",
            "Epoch [43/100], Step [175/211], Loss: 0.3119\n",
            "Epoch [43/100], Step [176/211], Loss: 0.2349\n",
            "Epoch [43/100], Step [177/211], Loss: 0.2569\n",
            "Epoch [43/100], Step [178/211], Loss: 0.2099\n",
            "Epoch [43/100], Step [179/211], Loss: 0.1750\n",
            "Epoch [43/100], Step [180/211], Loss: 0.3362\n",
            "Epoch [43/100], Step [181/211], Loss: 0.2282\n",
            "Epoch [43/100], Step [182/211], Loss: 0.2173\n",
            "Epoch [43/100], Step [183/211], Loss: 0.3152\n",
            "Epoch [43/100], Step [184/211], Loss: 0.1247\n",
            "Epoch [43/100], Step [185/211], Loss: 0.2498\n",
            "Epoch [43/100], Step [186/211], Loss: 0.3375\n",
            "Epoch [43/100], Step [187/211], Loss: 0.1940\n",
            "Epoch [43/100], Step [188/211], Loss: 0.3189\n",
            "Epoch [43/100], Step [189/211], Loss: 0.1800\n",
            "Epoch [43/100], Step [190/211], Loss: 0.2508\n",
            "Epoch [43/100], Step [191/211], Loss: 0.2469\n",
            "Epoch [43/100], Step [192/211], Loss: 0.3078\n",
            "Epoch [43/100], Step [193/211], Loss: 0.4763\n",
            "Epoch [43/100], Step [194/211], Loss: 0.2762\n",
            "Epoch [43/100], Step [195/211], Loss: 0.1661\n",
            "Epoch [43/100], Step [196/211], Loss: 0.5833\n",
            "Epoch [43/100], Step [197/211], Loss: 0.1820\n",
            "Epoch [43/100], Step [198/211], Loss: 0.1774\n",
            "Epoch [43/100], Step [199/211], Loss: 0.1014\n",
            "Epoch [43/100], Step [200/211], Loss: 0.2433\n",
            "Epoch [43/100], Step [201/211], Loss: 0.3504\n",
            "Epoch [43/100], Step [202/211], Loss: 0.4840\n",
            "Epoch [43/100], Step [203/211], Loss: 0.1636\n",
            "Epoch [43/100], Step [204/211], Loss: 0.3838\n",
            "Epoch [43/100], Step [205/211], Loss: 0.3861\n",
            "Epoch [43/100], Step [206/211], Loss: 0.2059\n",
            "Epoch [43/100], Step [207/211], Loss: 0.3769\n",
            "Epoch [43/100], Step [208/211], Loss: 0.2159\n",
            "Epoch [43/100], Step [209/211], Loss: 0.0889\n",
            "Epoch [43/100], Step [210/211], Loss: 0.4466\n",
            "Epoch [43/100], Step [211/211], Loss: 0.0682\n",
            "Loss media: 0.2618\n",
            "Epoch [44/100], Step [1/211], Loss: 0.4244\n",
            "Epoch [44/100], Step [2/211], Loss: 0.1775\n",
            "Epoch [44/100], Step [3/211], Loss: 0.2844\n",
            "Epoch [44/100], Step [4/211], Loss: 0.1369\n",
            "Epoch [44/100], Step [5/211], Loss: 0.2652\n",
            "Epoch [44/100], Step [6/211], Loss: 0.1424\n",
            "Epoch [44/100], Step [7/211], Loss: 0.1692\n",
            "Epoch [44/100], Step [8/211], Loss: 0.2374\n",
            "Epoch [44/100], Step [9/211], Loss: 0.2721\n",
            "Epoch [44/100], Step [10/211], Loss: 0.1936\n",
            "Epoch [44/100], Step [11/211], Loss: 0.1862\n",
            "Epoch [44/100], Step [12/211], Loss: 0.4330\n",
            "Epoch [44/100], Step [13/211], Loss: 0.6236\n",
            "Epoch [44/100], Step [14/211], Loss: 0.5437\n",
            "Epoch [44/100], Step [15/211], Loss: 0.1291\n",
            "Epoch [44/100], Step [16/211], Loss: 0.3275\n",
            "Epoch [44/100], Step [17/211], Loss: 0.3090\n",
            "Epoch [44/100], Step [18/211], Loss: 0.2482\n",
            "Epoch [44/100], Step [19/211], Loss: 0.1996\n",
            "Epoch [44/100], Step [20/211], Loss: 0.5010\n",
            "Epoch [44/100], Step [21/211], Loss: 0.3118\n",
            "Epoch [44/100], Step [22/211], Loss: 0.1638\n",
            "Epoch [44/100], Step [23/211], Loss: 0.2106\n",
            "Epoch [44/100], Step [24/211], Loss: 0.2104\n",
            "Epoch [44/100], Step [25/211], Loss: 0.1561\n",
            "Epoch [44/100], Step [26/211], Loss: 0.1715\n",
            "Epoch [44/100], Step [27/211], Loss: 0.1773\n",
            "Epoch [44/100], Step [28/211], Loss: 0.1752\n",
            "Epoch [44/100], Step [29/211], Loss: 0.3160\n",
            "Epoch [44/100], Step [30/211], Loss: 0.2190\n",
            "Epoch [44/100], Step [31/211], Loss: 0.1615\n",
            "Epoch [44/100], Step [32/211], Loss: 0.1590\n",
            "Epoch [44/100], Step [33/211], Loss: 0.3169\n",
            "Epoch [44/100], Step [34/211], Loss: 0.3974\n",
            "Epoch [44/100], Step [35/211], Loss: 0.3748\n",
            "Epoch [44/100], Step [36/211], Loss: 0.1396\n",
            "Epoch [44/100], Step [37/211], Loss: 0.3433\n",
            "Epoch [44/100], Step [38/211], Loss: 0.3765\n",
            "Epoch [44/100], Step [39/211], Loss: 0.1756\n",
            "Epoch [44/100], Step [40/211], Loss: 0.2391\n",
            "Epoch [44/100], Step [41/211], Loss: 0.1641\n",
            "Epoch [44/100], Step [42/211], Loss: 0.1349\n",
            "Epoch [44/100], Step [43/211], Loss: 0.1808\n",
            "Epoch [44/100], Step [44/211], Loss: 0.1454\n",
            "Epoch [44/100], Step [45/211], Loss: 0.2464\n",
            "Epoch [44/100], Step [46/211], Loss: 0.0901\n",
            "Epoch [44/100], Step [47/211], Loss: 0.1788\n",
            "Epoch [44/100], Step [48/211], Loss: 0.3200\n",
            "Epoch [44/100], Step [49/211], Loss: 0.1836\n",
            "Epoch [44/100], Step [50/211], Loss: 0.3232\n",
            "Epoch [44/100], Step [51/211], Loss: 0.2434\n",
            "Epoch [44/100], Step [52/211], Loss: 0.1986\n",
            "Epoch [44/100], Step [53/211], Loss: 0.1839\n",
            "Epoch [44/100], Step [54/211], Loss: 0.1318\n",
            "Epoch [44/100], Step [55/211], Loss: 0.4189\n",
            "Epoch [44/100], Step [56/211], Loss: 0.6983\n",
            "Epoch [44/100], Step [57/211], Loss: 0.2248\n",
            "Epoch [44/100], Step [58/211], Loss: 0.1106\n",
            "Epoch [44/100], Step [59/211], Loss: 0.2677\n",
            "Epoch [44/100], Step [60/211], Loss: 0.2200\n",
            "Epoch [44/100], Step [61/211], Loss: 0.2485\n",
            "Epoch [44/100], Step [62/211], Loss: 0.2887\n",
            "Epoch [44/100], Step [63/211], Loss: 0.1489\n",
            "Epoch [44/100], Step [64/211], Loss: 0.2700\n",
            "Epoch [44/100], Step [65/211], Loss: 0.1616\n",
            "Epoch [44/100], Step [66/211], Loss: 0.2303\n",
            "Epoch [44/100], Step [67/211], Loss: 0.2436\n",
            "Epoch [44/100], Step [68/211], Loss: 0.2943\n",
            "Epoch [44/100], Step [69/211], Loss: 0.4201\n",
            "Epoch [44/100], Step [70/211], Loss: 0.2748\n",
            "Epoch [44/100], Step [71/211], Loss: 0.2432\n",
            "Epoch [44/100], Step [72/211], Loss: 0.2388\n",
            "Epoch [44/100], Step [73/211], Loss: 0.3781\n",
            "Epoch [44/100], Step [74/211], Loss: 0.1681\n",
            "Epoch [44/100], Step [75/211], Loss: 0.2482\n",
            "Epoch [44/100], Step [76/211], Loss: 0.2587\n",
            "Epoch [44/100], Step [77/211], Loss: 0.1586\n",
            "Epoch [44/100], Step [78/211], Loss: 0.2548\n",
            "Epoch [44/100], Step [79/211], Loss: 0.2474\n",
            "Epoch [44/100], Step [80/211], Loss: 0.3071\n",
            "Epoch [44/100], Step [81/211], Loss: 0.3117\n",
            "Epoch [44/100], Step [82/211], Loss: 0.2464\n",
            "Epoch [44/100], Step [83/211], Loss: 0.2572\n",
            "Epoch [44/100], Step [84/211], Loss: 0.1018\n",
            "Epoch [44/100], Step [85/211], Loss: 0.1907\n",
            "Epoch [44/100], Step [86/211], Loss: 0.2542\n",
            "Epoch [44/100], Step [87/211], Loss: 0.1916\n",
            "Epoch [44/100], Step [88/211], Loss: 0.2067\n",
            "Epoch [44/100], Step [89/211], Loss: 0.2127\n",
            "Epoch [44/100], Step [90/211], Loss: 0.4209\n",
            "Epoch [44/100], Step [91/211], Loss: 0.2871\n",
            "Epoch [44/100], Step [92/211], Loss: 0.1936\n",
            "Epoch [44/100], Step [93/211], Loss: 0.2223\n",
            "Epoch [44/100], Step [94/211], Loss: 0.2651\n",
            "Epoch [44/100], Step [95/211], Loss: 0.3179\n",
            "Epoch [44/100], Step [96/211], Loss: 0.3729\n",
            "Epoch [44/100], Step [97/211], Loss: 0.3174\n",
            "Epoch [44/100], Step [98/211], Loss: 0.5404\n",
            "Epoch [44/100], Step [99/211], Loss: 0.2586\n",
            "Epoch [44/100], Step [100/211], Loss: 0.1837\n",
            "Epoch [44/100], Step [101/211], Loss: 0.4771\n",
            "Epoch [44/100], Step [102/211], Loss: 0.2793\n",
            "Epoch [44/100], Step [103/211], Loss: 0.3472\n",
            "Epoch [44/100], Step [104/211], Loss: 0.1306\n",
            "Epoch [44/100], Step [105/211], Loss: 0.3209\n",
            "Epoch [44/100], Step [106/211], Loss: 0.4995\n",
            "Epoch [44/100], Step [107/211], Loss: 0.2290\n",
            "Epoch [44/100], Step [108/211], Loss: 0.2490\n",
            "Epoch [44/100], Step [109/211], Loss: 0.2503\n",
            "Epoch [44/100], Step [110/211], Loss: 0.1084\n",
            "Epoch [44/100], Step [111/211], Loss: 0.2400\n",
            "Epoch [44/100], Step [112/211], Loss: 0.2378\n",
            "Epoch [44/100], Step [113/211], Loss: 0.2593\n",
            "Epoch [44/100], Step [114/211], Loss: 0.2100\n",
            "Epoch [44/100], Step [115/211], Loss: 0.1836\n",
            "Epoch [44/100], Step [116/211], Loss: 0.2056\n",
            "Epoch [44/100], Step [117/211], Loss: 0.2120\n",
            "Epoch [44/100], Step [118/211], Loss: 0.2908\n",
            "Epoch [44/100], Step [119/211], Loss: 0.2265\n",
            "Epoch [44/100], Step [120/211], Loss: 0.1959\n",
            "Epoch [44/100], Step [121/211], Loss: 0.3218\n",
            "Epoch [44/100], Step [122/211], Loss: 0.3224\n",
            "Epoch [44/100], Step [123/211], Loss: 0.2058\n",
            "Epoch [44/100], Step [124/211], Loss: 0.1453\n",
            "Epoch [44/100], Step [125/211], Loss: 0.2065\n",
            "Epoch [44/100], Step [126/211], Loss: 0.2678\n",
            "Epoch [44/100], Step [127/211], Loss: 0.2463\n",
            "Epoch [44/100], Step [128/211], Loss: 0.4350\n",
            "Epoch [44/100], Step [129/211], Loss: 0.1547\n",
            "Epoch [44/100], Step [130/211], Loss: 0.1872\n",
            "Epoch [44/100], Step [131/211], Loss: 0.3520\n",
            "Epoch [44/100], Step [132/211], Loss: 0.2500\n",
            "Epoch [44/100], Step [133/211], Loss: 0.2503\n",
            "Epoch [44/100], Step [134/211], Loss: 0.2034\n",
            "Epoch [44/100], Step [135/211], Loss: 0.4677\n",
            "Epoch [44/100], Step [136/211], Loss: 0.2088\n",
            "Epoch [44/100], Step [137/211], Loss: 0.1414\n",
            "Epoch [44/100], Step [138/211], Loss: 0.3799\n",
            "Epoch [44/100], Step [139/211], Loss: 0.2885\n",
            "Epoch [44/100], Step [140/211], Loss: 0.1775\n",
            "Epoch [44/100], Step [141/211], Loss: 0.3365\n",
            "Epoch [44/100], Step [142/211], Loss: 0.1648\n",
            "Epoch [44/100], Step [143/211], Loss: 0.2533\n",
            "Epoch [44/100], Step [144/211], Loss: 0.2277\n",
            "Epoch [44/100], Step [145/211], Loss: 0.2337\n",
            "Epoch [44/100], Step [146/211], Loss: 0.1350\n",
            "Epoch [44/100], Step [147/211], Loss: 0.2393\n",
            "Epoch [44/100], Step [148/211], Loss: 0.3210\n",
            "Epoch [44/100], Step [149/211], Loss: 0.1158\n",
            "Epoch [44/100], Step [150/211], Loss: 0.7008\n",
            "Epoch [44/100], Step [151/211], Loss: 0.3549\n",
            "Epoch [44/100], Step [152/211], Loss: 0.2718\n",
            "Epoch [44/100], Step [153/211], Loss: 0.2824\n",
            "Epoch [44/100], Step [154/211], Loss: 0.2985\n",
            "Epoch [44/100], Step [155/211], Loss: 0.3288\n",
            "Epoch [44/100], Step [156/211], Loss: 0.1654\n",
            "Epoch [44/100], Step [157/211], Loss: 0.1482\n",
            "Epoch [44/100], Step [158/211], Loss: 0.2587\n",
            "Epoch [44/100], Step [159/211], Loss: 0.1702\n",
            "Epoch [44/100], Step [160/211], Loss: 0.1783\n",
            "Epoch [44/100], Step [161/211], Loss: 0.4417\n",
            "Epoch [44/100], Step [162/211], Loss: 0.3310\n",
            "Epoch [44/100], Step [163/211], Loss: 0.1197\n",
            "Epoch [44/100], Step [164/211], Loss: 0.1087\n",
            "Epoch [44/100], Step [165/211], Loss: 0.2450\n",
            "Epoch [44/100], Step [166/211], Loss: 0.3321\n",
            "Epoch [44/100], Step [167/211], Loss: 0.2116\n",
            "Epoch [44/100], Step [168/211], Loss: 0.3500\n",
            "Epoch [44/100], Step [169/211], Loss: 0.2249\n",
            "Epoch [44/100], Step [170/211], Loss: 0.2821\n",
            "Epoch [44/100], Step [171/211], Loss: 0.3685\n",
            "Epoch [44/100], Step [172/211], Loss: 0.2038\n",
            "Epoch [44/100], Step [173/211], Loss: 0.3925\n",
            "Epoch [44/100], Step [174/211], Loss: 0.1857\n",
            "Epoch [44/100], Step [175/211], Loss: 0.0857\n",
            "Epoch [44/100], Step [176/211], Loss: 0.4087\n",
            "Epoch [44/100], Step [177/211], Loss: 0.3391\n",
            "Epoch [44/100], Step [178/211], Loss: 0.4478\n",
            "Epoch [44/100], Step [179/211], Loss: 0.2817\n",
            "Epoch [44/100], Step [180/211], Loss: 0.3144\n",
            "Epoch [44/100], Step [181/211], Loss: 0.2536\n",
            "Epoch [44/100], Step [182/211], Loss: 0.2208\n",
            "Epoch [44/100], Step [183/211], Loss: 0.1926\n",
            "Epoch [44/100], Step [184/211], Loss: 0.1768\n",
            "Epoch [44/100], Step [185/211], Loss: 0.2011\n",
            "Epoch [44/100], Step [186/211], Loss: 0.1199\n",
            "Epoch [44/100], Step [187/211], Loss: 0.1724\n",
            "Epoch [44/100], Step [188/211], Loss: 0.3135\n",
            "Epoch [44/100], Step [189/211], Loss: 0.2709\n",
            "Epoch [44/100], Step [190/211], Loss: 0.3539\n",
            "Epoch [44/100], Step [191/211], Loss: 0.4305\n",
            "Epoch [44/100], Step [192/211], Loss: 0.1307\n",
            "Epoch [44/100], Step [193/211], Loss: 0.1642\n",
            "Epoch [44/100], Step [194/211], Loss: 0.1360\n",
            "Epoch [44/100], Step [195/211], Loss: 0.2270\n",
            "Epoch [44/100], Step [196/211], Loss: 0.2658\n",
            "Epoch [44/100], Step [197/211], Loss: 0.2899\n",
            "Epoch [44/100], Step [198/211], Loss: 0.1685\n",
            "Epoch [44/100], Step [199/211], Loss: 0.2004\n",
            "Epoch [44/100], Step [200/211], Loss: 0.2287\n",
            "Epoch [44/100], Step [201/211], Loss: 0.1153\n",
            "Epoch [44/100], Step [202/211], Loss: 0.2271\n",
            "Epoch [44/100], Step [203/211], Loss: 0.2677\n",
            "Epoch [44/100], Step [204/211], Loss: 0.2625\n",
            "Epoch [44/100], Step [205/211], Loss: 0.2742\n",
            "Epoch [44/100], Step [206/211], Loss: 0.1298\n",
            "Epoch [44/100], Step [207/211], Loss: 0.1481\n",
            "Epoch [44/100], Step [208/211], Loss: 0.1012\n",
            "Epoch [44/100], Step [209/211], Loss: 0.4823\n",
            "Epoch [44/100], Step [210/211], Loss: 0.5293\n",
            "Epoch [44/100], Step [211/211], Loss: 0.7100\n",
            "Loss media: 0.2592\n",
            "Epoch [45/100], Step [1/211], Loss: 0.3199\n",
            "Epoch [45/100], Step [2/211], Loss: 0.2854\n",
            "Epoch [45/100], Step [3/211], Loss: 0.1538\n",
            "Epoch [45/100], Step [4/211], Loss: 0.1549\n",
            "Epoch [45/100], Step [5/211], Loss: 0.2191\n",
            "Epoch [45/100], Step [6/211], Loss: 0.2774\n",
            "Epoch [45/100], Step [7/211], Loss: 0.1339\n",
            "Epoch [45/100], Step [8/211], Loss: 0.4113\n",
            "Epoch [45/100], Step [9/211], Loss: 0.2104\n",
            "Epoch [45/100], Step [10/211], Loss: 0.3263\n",
            "Epoch [45/100], Step [11/211], Loss: 0.3885\n",
            "Epoch [45/100], Step [12/211], Loss: 0.8145\n",
            "Epoch [45/100], Step [13/211], Loss: 0.1387\n",
            "Epoch [45/100], Step [14/211], Loss: 0.1964\n",
            "Epoch [45/100], Step [15/211], Loss: 0.1481\n",
            "Epoch [45/100], Step [16/211], Loss: 0.2756\n",
            "Epoch [45/100], Step [17/211], Loss: 0.4849\n",
            "Epoch [45/100], Step [18/211], Loss: 0.3136\n",
            "Epoch [45/100], Step [19/211], Loss: 0.2147\n",
            "Epoch [45/100], Step [20/211], Loss: 0.3161\n",
            "Epoch [45/100], Step [21/211], Loss: 0.3196\n",
            "Epoch [45/100], Step [22/211], Loss: 0.1860\n",
            "Epoch [45/100], Step [23/211], Loss: 0.2092\n",
            "Epoch [45/100], Step [24/211], Loss: 0.3198\n",
            "Epoch [45/100], Step [25/211], Loss: 0.2598\n",
            "Epoch [45/100], Step [26/211], Loss: 0.3474\n",
            "Epoch [45/100], Step [27/211], Loss: 0.1165\n",
            "Epoch [45/100], Step [28/211], Loss: 0.3025\n",
            "Epoch [45/100], Step [29/211], Loss: 0.1077\n",
            "Epoch [45/100], Step [30/211], Loss: 0.1623\n",
            "Epoch [45/100], Step [31/211], Loss: 0.4857\n",
            "Epoch [45/100], Step [32/211], Loss: 0.2825\n",
            "Epoch [45/100], Step [33/211], Loss: 0.2068\n",
            "Epoch [45/100], Step [34/211], Loss: 0.1578\n",
            "Epoch [45/100], Step [35/211], Loss: 0.2422\n",
            "Epoch [45/100], Step [36/211], Loss: 0.1305\n",
            "Epoch [45/100], Step [37/211], Loss: 0.2701\n",
            "Epoch [45/100], Step [38/211], Loss: 0.2717\n",
            "Epoch [45/100], Step [39/211], Loss: 0.1068\n",
            "Epoch [45/100], Step [40/211], Loss: 0.2297\n",
            "Epoch [45/100], Step [41/211], Loss: 0.2847\n",
            "Epoch [45/100], Step [42/211], Loss: 0.3149\n",
            "Epoch [45/100], Step [43/211], Loss: 0.1744\n",
            "Epoch [45/100], Step [44/211], Loss: 0.2569\n",
            "Epoch [45/100], Step [45/211], Loss: 0.2878\n",
            "Epoch [45/100], Step [46/211], Loss: 0.1752\n",
            "Epoch [45/100], Step [47/211], Loss: 0.2013\n",
            "Epoch [45/100], Step [48/211], Loss: 0.3836\n",
            "Epoch [45/100], Step [49/211], Loss: 0.0973\n",
            "Epoch [45/100], Step [50/211], Loss: 0.1755\n",
            "Epoch [45/100], Step [51/211], Loss: 0.1241\n",
            "Epoch [45/100], Step [52/211], Loss: 0.2513\n",
            "Epoch [45/100], Step [53/211], Loss: 0.1213\n",
            "Epoch [45/100], Step [54/211], Loss: 0.2264\n",
            "Epoch [45/100], Step [55/211], Loss: 0.3800\n",
            "Epoch [45/100], Step [56/211], Loss: 0.1466\n",
            "Epoch [45/100], Step [57/211], Loss: 0.3413\n",
            "Epoch [45/100], Step [58/211], Loss: 0.3361\n",
            "Epoch [45/100], Step [59/211], Loss: 0.1179\n",
            "Epoch [45/100], Step [60/211], Loss: 0.2738\n",
            "Epoch [45/100], Step [61/211], Loss: 0.1929\n",
            "Epoch [45/100], Step [62/211], Loss: 0.3079\n",
            "Epoch [45/100], Step [63/211], Loss: 0.2127\n",
            "Epoch [45/100], Step [64/211], Loss: 0.2295\n",
            "Epoch [45/100], Step [65/211], Loss: 0.2806\n",
            "Epoch [45/100], Step [66/211], Loss: 0.1739\n",
            "Epoch [45/100], Step [67/211], Loss: 0.1958\n",
            "Epoch [45/100], Step [68/211], Loss: 0.1468\n",
            "Epoch [45/100], Step [69/211], Loss: 0.2623\n",
            "Epoch [45/100], Step [70/211], Loss: 0.5113\n",
            "Epoch [45/100], Step [71/211], Loss: 0.1761\n",
            "Epoch [45/100], Step [72/211], Loss: 0.3727\n",
            "Epoch [45/100], Step [73/211], Loss: 0.0889\n",
            "Epoch [45/100], Step [74/211], Loss: 0.2177\n",
            "Epoch [45/100], Step [75/211], Loss: 0.3280\n",
            "Epoch [45/100], Step [76/211], Loss: 0.4110\n",
            "Epoch [45/100], Step [77/211], Loss: 0.3302\n",
            "Epoch [45/100], Step [78/211], Loss: 0.2419\n",
            "Epoch [45/100], Step [79/211], Loss: 0.1997\n",
            "Epoch [45/100], Step [80/211], Loss: 0.1872\n",
            "Epoch [45/100], Step [81/211], Loss: 0.1754\n",
            "Epoch [45/100], Step [82/211], Loss: 0.4253\n",
            "Epoch [45/100], Step [83/211], Loss: 0.2874\n",
            "Epoch [45/100], Step [84/211], Loss: 0.1949\n",
            "Epoch [45/100], Step [85/211], Loss: 0.1924\n",
            "Epoch [45/100], Step [86/211], Loss: 0.1550\n",
            "Epoch [45/100], Step [87/211], Loss: 0.1540\n",
            "Epoch [45/100], Step [88/211], Loss: 0.7401\n",
            "Epoch [45/100], Step [89/211], Loss: 0.2644\n",
            "Epoch [45/100], Step [90/211], Loss: 0.2100\n",
            "Epoch [45/100], Step [91/211], Loss: 0.1117\n",
            "Epoch [45/100], Step [92/211], Loss: 0.2255\n",
            "Epoch [45/100], Step [93/211], Loss: 0.3720\n",
            "Epoch [45/100], Step [94/211], Loss: 0.3934\n",
            "Epoch [45/100], Step [95/211], Loss: 0.2493\n",
            "Epoch [45/100], Step [96/211], Loss: 0.1652\n",
            "Epoch [45/100], Step [97/211], Loss: 0.2159\n",
            "Epoch [45/100], Step [98/211], Loss: 0.1908\n",
            "Epoch [45/100], Step [99/211], Loss: 0.2027\n",
            "Epoch [45/100], Step [100/211], Loss: 0.3366\n",
            "Epoch [45/100], Step [101/211], Loss: 0.1376\n",
            "Epoch [45/100], Step [102/211], Loss: 0.4544\n",
            "Epoch [45/100], Step [103/211], Loss: 0.2409\n",
            "Epoch [45/100], Step [104/211], Loss: 0.3776\n",
            "Epoch [45/100], Step [105/211], Loss: 0.1921\n",
            "Epoch [45/100], Step [106/211], Loss: 0.2721\n",
            "Epoch [45/100], Step [107/211], Loss: 0.2600\n",
            "Epoch [45/100], Step [108/211], Loss: 0.4258\n",
            "Epoch [45/100], Step [109/211], Loss: 0.2911\n",
            "Epoch [45/100], Step [110/211], Loss: 0.1652\n",
            "Epoch [45/100], Step [111/211], Loss: 0.2740\n",
            "Epoch [45/100], Step [112/211], Loss: 0.2925\n",
            "Epoch [45/100], Step [113/211], Loss: 0.1718\n",
            "Epoch [45/100], Step [114/211], Loss: 0.2116\n",
            "Epoch [45/100], Step [115/211], Loss: 0.1485\n",
            "Epoch [45/100], Step [116/211], Loss: 0.1487\n",
            "Epoch [45/100], Step [117/211], Loss: 0.2493\n",
            "Epoch [45/100], Step [118/211], Loss: 0.3106\n",
            "Epoch [45/100], Step [119/211], Loss: 0.1322\n",
            "Epoch [45/100], Step [120/211], Loss: 0.1866\n",
            "Epoch [45/100], Step [121/211], Loss: 0.2067\n",
            "Epoch [45/100], Step [122/211], Loss: 0.1956\n",
            "Epoch [45/100], Step [123/211], Loss: 0.2260\n",
            "Epoch [45/100], Step [124/211], Loss: 0.3297\n",
            "Epoch [45/100], Step [125/211], Loss: 0.1968\n",
            "Epoch [45/100], Step [126/211], Loss: 0.3359\n",
            "Epoch [45/100], Step [127/211], Loss: 0.1493\n",
            "Epoch [45/100], Step [128/211], Loss: 0.1839\n",
            "Epoch [45/100], Step [129/211], Loss: 0.1903\n",
            "Epoch [45/100], Step [130/211], Loss: 0.2856\n",
            "Epoch [45/100], Step [131/211], Loss: 0.3632\n",
            "Epoch [45/100], Step [132/211], Loss: 0.4385\n",
            "Epoch [45/100], Step [133/211], Loss: 0.1952\n",
            "Epoch [45/100], Step [134/211], Loss: 0.2604\n",
            "Epoch [45/100], Step [135/211], Loss: 0.1571\n",
            "Epoch [45/100], Step [136/211], Loss: 0.4994\n",
            "Epoch [45/100], Step [137/211], Loss: 0.2340\n",
            "Epoch [45/100], Step [138/211], Loss: 0.3815\n",
            "Epoch [45/100], Step [139/211], Loss: 0.3810\n",
            "Epoch [45/100], Step [140/211], Loss: 0.1015\n",
            "Epoch [45/100], Step [141/211], Loss: 0.2545\n",
            "Epoch [45/100], Step [142/211], Loss: 0.2521\n",
            "Epoch [45/100], Step [143/211], Loss: 0.2614\n",
            "Epoch [45/100], Step [144/211], Loss: 0.3049\n",
            "Epoch [45/100], Step [145/211], Loss: 0.1521\n",
            "Epoch [45/100], Step [146/211], Loss: 0.2363\n",
            "Epoch [45/100], Step [147/211], Loss: 0.1301\n",
            "Epoch [45/100], Step [148/211], Loss: 0.2863\n",
            "Epoch [45/100], Step [149/211], Loss: 0.3664\n",
            "Epoch [45/100], Step [150/211], Loss: 0.2206\n",
            "Epoch [45/100], Step [151/211], Loss: 0.3100\n",
            "Epoch [45/100], Step [152/211], Loss: 0.1330\n",
            "Epoch [45/100], Step [153/211], Loss: 0.1777\n",
            "Epoch [45/100], Step [154/211], Loss: 0.3274\n",
            "Epoch [45/100], Step [155/211], Loss: 0.1139\n",
            "Epoch [45/100], Step [156/211], Loss: 0.3311\n",
            "Epoch [45/100], Step [157/211], Loss: 0.2793\n",
            "Epoch [45/100], Step [158/211], Loss: 0.2583\n",
            "Epoch [45/100], Step [159/211], Loss: 0.3343\n",
            "Epoch [45/100], Step [160/211], Loss: 0.1584\n",
            "Epoch [45/100], Step [161/211], Loss: 0.2578\n",
            "Epoch [45/100], Step [162/211], Loss: 0.4191\n",
            "Epoch [45/100], Step [163/211], Loss: 0.2328\n",
            "Epoch [45/100], Step [164/211], Loss: 0.2831\n",
            "Epoch [45/100], Step [165/211], Loss: 0.2015\n",
            "Epoch [45/100], Step [166/211], Loss: 0.1472\n",
            "Epoch [45/100], Step [167/211], Loss: 0.2099\n",
            "Epoch [45/100], Step [168/211], Loss: 0.3053\n",
            "Epoch [45/100], Step [169/211], Loss: 0.3383\n",
            "Epoch [45/100], Step [170/211], Loss: 0.2403\n",
            "Epoch [45/100], Step [171/211], Loss: 0.4034\n",
            "Epoch [45/100], Step [172/211], Loss: 0.2787\n",
            "Epoch [45/100], Step [173/211], Loss: 0.1180\n",
            "Epoch [45/100], Step [174/211], Loss: 0.4032\n",
            "Epoch [45/100], Step [175/211], Loss: 0.3628\n",
            "Epoch [45/100], Step [176/211], Loss: 0.1201\n",
            "Epoch [45/100], Step [177/211], Loss: 0.1736\n",
            "Epoch [45/100], Step [178/211], Loss: 0.3622\n",
            "Epoch [45/100], Step [179/211], Loss: 0.2225\n",
            "Epoch [45/100], Step [180/211], Loss: 0.3196\n",
            "Epoch [45/100], Step [181/211], Loss: 0.3454\n",
            "Epoch [45/100], Step [182/211], Loss: 0.1844\n",
            "Epoch [45/100], Step [183/211], Loss: 0.2861\n",
            "Epoch [45/100], Step [184/211], Loss: 0.2004\n",
            "Epoch [45/100], Step [185/211], Loss: 0.2617\n",
            "Epoch [45/100], Step [186/211], Loss: 0.2471\n",
            "Epoch [45/100], Step [187/211], Loss: 0.2967\n",
            "Epoch [45/100], Step [188/211], Loss: 0.1769\n",
            "Epoch [45/100], Step [189/211], Loss: 0.2401\n",
            "Epoch [45/100], Step [190/211], Loss: 0.4055\n",
            "Epoch [45/100], Step [191/211], Loss: 0.2066\n",
            "Epoch [45/100], Step [192/211], Loss: 0.2336\n",
            "Epoch [45/100], Step [193/211], Loss: 0.2945\n",
            "Epoch [45/100], Step [194/211], Loss: 0.2190\n",
            "Epoch [45/100], Step [195/211], Loss: 0.4818\n",
            "Epoch [45/100], Step [196/211], Loss: 0.1847\n",
            "Epoch [45/100], Step [197/211], Loss: 0.1822\n",
            "Epoch [45/100], Step [198/211], Loss: 0.5255\n",
            "Epoch [45/100], Step [199/211], Loss: 0.3534\n",
            "Epoch [45/100], Step [200/211], Loss: 0.1737\n",
            "Epoch [45/100], Step [201/211], Loss: 0.2802\n",
            "Epoch [45/100], Step [202/211], Loss: 0.1119\n",
            "Epoch [45/100], Step [203/211], Loss: 0.2618\n",
            "Epoch [45/100], Step [204/211], Loss: 0.1628\n",
            "Epoch [45/100], Step [205/211], Loss: 0.3333\n",
            "Epoch [45/100], Step [206/211], Loss: 0.3444\n",
            "Epoch [45/100], Step [207/211], Loss: 0.1051\n",
            "Epoch [45/100], Step [208/211], Loss: 0.2573\n",
            "Epoch [45/100], Step [209/211], Loss: 0.2013\n",
            "Epoch [45/100], Step [210/211], Loss: 0.2402\n",
            "Epoch [45/100], Step [211/211], Loss: 0.3054\n",
            "Loss media: 0.2566\n",
            "Epoch [46/100], Step [1/211], Loss: 0.1613\n",
            "Epoch [46/100], Step [2/211], Loss: 0.1664\n",
            "Epoch [46/100], Step [3/211], Loss: 0.3285\n",
            "Epoch [46/100], Step [4/211], Loss: 0.2106\n",
            "Epoch [46/100], Step [5/211], Loss: 0.0997\n",
            "Epoch [46/100], Step [6/211], Loss: 0.1811\n",
            "Epoch [46/100], Step [7/211], Loss: 0.4419\n",
            "Epoch [46/100], Step [8/211], Loss: 0.3092\n",
            "Epoch [46/100], Step [9/211], Loss: 0.1821\n",
            "Epoch [46/100], Step [10/211], Loss: 0.1925\n",
            "Epoch [46/100], Step [11/211], Loss: 0.0955\n",
            "Epoch [46/100], Step [12/211], Loss: 0.2030\n",
            "Epoch [46/100], Step [13/211], Loss: 0.1674\n",
            "Epoch [46/100], Step [14/211], Loss: 0.2034\n",
            "Epoch [46/100], Step [15/211], Loss: 0.1241\n",
            "Epoch [46/100], Step [16/211], Loss: 0.2169\n",
            "Epoch [46/100], Step [17/211], Loss: 0.2212\n",
            "Epoch [46/100], Step [18/211], Loss: 0.1822\n",
            "Epoch [46/100], Step [19/211], Loss: 0.2221\n",
            "Epoch [46/100], Step [20/211], Loss: 0.2726\n",
            "Epoch [46/100], Step [21/211], Loss: 0.2392\n",
            "Epoch [46/100], Step [22/211], Loss: 0.1600\n",
            "Epoch [46/100], Step [23/211], Loss: 0.1382\n",
            "Epoch [46/100], Step [24/211], Loss: 0.2254\n",
            "Epoch [46/100], Step [25/211], Loss: 0.1320\n",
            "Epoch [46/100], Step [26/211], Loss: 0.3455\n",
            "Epoch [46/100], Step [27/211], Loss: 0.1206\n",
            "Epoch [46/100], Step [28/211], Loss: 0.1787\n",
            "Epoch [46/100], Step [29/211], Loss: 0.2742\n",
            "Epoch [46/100], Step [30/211], Loss: 0.1766\n",
            "Epoch [46/100], Step [31/211], Loss: 0.2260\n",
            "Epoch [46/100], Step [32/211], Loss: 0.3472\n",
            "Epoch [46/100], Step [33/211], Loss: 0.2400\n",
            "Epoch [46/100], Step [34/211], Loss: 0.1956\n",
            "Epoch [46/100], Step [35/211], Loss: 0.2117\n",
            "Epoch [46/100], Step [36/211], Loss: 0.2656\n",
            "Epoch [46/100], Step [37/211], Loss: 0.1566\n",
            "Epoch [46/100], Step [38/211], Loss: 0.2440\n",
            "Epoch [46/100], Step [39/211], Loss: 0.1407\n",
            "Epoch [46/100], Step [40/211], Loss: 0.2904\n",
            "Epoch [46/100], Step [41/211], Loss: 0.2578\n",
            "Epoch [46/100], Step [42/211], Loss: 0.3590\n",
            "Epoch [46/100], Step [43/211], Loss: 0.3707\n",
            "Epoch [46/100], Step [44/211], Loss: 0.1960\n",
            "Epoch [46/100], Step [45/211], Loss: 0.2231\n",
            "Epoch [46/100], Step [46/211], Loss: 0.2094\n",
            "Epoch [46/100], Step [47/211], Loss: 0.3154\n",
            "Epoch [46/100], Step [48/211], Loss: 0.1477\n",
            "Epoch [46/100], Step [49/211], Loss: 0.3366\n",
            "Epoch [46/100], Step [50/211], Loss: 0.4205\n",
            "Epoch [46/100], Step [51/211], Loss: 0.1888\n",
            "Epoch [46/100], Step [52/211], Loss: 0.3441\n",
            "Epoch [46/100], Step [53/211], Loss: 0.2930\n",
            "Epoch [46/100], Step [54/211], Loss: 0.1742\n",
            "Epoch [46/100], Step [55/211], Loss: 0.1655\n",
            "Epoch [46/100], Step [56/211], Loss: 0.2165\n",
            "Epoch [46/100], Step [57/211], Loss: 0.1076\n",
            "Epoch [46/100], Step [58/211], Loss: 0.2859\n",
            "Epoch [46/100], Step [59/211], Loss: 0.2479\n",
            "Epoch [46/100], Step [60/211], Loss: 0.3607\n",
            "Epoch [46/100], Step [61/211], Loss: 0.3649\n",
            "Epoch [46/100], Step [62/211], Loss: 0.2895\n",
            "Epoch [46/100], Step [63/211], Loss: 0.1849\n",
            "Epoch [46/100], Step [64/211], Loss: 0.1921\n",
            "Epoch [46/100], Step [65/211], Loss: 0.1236\n",
            "Epoch [46/100], Step [66/211], Loss: 0.0946\n",
            "Epoch [46/100], Step [67/211], Loss: 0.3632\n",
            "Epoch [46/100], Step [68/211], Loss: 0.3256\n",
            "Epoch [46/100], Step [69/211], Loss: 0.2835\n",
            "Epoch [46/100], Step [70/211], Loss: 0.2374\n",
            "Epoch [46/100], Step [71/211], Loss: 0.1568\n",
            "Epoch [46/100], Step [72/211], Loss: 0.2058\n",
            "Epoch [46/100], Step [73/211], Loss: 0.1982\n",
            "Epoch [46/100], Step [74/211], Loss: 0.2755\n",
            "Epoch [46/100], Step [75/211], Loss: 0.2379\n",
            "Epoch [46/100], Step [76/211], Loss: 0.3244\n",
            "Epoch [46/100], Step [77/211], Loss: 0.0952\n",
            "Epoch [46/100], Step [78/211], Loss: 0.1940\n",
            "Epoch [46/100], Step [79/211], Loss: 0.1476\n",
            "Epoch [46/100], Step [80/211], Loss: 0.2139\n",
            "Epoch [46/100], Step [81/211], Loss: 0.1636\n",
            "Epoch [46/100], Step [82/211], Loss: 0.2645\n",
            "Epoch [46/100], Step [83/211], Loss: 0.2227\n",
            "Epoch [46/100], Step [84/211], Loss: 0.1195\n",
            "Epoch [46/100], Step [85/211], Loss: 0.3319\n",
            "Epoch [46/100], Step [86/211], Loss: 0.1999\n",
            "Epoch [46/100], Step [87/211], Loss: 0.1660\n",
            "Epoch [46/100], Step [88/211], Loss: 0.3577\n",
            "Epoch [46/100], Step [89/211], Loss: 0.2382\n",
            "Epoch [46/100], Step [90/211], Loss: 0.3008\n",
            "Epoch [46/100], Step [91/211], Loss: 0.4356\n",
            "Epoch [46/100], Step [92/211], Loss: 0.4773\n",
            "Epoch [46/100], Step [93/211], Loss: 0.2843\n",
            "Epoch [46/100], Step [94/211], Loss: 0.3313\n",
            "Epoch [46/100], Step [95/211], Loss: 0.1283\n",
            "Epoch [46/100], Step [96/211], Loss: 0.1310\n",
            "Epoch [46/100], Step [97/211], Loss: 0.1604\n",
            "Epoch [46/100], Step [98/211], Loss: 0.2764\n",
            "Epoch [46/100], Step [99/211], Loss: 0.1231\n",
            "Epoch [46/100], Step [100/211], Loss: 0.1943\n",
            "Epoch [46/100], Step [101/211], Loss: 0.3115\n",
            "Epoch [46/100], Step [102/211], Loss: 0.1623\n",
            "Epoch [46/100], Step [103/211], Loss: 0.0780\n",
            "Epoch [46/100], Step [104/211], Loss: 0.4164\n",
            "Epoch [46/100], Step [105/211], Loss: 0.3145\n",
            "Epoch [46/100], Step [106/211], Loss: 0.3137\n",
            "Epoch [46/100], Step [107/211], Loss: 0.2940\n",
            "Epoch [46/100], Step [108/211], Loss: 0.0982\n",
            "Epoch [46/100], Step [109/211], Loss: 0.3173\n",
            "Epoch [46/100], Step [110/211], Loss: 0.2321\n",
            "Epoch [46/100], Step [111/211], Loss: 0.2195\n",
            "Epoch [46/100], Step [112/211], Loss: 0.3210\n",
            "Epoch [46/100], Step [113/211], Loss: 0.2166\n",
            "Epoch [46/100], Step [114/211], Loss: 0.1794\n",
            "Epoch [46/100], Step [115/211], Loss: 0.3041\n",
            "Epoch [46/100], Step [116/211], Loss: 0.2895\n",
            "Epoch [46/100], Step [117/211], Loss: 0.2670\n",
            "Epoch [46/100], Step [118/211], Loss: 0.1513\n",
            "Epoch [46/100], Step [119/211], Loss: 0.1989\n",
            "Epoch [46/100], Step [120/211], Loss: 0.1957\n",
            "Epoch [46/100], Step [121/211], Loss: 0.1677\n",
            "Epoch [46/100], Step [122/211], Loss: 0.1645\n",
            "Epoch [46/100], Step [123/211], Loss: 0.3064\n",
            "Epoch [46/100], Step [124/211], Loss: 0.1874\n",
            "Epoch [46/100], Step [125/211], Loss: 0.2623\n",
            "Epoch [46/100], Step [126/211], Loss: 0.1915\n",
            "Epoch [46/100], Step [127/211], Loss: 0.4221\n",
            "Epoch [46/100], Step [128/211], Loss: 0.2229\n",
            "Epoch [46/100], Step [129/211], Loss: 0.1003\n",
            "Epoch [46/100], Step [130/211], Loss: 0.2645\n",
            "Epoch [46/100], Step [131/211], Loss: 0.1431\n",
            "Epoch [46/100], Step [132/211], Loss: 0.2423\n",
            "Epoch [46/100], Step [133/211], Loss: 0.1975\n",
            "Epoch [46/100], Step [134/211], Loss: 0.7230\n",
            "Epoch [46/100], Step [135/211], Loss: 0.2088\n",
            "Epoch [46/100], Step [136/211], Loss: 0.2333\n",
            "Epoch [46/100], Step [137/211], Loss: 0.0989\n",
            "Epoch [46/100], Step [138/211], Loss: 0.5681\n",
            "Epoch [46/100], Step [139/211], Loss: 0.2015\n",
            "Epoch [46/100], Step [140/211], Loss: 0.2049\n",
            "Epoch [46/100], Step [141/211], Loss: 0.2323\n",
            "Epoch [46/100], Step [142/211], Loss: 0.4952\n",
            "Epoch [46/100], Step [143/211], Loss: 0.3228\n",
            "Epoch [46/100], Step [144/211], Loss: 0.2146\n",
            "Epoch [46/100], Step [145/211], Loss: 0.2870\n",
            "Epoch [46/100], Step [146/211], Loss: 0.5569\n",
            "Epoch [46/100], Step [147/211], Loss: 0.4005\n",
            "Epoch [46/100], Step [148/211], Loss: 0.2637\n",
            "Epoch [46/100], Step [149/211], Loss: 0.1303\n",
            "Epoch [46/100], Step [150/211], Loss: 0.1986\n",
            "Epoch [46/100], Step [151/211], Loss: 0.1240\n",
            "Epoch [46/100], Step [152/211], Loss: 0.2536\n",
            "Epoch [46/100], Step [153/211], Loss: 0.2652\n",
            "Epoch [46/100], Step [154/211], Loss: 0.6215\n",
            "Epoch [46/100], Step [155/211], Loss: 0.3662\n",
            "Epoch [46/100], Step [156/211], Loss: 0.3448\n",
            "Epoch [46/100], Step [157/211], Loss: 0.1548\n",
            "Epoch [46/100], Step [158/211], Loss: 0.1535\n",
            "Epoch [46/100], Step [159/211], Loss: 0.2178\n",
            "Epoch [46/100], Step [160/211], Loss: 0.1382\n",
            "Epoch [46/100], Step [161/211], Loss: 0.1368\n",
            "Epoch [46/100], Step [162/211], Loss: 0.2673\n",
            "Epoch [46/100], Step [163/211], Loss: 0.2222\n",
            "Epoch [46/100], Step [164/211], Loss: 0.2463\n",
            "Epoch [46/100], Step [165/211], Loss: 0.3392\n",
            "Epoch [46/100], Step [166/211], Loss: 0.2527\n",
            "Epoch [46/100], Step [167/211], Loss: 0.1754\n",
            "Epoch [46/100], Step [168/211], Loss: 0.2225\n",
            "Epoch [46/100], Step [169/211], Loss: 0.2038\n",
            "Epoch [46/100], Step [170/211], Loss: 0.2567\n",
            "Epoch [46/100], Step [171/211], Loss: 0.0963\n",
            "Epoch [46/100], Step [172/211], Loss: 0.4557\n",
            "Epoch [46/100], Step [173/211], Loss: 0.3019\n",
            "Epoch [46/100], Step [174/211], Loss: 0.2494\n",
            "Epoch [46/100], Step [175/211], Loss: 0.2501\n",
            "Epoch [46/100], Step [176/211], Loss: 0.1952\n",
            "Epoch [46/100], Step [177/211], Loss: 0.2138\n",
            "Epoch [46/100], Step [178/211], Loss: 0.1619\n",
            "Epoch [46/100], Step [179/211], Loss: 0.1455\n",
            "Epoch [46/100], Step [180/211], Loss: 0.0791\n",
            "Epoch [46/100], Step [181/211], Loss: 0.2203\n",
            "Epoch [46/100], Step [182/211], Loss: 0.1832\n",
            "Epoch [46/100], Step [183/211], Loss: 0.3796\n",
            "Epoch [46/100], Step [184/211], Loss: 0.5046\n",
            "Epoch [46/100], Step [185/211], Loss: 0.1493\n",
            "Epoch [46/100], Step [186/211], Loss: 0.5134\n",
            "Epoch [46/100], Step [187/211], Loss: 0.2069\n",
            "Epoch [46/100], Step [188/211], Loss: 0.1384\n",
            "Epoch [46/100], Step [189/211], Loss: 0.2537\n",
            "Epoch [46/100], Step [190/211], Loss: 0.1957\n",
            "Epoch [46/100], Step [191/211], Loss: 0.5287\n",
            "Epoch [46/100], Step [192/211], Loss: 0.2359\n",
            "Epoch [46/100], Step [193/211], Loss: 0.2016\n",
            "Epoch [46/100], Step [194/211], Loss: 0.3416\n",
            "Epoch [46/100], Step [195/211], Loss: 0.1650\n",
            "Epoch [46/100], Step [196/211], Loss: 0.1675\n",
            "Epoch [46/100], Step [197/211], Loss: 0.2386\n",
            "Epoch [46/100], Step [198/211], Loss: 0.3188\n",
            "Epoch [46/100], Step [199/211], Loss: 0.4092\n",
            "Epoch [46/100], Step [200/211], Loss: 0.4291\n",
            "Epoch [46/100], Step [201/211], Loss: 0.3674\n",
            "Epoch [46/100], Step [202/211], Loss: 0.2765\n",
            "Epoch [46/100], Step [203/211], Loss: 0.1594\n",
            "Epoch [46/100], Step [204/211], Loss: 0.3794\n",
            "Epoch [46/100], Step [205/211], Loss: 0.1363\n",
            "Epoch [46/100], Step [206/211], Loss: 0.2160\n",
            "Epoch [46/100], Step [207/211], Loss: 0.1817\n",
            "Epoch [46/100], Step [208/211], Loss: 0.2898\n",
            "Epoch [46/100], Step [209/211], Loss: 0.2409\n",
            "Epoch [46/100], Step [210/211], Loss: 0.4419\n",
            "Epoch [46/100], Step [211/211], Loss: 0.2755\n",
            "Loss media: 0.2468\n",
            "Epoch [47/100], Step [1/211], Loss: 0.2083\n",
            "Epoch [47/100], Step [2/211], Loss: 0.2785\n",
            "Epoch [47/100], Step [3/211], Loss: 0.2516\n",
            "Epoch [47/100], Step [4/211], Loss: 0.3019\n",
            "Epoch [47/100], Step [5/211], Loss: 0.2887\n",
            "Epoch [47/100], Step [6/211], Loss: 0.2346\n",
            "Epoch [47/100], Step [7/211], Loss: 0.1632\n",
            "Epoch [47/100], Step [8/211], Loss: 0.1572\n",
            "Epoch [47/100], Step [9/211], Loss: 0.0676\n",
            "Epoch [47/100], Step [10/211], Loss: 0.0879\n",
            "Epoch [47/100], Step [11/211], Loss: 0.4691\n",
            "Epoch [47/100], Step [12/211], Loss: 0.2626\n",
            "Epoch [47/100], Step [13/211], Loss: 0.2240\n",
            "Epoch [47/100], Step [14/211], Loss: 0.1188\n",
            "Epoch [47/100], Step [15/211], Loss: 0.2581\n",
            "Epoch [47/100], Step [16/211], Loss: 0.2233\n",
            "Epoch [47/100], Step [17/211], Loss: 0.1515\n",
            "Epoch [47/100], Step [18/211], Loss: 0.1415\n",
            "Epoch [47/100], Step [19/211], Loss: 0.1527\n",
            "Epoch [47/100], Step [20/211], Loss: 0.2821\n",
            "Epoch [47/100], Step [21/211], Loss: 0.2450\n",
            "Epoch [47/100], Step [22/211], Loss: 0.1952\n",
            "Epoch [47/100], Step [23/211], Loss: 0.3964\n",
            "Epoch [47/100], Step [24/211], Loss: 0.1440\n",
            "Epoch [47/100], Step [25/211], Loss: 0.3499\n",
            "Epoch [47/100], Step [26/211], Loss: 0.1894\n",
            "Epoch [47/100], Step [27/211], Loss: 0.1861\n",
            "Epoch [47/100], Step [28/211], Loss: 0.1697\n",
            "Epoch [47/100], Step [29/211], Loss: 0.3247\n",
            "Epoch [47/100], Step [30/211], Loss: 0.1006\n",
            "Epoch [47/100], Step [31/211], Loss: 0.1711\n",
            "Epoch [47/100], Step [32/211], Loss: 0.3978\n",
            "Epoch [47/100], Step [33/211], Loss: 0.1976\n",
            "Epoch [47/100], Step [34/211], Loss: 0.2174\n",
            "Epoch [47/100], Step [35/211], Loss: 0.3137\n",
            "Epoch [47/100], Step [36/211], Loss: 0.4444\n",
            "Epoch [47/100], Step [37/211], Loss: 0.1685\n",
            "Epoch [47/100], Step [38/211], Loss: 0.2719\n",
            "Epoch [47/100], Step [39/211], Loss: 0.4770\n",
            "Epoch [47/100], Step [40/211], Loss: 0.2699\n",
            "Epoch [47/100], Step [41/211], Loss: 0.2834\n",
            "Epoch [47/100], Step [42/211], Loss: 0.2296\n",
            "Epoch [47/100], Step [43/211], Loss: 0.1532\n",
            "Epoch [47/100], Step [44/211], Loss: 0.1416\n",
            "Epoch [47/100], Step [45/211], Loss: 0.2621\n",
            "Epoch [47/100], Step [46/211], Loss: 0.3442\n",
            "Epoch [47/100], Step [47/211], Loss: 0.1585\n",
            "Epoch [47/100], Step [48/211], Loss: 0.1826\n",
            "Epoch [47/100], Step [49/211], Loss: 0.3347\n",
            "Epoch [47/100], Step [50/211], Loss: 0.1391\n",
            "Epoch [47/100], Step [51/211], Loss: 0.2501\n",
            "Epoch [47/100], Step [52/211], Loss: 0.3597\n",
            "Epoch [47/100], Step [53/211], Loss: 0.1251\n",
            "Epoch [47/100], Step [54/211], Loss: 0.2406\n",
            "Epoch [47/100], Step [55/211], Loss: 0.1308\n",
            "Epoch [47/100], Step [56/211], Loss: 0.2957\n",
            "Epoch [47/100], Step [57/211], Loss: 0.2111\n",
            "Epoch [47/100], Step [58/211], Loss: 0.1011\n",
            "Epoch [47/100], Step [59/211], Loss: 0.2517\n",
            "Epoch [47/100], Step [60/211], Loss: 0.2222\n",
            "Epoch [47/100], Step [61/211], Loss: 0.2702\n",
            "Epoch [47/100], Step [62/211], Loss: 0.3966\n",
            "Epoch [47/100], Step [63/211], Loss: 0.1503\n",
            "Epoch [47/100], Step [64/211], Loss: 0.3163\n",
            "Epoch [47/100], Step [65/211], Loss: 0.1286\n",
            "Epoch [47/100], Step [66/211], Loss: 0.3008\n",
            "Epoch [47/100], Step [67/211], Loss: 0.1971\n",
            "Epoch [47/100], Step [68/211], Loss: 0.3504\n",
            "Epoch [47/100], Step [69/211], Loss: 0.5593\n",
            "Epoch [47/100], Step [70/211], Loss: 0.1971\n",
            "Epoch [47/100], Step [71/211], Loss: 0.3799\n",
            "Epoch [47/100], Step [72/211], Loss: 0.4075\n",
            "Epoch [47/100], Step [73/211], Loss: 0.2675\n",
            "Epoch [47/100], Step [74/211], Loss: 0.2974\n",
            "Epoch [47/100], Step [75/211], Loss: 0.2072\n",
            "Epoch [47/100], Step [76/211], Loss: 0.2489\n",
            "Epoch [47/100], Step [77/211], Loss: 0.1090\n",
            "Epoch [47/100], Step [78/211], Loss: 0.3222\n",
            "Epoch [47/100], Step [79/211], Loss: 0.1465\n",
            "Epoch [47/100], Step [80/211], Loss: 0.2991\n",
            "Epoch [47/100], Step [81/211], Loss: 0.2279\n",
            "Epoch [47/100], Step [82/211], Loss: 0.5650\n",
            "Epoch [47/100], Step [83/211], Loss: 0.2410\n",
            "Epoch [47/100], Step [84/211], Loss: 0.2543\n",
            "Epoch [47/100], Step [85/211], Loss: 0.1584\n",
            "Epoch [47/100], Step [86/211], Loss: 0.1902\n",
            "Epoch [47/100], Step [87/211], Loss: 0.3220\n",
            "Epoch [47/100], Step [88/211], Loss: 0.2105\n",
            "Epoch [47/100], Step [89/211], Loss: 0.3932\n",
            "Epoch [47/100], Step [90/211], Loss: 0.3060\n",
            "Epoch [47/100], Step [91/211], Loss: 0.2231\n",
            "Epoch [47/100], Step [92/211], Loss: 0.3198\n",
            "Epoch [47/100], Step [93/211], Loss: 0.1801\n",
            "Epoch [47/100], Step [94/211], Loss: 0.2060\n",
            "Epoch [47/100], Step [95/211], Loss: 0.1972\n",
            "Epoch [47/100], Step [96/211], Loss: 0.2971\n",
            "Epoch [47/100], Step [97/211], Loss: 0.1772\n",
            "Epoch [47/100], Step [98/211], Loss: 0.1718\n",
            "Epoch [47/100], Step [99/211], Loss: 0.1901\n",
            "Epoch [47/100], Step [100/211], Loss: 0.2870\n",
            "Epoch [47/100], Step [101/211], Loss: 0.2308\n",
            "Epoch [47/100], Step [102/211], Loss: 0.1639\n",
            "Epoch [47/100], Step [103/211], Loss: 0.1215\n",
            "Epoch [47/100], Step [104/211], Loss: 0.3500\n",
            "Epoch [47/100], Step [105/211], Loss: 0.1409\n",
            "Epoch [47/100], Step [106/211], Loss: 0.4482\n",
            "Epoch [47/100], Step [107/211], Loss: 0.2037\n",
            "Epoch [47/100], Step [108/211], Loss: 0.1678\n",
            "Epoch [47/100], Step [109/211], Loss: 0.1447\n",
            "Epoch [47/100], Step [110/211], Loss: 0.2271\n",
            "Epoch [47/100], Step [111/211], Loss: 0.1211\n",
            "Epoch [47/100], Step [112/211], Loss: 0.3148\n",
            "Epoch [47/100], Step [113/211], Loss: 0.1804\n",
            "Epoch [47/100], Step [114/211], Loss: 0.2430\n",
            "Epoch [47/100], Step [115/211], Loss: 0.2016\n",
            "Epoch [47/100], Step [116/211], Loss: 0.1875\n",
            "Epoch [47/100], Step [117/211], Loss: 0.2698\n",
            "Epoch [47/100], Step [118/211], Loss: 0.3118\n",
            "Epoch [47/100], Step [119/211], Loss: 0.1614\n",
            "Epoch [47/100], Step [120/211], Loss: 0.2872\n",
            "Epoch [47/100], Step [121/211], Loss: 0.1708\n",
            "Epoch [47/100], Step [122/211], Loss: 0.3418\n",
            "Epoch [47/100], Step [123/211], Loss: 0.3088\n",
            "Epoch [47/100], Step [124/211], Loss: 0.1066\n",
            "Epoch [47/100], Step [125/211], Loss: 0.3296\n",
            "Epoch [47/100], Step [126/211], Loss: 0.2389\n",
            "Epoch [47/100], Step [127/211], Loss: 0.1838\n",
            "Epoch [47/100], Step [128/211], Loss: 0.0774\n",
            "Epoch [47/100], Step [129/211], Loss: 0.3072\n",
            "Epoch [47/100], Step [130/211], Loss: 0.3054\n",
            "Epoch [47/100], Step [131/211], Loss: 0.3448\n",
            "Epoch [47/100], Step [132/211], Loss: 0.1889\n",
            "Epoch [47/100], Step [133/211], Loss: 0.1434\n",
            "Epoch [47/100], Step [134/211], Loss: 0.2235\n",
            "Epoch [47/100], Step [135/211], Loss: 0.3473\n",
            "Epoch [47/100], Step [136/211], Loss: 0.2350\n",
            "Epoch [47/100], Step [137/211], Loss: 0.1739\n",
            "Epoch [47/100], Step [138/211], Loss: 0.3165\n",
            "Epoch [47/100], Step [139/211], Loss: 0.2257\n",
            "Epoch [47/100], Step [140/211], Loss: 0.1662\n",
            "Epoch [47/100], Step [141/211], Loss: 0.1647\n",
            "Epoch [47/100], Step [142/211], Loss: 0.4321\n",
            "Epoch [47/100], Step [143/211], Loss: 0.1655\n",
            "Epoch [47/100], Step [144/211], Loss: 0.2153\n",
            "Epoch [47/100], Step [145/211], Loss: 0.1969\n",
            "Epoch [47/100], Step [146/211], Loss: 0.2226\n",
            "Epoch [47/100], Step [147/211], Loss: 0.1958\n",
            "Epoch [47/100], Step [148/211], Loss: 0.2268\n",
            "Epoch [47/100], Step [149/211], Loss: 0.1652\n",
            "Epoch [47/100], Step [150/211], Loss: 0.2460\n",
            "Epoch [47/100], Step [151/211], Loss: 0.1601\n",
            "Epoch [47/100], Step [152/211], Loss: 0.1464\n",
            "Epoch [47/100], Step [153/211], Loss: 0.3173\n",
            "Epoch [47/100], Step [154/211], Loss: 0.2966\n",
            "Epoch [47/100], Step [155/211], Loss: 0.2202\n",
            "Epoch [47/100], Step [156/211], Loss: 0.2701\n",
            "Epoch [47/100], Step [157/211], Loss: 0.6289\n",
            "Epoch [47/100], Step [158/211], Loss: 0.2352\n",
            "Epoch [47/100], Step [159/211], Loss: 0.1832\n",
            "Epoch [47/100], Step [160/211], Loss: 0.2204\n",
            "Epoch [47/100], Step [161/211], Loss: 0.1898\n",
            "Epoch [47/100], Step [162/211], Loss: 0.2369\n",
            "Epoch [47/100], Step [163/211], Loss: 0.4144\n",
            "Epoch [47/100], Step [164/211], Loss: 0.3734\n",
            "Epoch [47/100], Step [165/211], Loss: 0.2595\n",
            "Epoch [47/100], Step [166/211], Loss: 0.3104\n",
            "Epoch [47/100], Step [167/211], Loss: 0.2921\n",
            "Epoch [47/100], Step [168/211], Loss: 0.3283\n",
            "Epoch [47/100], Step [169/211], Loss: 0.3727\n",
            "Epoch [47/100], Step [170/211], Loss: 0.1271\n",
            "Epoch [47/100], Step [171/211], Loss: 0.3815\n",
            "Epoch [47/100], Step [172/211], Loss: 0.1136\n",
            "Epoch [47/100], Step [173/211], Loss: 0.3782\n",
            "Epoch [47/100], Step [174/211], Loss: 0.2200\n",
            "Epoch [47/100], Step [175/211], Loss: 0.3407\n",
            "Epoch [47/100], Step [176/211], Loss: 0.3041\n",
            "Epoch [47/100], Step [177/211], Loss: 0.2034\n",
            "Epoch [47/100], Step [178/211], Loss: 0.2012\n",
            "Epoch [47/100], Step [179/211], Loss: 0.3079\n",
            "Epoch [47/100], Step [180/211], Loss: 0.1778\n",
            "Epoch [47/100], Step [181/211], Loss: 0.5830\n",
            "Epoch [47/100], Step [182/211], Loss: 0.2227\n",
            "Epoch [47/100], Step [183/211], Loss: 0.2496\n",
            "Epoch [47/100], Step [184/211], Loss: 0.1550\n",
            "Epoch [47/100], Step [185/211], Loss: 0.0956\n",
            "Epoch [47/100], Step [186/211], Loss: 0.2898\n",
            "Epoch [47/100], Step [187/211], Loss: 0.1487\n",
            "Epoch [47/100], Step [188/211], Loss: 0.4664\n",
            "Epoch [47/100], Step [189/211], Loss: 0.2431\n",
            "Epoch [47/100], Step [190/211], Loss: 0.3227\n",
            "Epoch [47/100], Step [191/211], Loss: 0.1220\n",
            "Epoch [47/100], Step [192/211], Loss: 0.2305\n",
            "Epoch [47/100], Step [193/211], Loss: 0.0933\n",
            "Epoch [47/100], Step [194/211], Loss: 0.2056\n",
            "Epoch [47/100], Step [195/211], Loss: 0.3106\n",
            "Epoch [47/100], Step [196/211], Loss: 0.1780\n",
            "Epoch [47/100], Step [197/211], Loss: 0.2442\n",
            "Epoch [47/100], Step [198/211], Loss: 0.1184\n",
            "Epoch [47/100], Step [199/211], Loss: 0.0648\n",
            "Epoch [47/100], Step [200/211], Loss: 0.1473\n",
            "Epoch [47/100], Step [201/211], Loss: 0.3148\n",
            "Epoch [47/100], Step [202/211], Loss: 0.2623\n",
            "Epoch [47/100], Step [203/211], Loss: 0.3276\n",
            "Epoch [47/100], Step [204/211], Loss: 0.2792\n",
            "Epoch [47/100], Step [205/211], Loss: 0.2124\n",
            "Epoch [47/100], Step [206/211], Loss: 0.5489\n",
            "Epoch [47/100], Step [207/211], Loss: 0.1174\n",
            "Epoch [47/100], Step [208/211], Loss: 0.2477\n",
            "Epoch [47/100], Step [209/211], Loss: 0.2237\n",
            "Epoch [47/100], Step [210/211], Loss: 0.1972\n",
            "Epoch [47/100], Step [211/211], Loss: 0.1383\n",
            "Loss media: 0.2448\n",
            "Epoch [48/100], Step [1/211], Loss: 0.2069\n",
            "Epoch [48/100], Step [2/211], Loss: 0.3288\n",
            "Epoch [48/100], Step [3/211], Loss: 0.2372\n",
            "Epoch [48/100], Step [4/211], Loss: 0.2709\n",
            "Epoch [48/100], Step [5/211], Loss: 0.2293\n",
            "Epoch [48/100], Step [6/211], Loss: 0.3270\n",
            "Epoch [48/100], Step [7/211], Loss: 0.2184\n",
            "Epoch [48/100], Step [8/211], Loss: 0.2787\n",
            "Epoch [48/100], Step [9/211], Loss: 0.2710\n",
            "Epoch [48/100], Step [10/211], Loss: 0.3768\n",
            "Epoch [48/100], Step [11/211], Loss: 0.2441\n",
            "Epoch [48/100], Step [12/211], Loss: 0.1969\n",
            "Epoch [48/100], Step [13/211], Loss: 0.3543\n",
            "Epoch [48/100], Step [14/211], Loss: 0.1203\n",
            "Epoch [48/100], Step [15/211], Loss: 0.2587\n",
            "Epoch [48/100], Step [16/211], Loss: 0.2106\n",
            "Epoch [48/100], Step [17/211], Loss: 0.4186\n",
            "Epoch [48/100], Step [18/211], Loss: 0.3003\n",
            "Epoch [48/100], Step [19/211], Loss: 0.3391\n",
            "Epoch [48/100], Step [20/211], Loss: 0.1877\n",
            "Epoch [48/100], Step [21/211], Loss: 0.4949\n",
            "Epoch [48/100], Step [22/211], Loss: 0.2055\n",
            "Epoch [48/100], Step [23/211], Loss: 0.3799\n",
            "Epoch [48/100], Step [24/211], Loss: 0.2060\n",
            "Epoch [48/100], Step [25/211], Loss: 0.4252\n",
            "Epoch [48/100], Step [26/211], Loss: 0.1316\n",
            "Epoch [48/100], Step [27/211], Loss: 0.1632\n",
            "Epoch [48/100], Step [28/211], Loss: 0.3165\n",
            "Epoch [48/100], Step [29/211], Loss: 0.2752\n",
            "Epoch [48/100], Step [30/211], Loss: 0.2898\n",
            "Epoch [48/100], Step [31/211], Loss: 0.2082\n",
            "Epoch [48/100], Step [32/211], Loss: 0.4406\n",
            "Epoch [48/100], Step [33/211], Loss: 0.3897\n",
            "Epoch [48/100], Step [34/211], Loss: 0.1952\n",
            "Epoch [48/100], Step [35/211], Loss: 0.2164\n",
            "Epoch [48/100], Step [36/211], Loss: 0.3268\n",
            "Epoch [48/100], Step [37/211], Loss: 0.1696\n",
            "Epoch [48/100], Step [38/211], Loss: 0.3058\n",
            "Epoch [48/100], Step [39/211], Loss: 0.3121\n",
            "Epoch [48/100], Step [40/211], Loss: 0.1719\n",
            "Epoch [48/100], Step [41/211], Loss: 0.3139\n",
            "Epoch [48/100], Step [42/211], Loss: 0.1444\n",
            "Epoch [48/100], Step [43/211], Loss: 0.3588\n",
            "Epoch [48/100], Step [44/211], Loss: 0.3341\n",
            "Epoch [48/100], Step [45/211], Loss: 0.1861\n",
            "Epoch [48/100], Step [46/211], Loss: 0.2054\n",
            "Epoch [48/100], Step [47/211], Loss: 0.1714\n",
            "Epoch [48/100], Step [48/211], Loss: 0.1638\n",
            "Epoch [48/100], Step [49/211], Loss: 0.1852\n",
            "Epoch [48/100], Step [50/211], Loss: 0.1148\n",
            "Epoch [48/100], Step [51/211], Loss: 0.2447\n",
            "Epoch [48/100], Step [52/211], Loss: 0.2612\n",
            "Epoch [48/100], Step [53/211], Loss: 0.3028\n",
            "Epoch [48/100], Step [54/211], Loss: 0.2658\n",
            "Epoch [48/100], Step [55/211], Loss: 0.1298\n",
            "Epoch [48/100], Step [56/211], Loss: 0.4158\n",
            "Epoch [48/100], Step [57/211], Loss: 0.2420\n",
            "Epoch [48/100], Step [58/211], Loss: 0.3135\n",
            "Epoch [48/100], Step [59/211], Loss: 0.3469\n",
            "Epoch [48/100], Step [60/211], Loss: 0.1594\n",
            "Epoch [48/100], Step [61/211], Loss: 0.1912\n",
            "Epoch [48/100], Step [62/211], Loss: 0.1378\n",
            "Epoch [48/100], Step [63/211], Loss: 0.2481\n",
            "Epoch [48/100], Step [64/211], Loss: 0.3024\n",
            "Epoch [48/100], Step [65/211], Loss: 0.1283\n",
            "Epoch [48/100], Step [66/211], Loss: 0.2522\n",
            "Epoch [48/100], Step [67/211], Loss: 0.2380\n",
            "Epoch [48/100], Step [68/211], Loss: 0.1947\n",
            "Epoch [48/100], Step [69/211], Loss: 0.1171\n",
            "Epoch [48/100], Step [70/211], Loss: 0.1521\n",
            "Epoch [48/100], Step [71/211], Loss: 0.2778\n",
            "Epoch [48/100], Step [72/211], Loss: 0.1080\n",
            "Epoch [48/100], Step [73/211], Loss: 0.3141\n",
            "Epoch [48/100], Step [74/211], Loss: 0.3381\n",
            "Epoch [48/100], Step [75/211], Loss: 0.2025\n",
            "Epoch [48/100], Step [76/211], Loss: 0.1458\n",
            "Epoch [48/100], Step [77/211], Loss: 0.3522\n",
            "Epoch [48/100], Step [78/211], Loss: 0.1151\n",
            "Epoch [48/100], Step [79/211], Loss: 0.1665\n",
            "Epoch [48/100], Step [80/211], Loss: 0.3518\n",
            "Epoch [48/100], Step [81/211], Loss: 0.3722\n",
            "Epoch [48/100], Step [82/211], Loss: 0.2235\n",
            "Epoch [48/100], Step [83/211], Loss: 0.2104\n",
            "Epoch [48/100], Step [84/211], Loss: 0.3451\n",
            "Epoch [48/100], Step [85/211], Loss: 0.2620\n",
            "Epoch [48/100], Step [86/211], Loss: 0.3568\n",
            "Epoch [48/100], Step [87/211], Loss: 0.1588\n",
            "Epoch [48/100], Step [88/211], Loss: 0.2540\n",
            "Epoch [48/100], Step [89/211], Loss: 0.3662\n",
            "Epoch [48/100], Step [90/211], Loss: 0.2228\n",
            "Epoch [48/100], Step [91/211], Loss: 0.2383\n",
            "Epoch [48/100], Step [92/211], Loss: 0.3983\n",
            "Epoch [48/100], Step [93/211], Loss: 0.1515\n",
            "Epoch [48/100], Step [94/211], Loss: 0.4210\n",
            "Epoch [48/100], Step [95/211], Loss: 0.1202\n",
            "Epoch [48/100], Step [96/211], Loss: 0.2558\n",
            "Epoch [48/100], Step [97/211], Loss: 0.2618\n",
            "Epoch [48/100], Step [98/211], Loss: 0.2566\n",
            "Epoch [48/100], Step [99/211], Loss: 0.3759\n",
            "Epoch [48/100], Step [100/211], Loss: 0.1819\n",
            "Epoch [48/100], Step [101/211], Loss: 0.2693\n",
            "Epoch [48/100], Step [102/211], Loss: 0.2733\n",
            "Epoch [48/100], Step [103/211], Loss: 0.1876\n",
            "Epoch [48/100], Step [104/211], Loss: 0.3112\n",
            "Epoch [48/100], Step [105/211], Loss: 0.1651\n",
            "Epoch [48/100], Step [106/211], Loss: 0.1232\n",
            "Epoch [48/100], Step [107/211], Loss: 0.4235\n",
            "Epoch [48/100], Step [108/211], Loss: 0.1392\n",
            "Epoch [48/100], Step [109/211], Loss: 0.2520\n",
            "Epoch [48/100], Step [110/211], Loss: 0.1770\n",
            "Epoch [48/100], Step [111/211], Loss: 0.2355\n",
            "Epoch [48/100], Step [112/211], Loss: 0.1551\n",
            "Epoch [48/100], Step [113/211], Loss: 0.0832\n",
            "Epoch [48/100], Step [114/211], Loss: 0.5301\n",
            "Epoch [48/100], Step [115/211], Loss: 0.1814\n",
            "Epoch [48/100], Step [116/211], Loss: 0.4108\n",
            "Epoch [48/100], Step [117/211], Loss: 0.1887\n",
            "Epoch [48/100], Step [118/211], Loss: 0.3039\n",
            "Epoch [48/100], Step [119/211], Loss: 0.1655\n",
            "Epoch [48/100], Step [120/211], Loss: 0.2841\n",
            "Epoch [48/100], Step [121/211], Loss: 0.3051\n",
            "Epoch [48/100], Step [122/211], Loss: 0.1979\n",
            "Epoch [48/100], Step [123/211], Loss: 0.2363\n",
            "Epoch [48/100], Step [124/211], Loss: 0.2784\n",
            "Epoch [48/100], Step [125/211], Loss: 0.2217\n",
            "Epoch [48/100], Step [126/211], Loss: 0.2100\n",
            "Epoch [48/100], Step [127/211], Loss: 0.2162\n",
            "Epoch [48/100], Step [128/211], Loss: 0.1941\n",
            "Epoch [48/100], Step [129/211], Loss: 0.2796\n",
            "Epoch [48/100], Step [130/211], Loss: 0.2447\n",
            "Epoch [48/100], Step [131/211], Loss: 0.1213\n",
            "Epoch [48/100], Step [132/211], Loss: 0.2038\n",
            "Epoch [48/100], Step [133/211], Loss: 0.2662\n",
            "Epoch [48/100], Step [134/211], Loss: 0.2355\n",
            "Epoch [48/100], Step [135/211], Loss: 0.2451\n",
            "Epoch [48/100], Step [136/211], Loss: 0.2076\n",
            "Epoch [48/100], Step [137/211], Loss: 0.0934\n",
            "Epoch [48/100], Step [138/211], Loss: 0.1685\n",
            "Epoch [48/100], Step [139/211], Loss: 0.2186\n",
            "Epoch [48/100], Step [140/211], Loss: 0.2559\n",
            "Epoch [48/100], Step [141/211], Loss: 0.2312\n",
            "Epoch [48/100], Step [142/211], Loss: 0.2720\n",
            "Epoch [48/100], Step [143/211], Loss: 0.1910\n",
            "Epoch [48/100], Step [144/211], Loss: 0.1972\n",
            "Epoch [48/100], Step [145/211], Loss: 0.2404\n",
            "Epoch [48/100], Step [146/211], Loss: 0.2195\n",
            "Epoch [48/100], Step [147/211], Loss: 0.2579\n",
            "Epoch [48/100], Step [148/211], Loss: 0.4929\n",
            "Epoch [48/100], Step [149/211], Loss: 0.2186\n",
            "Epoch [48/100], Step [150/211], Loss: 0.5413\n",
            "Epoch [48/100], Step [151/211], Loss: 0.3585\n",
            "Epoch [48/100], Step [152/211], Loss: 0.2505\n",
            "Epoch [48/100], Step [153/211], Loss: 0.1562\n",
            "Epoch [48/100], Step [154/211], Loss: 0.2231\n",
            "Epoch [48/100], Step [155/211], Loss: 0.2325\n",
            "Epoch [48/100], Step [156/211], Loss: 0.3846\n",
            "Epoch [48/100], Step [157/211], Loss: 0.4377\n",
            "Epoch [48/100], Step [158/211], Loss: 0.1796\n",
            "Epoch [48/100], Step [159/211], Loss: 0.5170\n",
            "Epoch [48/100], Step [160/211], Loss: 0.1312\n",
            "Epoch [48/100], Step [161/211], Loss: 0.2350\n",
            "Epoch [48/100], Step [162/211], Loss: 0.1368\n",
            "Epoch [48/100], Step [163/211], Loss: 0.1560\n",
            "Epoch [48/100], Step [164/211], Loss: 0.2607\n",
            "Epoch [48/100], Step [165/211], Loss: 0.2231\n",
            "Epoch [48/100], Step [166/211], Loss: 0.3078\n",
            "Epoch [48/100], Step [167/211], Loss: 0.1893\n",
            "Epoch [48/100], Step [168/211], Loss: 0.4032\n",
            "Epoch [48/100], Step [169/211], Loss: 0.2776\n",
            "Epoch [48/100], Step [170/211], Loss: 0.3408\n",
            "Epoch [48/100], Step [171/211], Loss: 0.1949\n",
            "Epoch [48/100], Step [172/211], Loss: 0.1982\n",
            "Epoch [48/100], Step [173/211], Loss: 0.2792\n",
            "Epoch [48/100], Step [174/211], Loss: 0.2417\n",
            "Epoch [48/100], Step [175/211], Loss: 0.1306\n",
            "Epoch [48/100], Step [176/211], Loss: 0.2640\n",
            "Epoch [48/100], Step [177/211], Loss: 0.2076\n",
            "Epoch [48/100], Step [178/211], Loss: 0.2630\n",
            "Epoch [48/100], Step [179/211], Loss: 0.1517\n",
            "Epoch [48/100], Step [180/211], Loss: 0.1642\n",
            "Epoch [48/100], Step [181/211], Loss: 0.3659\n",
            "Epoch [48/100], Step [182/211], Loss: 0.1985\n",
            "Epoch [48/100], Step [183/211], Loss: 0.1449\n",
            "Epoch [48/100], Step [184/211], Loss: 0.2689\n",
            "Epoch [48/100], Step [185/211], Loss: 0.3062\n",
            "Epoch [48/100], Step [186/211], Loss: 0.2750\n",
            "Epoch [48/100], Step [187/211], Loss: 0.1359\n",
            "Epoch [48/100], Step [188/211], Loss: 0.3447\n",
            "Epoch [48/100], Step [189/211], Loss: 0.1108\n",
            "Epoch [48/100], Step [190/211], Loss: 0.2378\n",
            "Epoch [48/100], Step [191/211], Loss: 0.1653\n",
            "Epoch [48/100], Step [192/211], Loss: 0.2163\n",
            "Epoch [48/100], Step [193/211], Loss: 0.1967\n",
            "Epoch [48/100], Step [194/211], Loss: 0.1553\n",
            "Epoch [48/100], Step [195/211], Loss: 0.1691\n",
            "Epoch [48/100], Step [196/211], Loss: 0.2788\n",
            "Epoch [48/100], Step [197/211], Loss: 0.2640\n",
            "Epoch [48/100], Step [198/211], Loss: 0.1309\n",
            "Epoch [48/100], Step [199/211], Loss: 0.4860\n",
            "Epoch [48/100], Step [200/211], Loss: 0.1874\n",
            "Epoch [48/100], Step [201/211], Loss: 0.3847\n",
            "Epoch [48/100], Step [202/211], Loss: 0.2576\n",
            "Epoch [48/100], Step [203/211], Loss: 0.0849\n",
            "Epoch [48/100], Step [204/211], Loss: 0.3149\n",
            "Epoch [48/100], Step [205/211], Loss: 0.1274\n",
            "Epoch [48/100], Step [206/211], Loss: 0.1042\n",
            "Epoch [48/100], Step [207/211], Loss: 0.1905\n",
            "Epoch [48/100], Step [208/211], Loss: 0.2164\n",
            "Epoch [48/100], Step [209/211], Loss: 0.1770\n",
            "Epoch [48/100], Step [210/211], Loss: 0.2928\n",
            "Epoch [48/100], Step [211/211], Loss: 0.1586\n",
            "Loss media: 0.2474\n",
            "Epoch [49/100], Step [1/211], Loss: 0.2284\n",
            "Epoch [49/100], Step [2/211], Loss: 0.1491\n",
            "Epoch [49/100], Step [3/211], Loss: 0.1516\n",
            "Epoch [49/100], Step [4/211], Loss: 0.0918\n",
            "Epoch [49/100], Step [5/211], Loss: 0.2098\n",
            "Epoch [49/100], Step [6/211], Loss: 0.3395\n",
            "Epoch [49/100], Step [7/211], Loss: 0.1347\n",
            "Epoch [49/100], Step [8/211], Loss: 0.2780\n",
            "Epoch [49/100], Step [9/211], Loss: 0.1840\n",
            "Epoch [49/100], Step [10/211], Loss: 0.2908\n",
            "Epoch [49/100], Step [11/211], Loss: 0.1579\n",
            "Epoch [49/100], Step [12/211], Loss: 0.1791\n",
            "Epoch [49/100], Step [13/211], Loss: 0.2597\n",
            "Epoch [49/100], Step [14/211], Loss: 0.3179\n",
            "Epoch [49/100], Step [15/211], Loss: 0.2382\n",
            "Epoch [49/100], Step [16/211], Loss: 0.2060\n",
            "Epoch [49/100], Step [17/211], Loss: 0.1395\n",
            "Epoch [49/100], Step [18/211], Loss: 0.3159\n",
            "Epoch [49/100], Step [19/211], Loss: 0.1346\n",
            "Epoch [49/100], Step [20/211], Loss: 0.1921\n",
            "Epoch [49/100], Step [21/211], Loss: 0.2536\n",
            "Epoch [49/100], Step [22/211], Loss: 0.2048\n",
            "Epoch [49/100], Step [23/211], Loss: 0.1878\n",
            "Epoch [49/100], Step [24/211], Loss: 0.4891\n",
            "Epoch [49/100], Step [25/211], Loss: 0.2167\n",
            "Epoch [49/100], Step [26/211], Loss: 0.2333\n",
            "Epoch [49/100], Step [27/211], Loss: 0.1233\n",
            "Epoch [49/100], Step [28/211], Loss: 0.2554\n",
            "Epoch [49/100], Step [29/211], Loss: 0.0978\n",
            "Epoch [49/100], Step [30/211], Loss: 0.3123\n",
            "Epoch [49/100], Step [31/211], Loss: 0.1414\n",
            "Epoch [49/100], Step [32/211], Loss: 0.2573\n",
            "Epoch [49/100], Step [33/211], Loss: 0.1424\n",
            "Epoch [49/100], Step [34/211], Loss: 0.2667\n",
            "Epoch [49/100], Step [35/211], Loss: 0.1315\n",
            "Epoch [49/100], Step [36/211], Loss: 0.4095\n",
            "Epoch [49/100], Step [37/211], Loss: 0.1439\n",
            "Epoch [49/100], Step [38/211], Loss: 0.2835\n",
            "Epoch [49/100], Step [39/211], Loss: 0.2019\n",
            "Epoch [49/100], Step [40/211], Loss: 0.1695\n",
            "Epoch [49/100], Step [41/211], Loss: 0.1052\n",
            "Epoch [49/100], Step [42/211], Loss: 0.1276\n",
            "Epoch [49/100], Step [43/211], Loss: 0.2552\n",
            "Epoch [49/100], Step [44/211], Loss: 0.1848\n",
            "Epoch [49/100], Step [45/211], Loss: 0.3918\n",
            "Epoch [49/100], Step [46/211], Loss: 0.2351\n",
            "Epoch [49/100], Step [47/211], Loss: 0.1203\n",
            "Epoch [49/100], Step [48/211], Loss: 0.2100\n",
            "Epoch [49/100], Step [49/211], Loss: 0.1512\n",
            "Epoch [49/100], Step [50/211], Loss: 0.2721\n",
            "Epoch [49/100], Step [51/211], Loss: 0.2944\n",
            "Epoch [49/100], Step [52/211], Loss: 0.4359\n",
            "Epoch [49/100], Step [53/211], Loss: 0.1176\n",
            "Epoch [49/100], Step [54/211], Loss: 0.2954\n",
            "Epoch [49/100], Step [55/211], Loss: 0.2336\n",
            "Epoch [49/100], Step [56/211], Loss: 0.2566\n",
            "Epoch [49/100], Step [57/211], Loss: 0.2546\n",
            "Epoch [49/100], Step [58/211], Loss: 0.2169\n",
            "Epoch [49/100], Step [59/211], Loss: 0.1557\n",
            "Epoch [49/100], Step [60/211], Loss: 0.1740\n",
            "Epoch [49/100], Step [61/211], Loss: 0.4612\n",
            "Epoch [49/100], Step [62/211], Loss: 0.3117\n",
            "Epoch [49/100], Step [63/211], Loss: 0.4080\n",
            "Epoch [49/100], Step [64/211], Loss: 0.4599\n",
            "Epoch [49/100], Step [65/211], Loss: 0.2863\n",
            "Epoch [49/100], Step [66/211], Loss: 0.2535\n",
            "Epoch [49/100], Step [67/211], Loss: 0.2267\n",
            "Epoch [49/100], Step [68/211], Loss: 0.1952\n",
            "Epoch [49/100], Step [69/211], Loss: 0.2632\n",
            "Epoch [49/100], Step [70/211], Loss: 0.3538\n",
            "Epoch [49/100], Step [71/211], Loss: 0.1982\n",
            "Epoch [49/100], Step [72/211], Loss: 0.1406\n",
            "Epoch [49/100], Step [73/211], Loss: 0.3681\n",
            "Epoch [49/100], Step [74/211], Loss: 0.3976\n",
            "Epoch [49/100], Step [75/211], Loss: 0.3610\n",
            "Epoch [49/100], Step [76/211], Loss: 0.2773\n",
            "Epoch [49/100], Step [77/211], Loss: 0.1599\n",
            "Epoch [49/100], Step [78/211], Loss: 0.3486\n",
            "Epoch [49/100], Step [79/211], Loss: 0.2876\n",
            "Epoch [49/100], Step [80/211], Loss: 0.1223\n",
            "Epoch [49/100], Step [81/211], Loss: 0.3062\n",
            "Epoch [49/100], Step [82/211], Loss: 0.2536\n",
            "Epoch [49/100], Step [83/211], Loss: 0.2630\n",
            "Epoch [49/100], Step [84/211], Loss: 0.2807\n",
            "Epoch [49/100], Step [85/211], Loss: 0.2725\n",
            "Epoch [49/100], Step [86/211], Loss: 0.3593\n",
            "Epoch [49/100], Step [87/211], Loss: 0.4238\n",
            "Epoch [49/100], Step [88/211], Loss: 0.2687\n",
            "Epoch [49/100], Step [89/211], Loss: 0.1605\n",
            "Epoch [49/100], Step [90/211], Loss: 0.2115\n",
            "Epoch [49/100], Step [91/211], Loss: 0.2044\n",
            "Epoch [49/100], Step [92/211], Loss: 0.2418\n",
            "Epoch [49/100], Step [93/211], Loss: 0.0960\n",
            "Epoch [49/100], Step [94/211], Loss: 0.2356\n",
            "Epoch [49/100], Step [95/211], Loss: 0.2789\n",
            "Epoch [49/100], Step [96/211], Loss: 0.2432\n",
            "Epoch [49/100], Step [97/211], Loss: 0.2275\n",
            "Epoch [49/100], Step [98/211], Loss: 0.2515\n",
            "Epoch [49/100], Step [99/211], Loss: 0.4605\n",
            "Epoch [49/100], Step [100/211], Loss: 0.1428\n",
            "Epoch [49/100], Step [101/211], Loss: 0.2308\n",
            "Epoch [49/100], Step [102/211], Loss: 0.0920\n",
            "Epoch [49/100], Step [103/211], Loss: 0.2217\n",
            "Epoch [49/100], Step [104/211], Loss: 0.1985\n",
            "Epoch [49/100], Step [105/211], Loss: 0.4751\n",
            "Epoch [49/100], Step [106/211], Loss: 0.2148\n",
            "Epoch [49/100], Step [107/211], Loss: 0.4174\n",
            "Epoch [49/100], Step [108/211], Loss: 0.1630\n",
            "Epoch [49/100], Step [109/211], Loss: 0.3196\n",
            "Epoch [49/100], Step [110/211], Loss: 0.3443\n",
            "Epoch [49/100], Step [111/211], Loss: 0.1757\n",
            "Epoch [49/100], Step [112/211], Loss: 0.1475\n",
            "Epoch [49/100], Step [113/211], Loss: 0.2542\n",
            "Epoch [49/100], Step [114/211], Loss: 0.1065\n",
            "Epoch [49/100], Step [115/211], Loss: 0.1867\n",
            "Epoch [49/100], Step [116/211], Loss: 0.2144\n",
            "Epoch [49/100], Step [117/211], Loss: 0.1634\n",
            "Epoch [49/100], Step [118/211], Loss: 0.1192\n",
            "Epoch [49/100], Step [119/211], Loss: 0.2345\n",
            "Epoch [49/100], Step [120/211], Loss: 0.1632\n",
            "Epoch [49/100], Step [121/211], Loss: 0.1794\n",
            "Epoch [49/100], Step [122/211], Loss: 0.3133\n",
            "Epoch [49/100], Step [123/211], Loss: 0.2309\n",
            "Epoch [49/100], Step [124/211], Loss: 0.1123\n",
            "Epoch [49/100], Step [125/211], Loss: 0.2429\n",
            "Epoch [49/100], Step [126/211], Loss: 0.5416\n",
            "Epoch [49/100], Step [127/211], Loss: 0.1466\n",
            "Epoch [49/100], Step [128/211], Loss: 0.4775\n",
            "Epoch [49/100], Step [129/211], Loss: 0.5374\n",
            "Epoch [49/100], Step [130/211], Loss: 0.3259\n",
            "Epoch [49/100], Step [131/211], Loss: 0.0752\n",
            "Epoch [49/100], Step [132/211], Loss: 0.3240\n",
            "Epoch [49/100], Step [133/211], Loss: 0.1157\n",
            "Epoch [49/100], Step [134/211], Loss: 0.1119\n",
            "Epoch [49/100], Step [135/211], Loss: 0.1466\n",
            "Epoch [49/100], Step [136/211], Loss: 0.1205\n",
            "Epoch [49/100], Step [137/211], Loss: 0.2891\n",
            "Epoch [49/100], Step [138/211], Loss: 0.1559\n",
            "Epoch [49/100], Step [139/211], Loss: 0.1886\n",
            "Epoch [49/100], Step [140/211], Loss: 0.3346\n",
            "Epoch [49/100], Step [141/211], Loss: 0.1410\n",
            "Epoch [49/100], Step [142/211], Loss: 0.1204\n",
            "Epoch [49/100], Step [143/211], Loss: 0.2438\n",
            "Epoch [49/100], Step [144/211], Loss: 0.1524\n",
            "Epoch [49/100], Step [145/211], Loss: 0.3042\n",
            "Epoch [49/100], Step [146/211], Loss: 0.2283\n",
            "Epoch [49/100], Step [147/211], Loss: 0.4721\n",
            "Epoch [49/100], Step [148/211], Loss: 0.2448\n",
            "Epoch [49/100], Step [149/211], Loss: 0.1765\n",
            "Epoch [49/100], Step [150/211], Loss: 0.2921\n",
            "Epoch [49/100], Step [151/211], Loss: 0.2782\n",
            "Epoch [49/100], Step [152/211], Loss: 0.1089\n",
            "Epoch [49/100], Step [153/211], Loss: 0.1034\n",
            "Epoch [49/100], Step [154/211], Loss: 0.3257\n",
            "Epoch [49/100], Step [155/211], Loss: 0.4615\n",
            "Epoch [49/100], Step [156/211], Loss: 0.7360\n",
            "Epoch [49/100], Step [157/211], Loss: 0.1989\n",
            "Epoch [49/100], Step [158/211], Loss: 0.3119\n",
            "Epoch [49/100], Step [159/211], Loss: 0.2423\n",
            "Epoch [49/100], Step [160/211], Loss: 0.1925\n",
            "Epoch [49/100], Step [161/211], Loss: 0.1341\n",
            "Epoch [49/100], Step [162/211], Loss: 0.2391\n",
            "Epoch [49/100], Step [163/211], Loss: 0.2107\n",
            "Epoch [49/100], Step [164/211], Loss: 0.1450\n",
            "Epoch [49/100], Step [165/211], Loss: 0.2374\n",
            "Epoch [49/100], Step [166/211], Loss: 0.1737\n",
            "Epoch [49/100], Step [167/211], Loss: 0.2056\n",
            "Epoch [49/100], Step [168/211], Loss: 0.1742\n",
            "Epoch [49/100], Step [169/211], Loss: 0.2916\n",
            "Epoch [49/100], Step [170/211], Loss: 0.1660\n",
            "Epoch [49/100], Step [171/211], Loss: 0.1178\n",
            "Epoch [49/100], Step [172/211], Loss: 0.4126\n",
            "Epoch [49/100], Step [173/211], Loss: 0.2695\n",
            "Epoch [49/100], Step [174/211], Loss: 0.1561\n",
            "Epoch [49/100], Step [175/211], Loss: 0.1614\n",
            "Epoch [49/100], Step [176/211], Loss: 0.2326\n",
            "Epoch [49/100], Step [177/211], Loss: 0.3786\n",
            "Epoch [49/100], Step [178/211], Loss: 0.3131\n",
            "Epoch [49/100], Step [179/211], Loss: 0.1012\n",
            "Epoch [49/100], Step [180/211], Loss: 0.4465\n",
            "Epoch [49/100], Step [181/211], Loss: 0.2883\n",
            "Epoch [49/100], Step [182/211], Loss: 0.1807\n",
            "Epoch [49/100], Step [183/211], Loss: 0.2153\n",
            "Epoch [49/100], Step [184/211], Loss: 0.2583\n",
            "Epoch [49/100], Step [185/211], Loss: 0.1925\n",
            "Epoch [49/100], Step [186/211], Loss: 0.3309\n",
            "Epoch [49/100], Step [187/211], Loss: 0.2075\n",
            "Epoch [49/100], Step [188/211], Loss: 0.2550\n",
            "Epoch [49/100], Step [189/211], Loss: 0.1986\n",
            "Epoch [49/100], Step [190/211], Loss: 0.2279\n",
            "Epoch [49/100], Step [191/211], Loss: 0.1637\n",
            "Epoch [49/100], Step [192/211], Loss: 0.3117\n",
            "Epoch [49/100], Step [193/211], Loss: 0.5206\n",
            "Epoch [49/100], Step [194/211], Loss: 0.3563\n",
            "Epoch [49/100], Step [195/211], Loss: 0.1454\n",
            "Epoch [49/100], Step [196/211], Loss: 0.3619\n",
            "Epoch [49/100], Step [197/211], Loss: 0.3520\n",
            "Epoch [49/100], Step [198/211], Loss: 0.2378\n",
            "Epoch [49/100], Step [199/211], Loss: 0.3229\n",
            "Epoch [49/100], Step [200/211], Loss: 0.2954\n",
            "Epoch [49/100], Step [201/211], Loss: 0.2652\n",
            "Epoch [49/100], Step [202/211], Loss: 0.2014\n",
            "Epoch [49/100], Step [203/211], Loss: 0.3685\n",
            "Epoch [49/100], Step [204/211], Loss: 0.2498\n",
            "Epoch [49/100], Step [205/211], Loss: 0.3038\n",
            "Epoch [49/100], Step [206/211], Loss: 0.3091\n",
            "Epoch [49/100], Step [207/211], Loss: 0.1087\n",
            "Epoch [49/100], Step [208/211], Loss: 0.2321\n",
            "Epoch [49/100], Step [209/211], Loss: 0.1569\n",
            "Epoch [49/100], Step [210/211], Loss: 0.3015\n",
            "Epoch [49/100], Step [211/211], Loss: 0.2080\n",
            "Loss media: 0.2451\n",
            "Epoch [50/100], Step [1/211], Loss: 0.2658\n",
            "Epoch [50/100], Step [2/211], Loss: 0.2213\n",
            "Epoch [50/100], Step [3/211], Loss: 0.2623\n",
            "Epoch [50/100], Step [4/211], Loss: 0.1452\n",
            "Epoch [50/100], Step [5/211], Loss: 0.0905\n",
            "Epoch [50/100], Step [6/211], Loss: 0.3574\n",
            "Epoch [50/100], Step [7/211], Loss: 0.2071\n",
            "Epoch [50/100], Step [8/211], Loss: 0.4018\n",
            "Epoch [50/100], Step [9/211], Loss: 0.2258\n",
            "Epoch [50/100], Step [10/211], Loss: 0.3757\n",
            "Epoch [50/100], Step [11/211], Loss: 0.1095\n",
            "Epoch [50/100], Step [12/211], Loss: 0.4437\n",
            "Epoch [50/100], Step [13/211], Loss: 0.2319\n",
            "Epoch [50/100], Step [14/211], Loss: 0.1330\n",
            "Epoch [50/100], Step [15/211], Loss: 0.1998\n",
            "Epoch [50/100], Step [16/211], Loss: 0.2311\n",
            "Epoch [50/100], Step [17/211], Loss: 0.2819\n",
            "Epoch [50/100], Step [18/211], Loss: 0.2091\n",
            "Epoch [50/100], Step [19/211], Loss: 0.1784\n",
            "Epoch [50/100], Step [20/211], Loss: 0.1057\n",
            "Epoch [50/100], Step [21/211], Loss: 0.4455\n",
            "Epoch [50/100], Step [22/211], Loss: 0.2233\n",
            "Epoch [50/100], Step [23/211], Loss: 0.0984\n",
            "Epoch [50/100], Step [24/211], Loss: 0.1829\n",
            "Epoch [50/100], Step [25/211], Loss: 0.2280\n",
            "Epoch [50/100], Step [26/211], Loss: 0.3298\n",
            "Epoch [50/100], Step [27/211], Loss: 0.2683\n",
            "Epoch [50/100], Step [28/211], Loss: 0.1913\n",
            "Epoch [50/100], Step [29/211], Loss: 0.1820\n",
            "Epoch [50/100], Step [30/211], Loss: 0.3152\n",
            "Epoch [50/100], Step [31/211], Loss: 0.1504\n",
            "Epoch [50/100], Step [32/211], Loss: 0.3782\n",
            "Epoch [50/100], Step [33/211], Loss: 0.1369\n",
            "Epoch [50/100], Step [34/211], Loss: 0.2733\n",
            "Epoch [50/100], Step [35/211], Loss: 0.0890\n",
            "Epoch [50/100], Step [36/211], Loss: 0.1780\n",
            "Epoch [50/100], Step [37/211], Loss: 0.2336\n",
            "Epoch [50/100], Step [38/211], Loss: 0.1236\n",
            "Epoch [50/100], Step [39/211], Loss: 0.2962\n",
            "Epoch [50/100], Step [40/211], Loss: 0.2520\n",
            "Epoch [50/100], Step [41/211], Loss: 0.2973\n",
            "Epoch [50/100], Step [42/211], Loss: 0.2788\n",
            "Epoch [50/100], Step [43/211], Loss: 0.1255\n",
            "Epoch [50/100], Step [44/211], Loss: 0.2334\n",
            "Epoch [50/100], Step [45/211], Loss: 0.2124\n",
            "Epoch [50/100], Step [46/211], Loss: 0.0790\n",
            "Epoch [50/100], Step [47/211], Loss: 0.3426\n",
            "Epoch [50/100], Step [48/211], Loss: 0.3353\n",
            "Epoch [50/100], Step [49/211], Loss: 0.1552\n",
            "Epoch [50/100], Step [50/211], Loss: 0.3685\n",
            "Epoch [50/100], Step [51/211], Loss: 0.3929\n",
            "Epoch [50/100], Step [52/211], Loss: 0.3437\n",
            "Epoch [50/100], Step [53/211], Loss: 0.1505\n",
            "Epoch [50/100], Step [54/211], Loss: 0.2187\n",
            "Epoch [50/100], Step [55/211], Loss: 0.2373\n",
            "Epoch [50/100], Step [56/211], Loss: 0.1609\n",
            "Epoch [50/100], Step [57/211], Loss: 0.2147\n",
            "Epoch [50/100], Step [58/211], Loss: 0.2744\n",
            "Epoch [50/100], Step [59/211], Loss: 0.1359\n",
            "Epoch [50/100], Step [60/211], Loss: 0.3349\n",
            "Epoch [50/100], Step [61/211], Loss: 0.2042\n",
            "Epoch [50/100], Step [62/211], Loss: 0.2555\n",
            "Epoch [50/100], Step [63/211], Loss: 0.4107\n",
            "Epoch [50/100], Step [64/211], Loss: 0.0959\n",
            "Epoch [50/100], Step [65/211], Loss: 0.0980\n",
            "Epoch [50/100], Step [66/211], Loss: 0.1942\n",
            "Epoch [50/100], Step [67/211], Loss: 0.2789\n",
            "Epoch [50/100], Step [68/211], Loss: 0.3423\n",
            "Epoch [50/100], Step [69/211], Loss: 0.4000\n",
            "Epoch [50/100], Step [70/211], Loss: 0.2352\n",
            "Epoch [50/100], Step [71/211], Loss: 0.1573\n",
            "Epoch [50/100], Step [72/211], Loss: 0.1640\n",
            "Epoch [50/100], Step [73/211], Loss: 0.2954\n",
            "Epoch [50/100], Step [74/211], Loss: 0.2415\n",
            "Epoch [50/100], Step [75/211], Loss: 0.1832\n",
            "Epoch [50/100], Step [76/211], Loss: 0.3937\n",
            "Epoch [50/100], Step [77/211], Loss: 0.1893\n",
            "Epoch [50/100], Step [78/211], Loss: 0.1415\n",
            "Epoch [50/100], Step [79/211], Loss: 0.1448\n",
            "Epoch [50/100], Step [80/211], Loss: 0.1506\n",
            "Epoch [50/100], Step [81/211], Loss: 0.1368\n",
            "Epoch [50/100], Step [82/211], Loss: 0.2164\n",
            "Epoch [50/100], Step [83/211], Loss: 0.3760\n",
            "Epoch [50/100], Step [84/211], Loss: 0.2396\n",
            "Epoch [50/100], Step [85/211], Loss: 0.1913\n",
            "Epoch [50/100], Step [86/211], Loss: 0.4150\n",
            "Epoch [50/100], Step [87/211], Loss: 0.3689\n",
            "Epoch [50/100], Step [88/211], Loss: 0.2107\n",
            "Epoch [50/100], Step [89/211], Loss: 0.0975\n",
            "Epoch [50/100], Step [90/211], Loss: 0.1846\n",
            "Epoch [50/100], Step [91/211], Loss: 0.2411\n",
            "Epoch [50/100], Step [92/211], Loss: 0.1244\n",
            "Epoch [50/100], Step [93/211], Loss: 0.3120\n",
            "Epoch [50/100], Step [94/211], Loss: 0.1307\n",
            "Epoch [50/100], Step [95/211], Loss: 0.1733\n",
            "Epoch [50/100], Step [96/211], Loss: 0.3205\n",
            "Epoch [50/100], Step [97/211], Loss: 0.1168\n",
            "Epoch [50/100], Step [98/211], Loss: 0.2595\n",
            "Epoch [50/100], Step [99/211], Loss: 0.0774\n",
            "Epoch [50/100], Step [100/211], Loss: 0.0739\n",
            "Epoch [50/100], Step [101/211], Loss: 0.1813\n",
            "Epoch [50/100], Step [102/211], Loss: 0.1459\n",
            "Epoch [50/100], Step [103/211], Loss: 0.2176\n",
            "Epoch [50/100], Step [104/211], Loss: 0.1153\n",
            "Epoch [50/100], Step [105/211], Loss: 0.4017\n",
            "Epoch [50/100], Step [106/211], Loss: 0.1777\n",
            "Epoch [50/100], Step [107/211], Loss: 0.1969\n",
            "Epoch [50/100], Step [108/211], Loss: 0.2004\n",
            "Epoch [50/100], Step [109/211], Loss: 0.2388\n",
            "Epoch [50/100], Step [110/211], Loss: 0.2220\n",
            "Epoch [50/100], Step [111/211], Loss: 0.2553\n",
            "Epoch [50/100], Step [112/211], Loss: 0.1150\n",
            "Epoch [50/100], Step [113/211], Loss: 0.2707\n",
            "Epoch [50/100], Step [114/211], Loss: 0.2172\n",
            "Epoch [50/100], Step [115/211], Loss: 0.2107\n",
            "Epoch [50/100], Step [116/211], Loss: 0.3286\n",
            "Epoch [50/100], Step [117/211], Loss: 0.1488\n",
            "Epoch [50/100], Step [118/211], Loss: 0.4733\n",
            "Epoch [50/100], Step [119/211], Loss: 0.1254\n",
            "Epoch [50/100], Step [120/211], Loss: 0.3091\n",
            "Epoch [50/100], Step [121/211], Loss: 0.2470\n",
            "Epoch [50/100], Step [122/211], Loss: 0.2488\n",
            "Epoch [50/100], Step [123/211], Loss: 0.4489\n",
            "Epoch [50/100], Step [124/211], Loss: 0.1855\n",
            "Epoch [50/100], Step [125/211], Loss: 0.3213\n",
            "Epoch [50/100], Step [126/211], Loss: 0.3292\n",
            "Epoch [50/100], Step [127/211], Loss: 0.2023\n",
            "Epoch [50/100], Step [128/211], Loss: 0.1606\n",
            "Epoch [50/100], Step [129/211], Loss: 0.1273\n",
            "Epoch [50/100], Step [130/211], Loss: 0.4559\n",
            "Epoch [50/100], Step [131/211], Loss: 0.3398\n",
            "Epoch [50/100], Step [132/211], Loss: 0.1087\n",
            "Epoch [50/100], Step [133/211], Loss: 0.3828\n",
            "Epoch [50/100], Step [134/211], Loss: 0.1887\n",
            "Epoch [50/100], Step [135/211], Loss: 0.4061\n",
            "Epoch [50/100], Step [136/211], Loss: 0.2811\n",
            "Epoch [50/100], Step [137/211], Loss: 0.1169\n",
            "Epoch [50/100], Step [138/211], Loss: 0.1074\n",
            "Epoch [50/100], Step [139/211], Loss: 0.2155\n",
            "Epoch [50/100], Step [140/211], Loss: 0.4410\n",
            "Epoch [50/100], Step [141/211], Loss: 0.4483\n",
            "Epoch [50/100], Step [142/211], Loss: 0.3188\n",
            "Epoch [50/100], Step [143/211], Loss: 0.2996\n",
            "Epoch [50/100], Step [144/211], Loss: 0.1246\n",
            "Epoch [50/100], Step [145/211], Loss: 0.1506\n",
            "Epoch [50/100], Step [146/211], Loss: 0.2049\n",
            "Epoch [50/100], Step [147/211], Loss: 0.1919\n",
            "Epoch [50/100], Step [148/211], Loss: 0.1834\n",
            "Epoch [50/100], Step [149/211], Loss: 0.2836\n",
            "Epoch [50/100], Step [150/211], Loss: 0.2278\n",
            "Epoch [50/100], Step [151/211], Loss: 0.1289\n",
            "Epoch [50/100], Step [152/211], Loss: 0.1730\n",
            "Epoch [50/100], Step [153/211], Loss: 0.4530\n",
            "Epoch [50/100], Step [154/211], Loss: 0.1560\n",
            "Epoch [50/100], Step [155/211], Loss: 0.0872\n",
            "Epoch [50/100], Step [156/211], Loss: 0.4631\n",
            "Epoch [50/100], Step [157/211], Loss: 0.2175\n",
            "Epoch [50/100], Step [158/211], Loss: 0.0765\n",
            "Epoch [50/100], Step [159/211], Loss: 0.2065\n",
            "Epoch [50/100], Step [160/211], Loss: 0.3975\n",
            "Epoch [50/100], Step [161/211], Loss: 0.4314\n",
            "Epoch [50/100], Step [162/211], Loss: 0.2042\n",
            "Epoch [50/100], Step [163/211], Loss: 0.1190\n",
            "Epoch [50/100], Step [164/211], Loss: 0.4011\n",
            "Epoch [50/100], Step [165/211], Loss: 0.1205\n",
            "Epoch [50/100], Step [166/211], Loss: 0.4274\n",
            "Epoch [50/100], Step [167/211], Loss: 0.2516\n",
            "Epoch [50/100], Step [168/211], Loss: 0.2130\n",
            "Epoch [50/100], Step [169/211], Loss: 0.1373\n",
            "Epoch [50/100], Step [170/211], Loss: 0.1755\n",
            "Epoch [50/100], Step [171/211], Loss: 0.3638\n",
            "Epoch [50/100], Step [172/211], Loss: 0.1370\n",
            "Epoch [50/100], Step [173/211], Loss: 0.2666\n",
            "Epoch [50/100], Step [174/211], Loss: 0.3708\n",
            "Epoch [50/100], Step [175/211], Loss: 0.2621\n",
            "Epoch [50/100], Step [176/211], Loss: 0.3552\n",
            "Epoch [50/100], Step [177/211], Loss: 0.2015\n",
            "Epoch [50/100], Step [178/211], Loss: 0.2496\n",
            "Epoch [50/100], Step [179/211], Loss: 0.1575\n",
            "Epoch [50/100], Step [180/211], Loss: 0.1635\n",
            "Epoch [50/100], Step [181/211], Loss: 0.3149\n",
            "Epoch [50/100], Step [182/211], Loss: 0.5268\n",
            "Epoch [50/100], Step [183/211], Loss: 0.2192\n",
            "Epoch [50/100], Step [184/211], Loss: 0.1679\n",
            "Epoch [50/100], Step [185/211], Loss: 0.2052\n",
            "Epoch [50/100], Step [186/211], Loss: 0.1390\n",
            "Epoch [50/100], Step [187/211], Loss: 0.1459\n",
            "Epoch [50/100], Step [188/211], Loss: 0.2067\n",
            "Epoch [50/100], Step [189/211], Loss: 0.2317\n",
            "Epoch [50/100], Step [190/211], Loss: 0.1604\n",
            "Epoch [50/100], Step [191/211], Loss: 0.1955\n",
            "Epoch [50/100], Step [192/211], Loss: 0.2697\n",
            "Epoch [50/100], Step [193/211], Loss: 0.1136\n",
            "Epoch [50/100], Step [194/211], Loss: 0.1018\n",
            "Epoch [50/100], Step [195/211], Loss: 0.1596\n",
            "Epoch [50/100], Step [196/211], Loss: 0.3497\n",
            "Epoch [50/100], Step [197/211], Loss: 0.2054\n",
            "Epoch [50/100], Step [198/211], Loss: 0.2065\n",
            "Epoch [50/100], Step [199/211], Loss: 0.2530\n",
            "Epoch [50/100], Step [200/211], Loss: 0.2997\n",
            "Epoch [50/100], Step [201/211], Loss: 0.2163\n",
            "Epoch [50/100], Step [202/211], Loss: 0.3440\n",
            "Epoch [50/100], Step [203/211], Loss: 0.2199\n",
            "Epoch [50/100], Step [204/211], Loss: 0.2104\n",
            "Epoch [50/100], Step [205/211], Loss: 0.1609\n",
            "Epoch [50/100], Step [206/211], Loss: 0.2930\n",
            "Epoch [50/100], Step [207/211], Loss: 0.3697\n",
            "Epoch [50/100], Step [208/211], Loss: 0.1842\n",
            "Epoch [50/100], Step [209/211], Loss: 0.2166\n",
            "Epoch [50/100], Step [210/211], Loss: 0.3151\n",
            "Epoch [50/100], Step [211/211], Loss: 0.2210\n",
            "Loss media: 0.2364\n",
            "Epoch [51/100], Step [1/211], Loss: 0.2682\n",
            "Epoch [51/100], Step [2/211], Loss: 0.3319\n",
            "Epoch [51/100], Step [3/211], Loss: 0.2496\n",
            "Epoch [51/100], Step [4/211], Loss: 0.1980\n",
            "Epoch [51/100], Step [5/211], Loss: 0.2041\n",
            "Epoch [51/100], Step [6/211], Loss: 0.2244\n",
            "Epoch [51/100], Step [7/211], Loss: 0.3121\n",
            "Epoch [51/100], Step [8/211], Loss: 0.3561\n",
            "Epoch [51/100], Step [9/211], Loss: 0.2558\n",
            "Epoch [51/100], Step [10/211], Loss: 0.1377\n",
            "Epoch [51/100], Step [11/211], Loss: 0.3328\n",
            "Epoch [51/100], Step [12/211], Loss: 0.6214\n",
            "Epoch [51/100], Step [13/211], Loss: 0.3926\n",
            "Epoch [51/100], Step [14/211], Loss: 0.3583\n",
            "Epoch [51/100], Step [15/211], Loss: 0.2108\n",
            "Epoch [51/100], Step [16/211], Loss: 0.2140\n",
            "Epoch [51/100], Step [17/211], Loss: 0.2904\n",
            "Epoch [51/100], Step [18/211], Loss: 0.1692\n",
            "Epoch [51/100], Step [19/211], Loss: 0.2396\n",
            "Epoch [51/100], Step [20/211], Loss: 0.2631\n",
            "Epoch [51/100], Step [21/211], Loss: 0.2616\n",
            "Epoch [51/100], Step [22/211], Loss: 0.1620\n",
            "Epoch [51/100], Step [23/211], Loss: 0.2716\n",
            "Epoch [51/100], Step [24/211], Loss: 0.2705\n",
            "Epoch [51/100], Step [25/211], Loss: 0.2003\n",
            "Epoch [51/100], Step [26/211], Loss: 0.2666\n",
            "Epoch [51/100], Step [27/211], Loss: 0.4171\n",
            "Epoch [51/100], Step [28/211], Loss: 0.2025\n",
            "Epoch [51/100], Step [29/211], Loss: 0.1521\n",
            "Epoch [51/100], Step [30/211], Loss: 0.2118\n",
            "Epoch [51/100], Step [31/211], Loss: 0.1547\n",
            "Epoch [51/100], Step [32/211], Loss: 0.1618\n",
            "Epoch [51/100], Step [33/211], Loss: 0.3464\n",
            "Epoch [51/100], Step [34/211], Loss: 0.1707\n",
            "Epoch [51/100], Step [35/211], Loss: 0.4721\n",
            "Epoch [51/100], Step [36/211], Loss: 0.1089\n",
            "Epoch [51/100], Step [37/211], Loss: 0.1823\n",
            "Epoch [51/100], Step [38/211], Loss: 0.1840\n",
            "Epoch [51/100], Step [39/211], Loss: 0.1443\n",
            "Epoch [51/100], Step [40/211], Loss: 0.2333\n",
            "Epoch [51/100], Step [41/211], Loss: 0.1310\n",
            "Epoch [51/100], Step [42/211], Loss: 0.2788\n",
            "Epoch [51/100], Step [43/211], Loss: 0.1249\n",
            "Epoch [51/100], Step [44/211], Loss: 0.1900\n",
            "Epoch [51/100], Step [45/211], Loss: 0.1062\n",
            "Epoch [51/100], Step [46/211], Loss: 0.1138\n",
            "Epoch [51/100], Step [47/211], Loss: 0.2403\n",
            "Epoch [51/100], Step [48/211], Loss: 0.2099\n",
            "Epoch [51/100], Step [49/211], Loss: 0.1632\n",
            "Epoch [51/100], Step [50/211], Loss: 0.1581\n",
            "Epoch [51/100], Step [51/211], Loss: 0.1832\n",
            "Epoch [51/100], Step [52/211], Loss: 0.1658\n",
            "Epoch [51/100], Step [53/211], Loss: 0.3266\n",
            "Epoch [51/100], Step [54/211], Loss: 0.3294\n",
            "Epoch [51/100], Step [55/211], Loss: 0.1283\n",
            "Epoch [51/100], Step [56/211], Loss: 0.1417\n",
            "Epoch [51/100], Step [57/211], Loss: 0.2194\n",
            "Epoch [51/100], Step [58/211], Loss: 0.3477\n",
            "Epoch [51/100], Step [59/211], Loss: 0.1920\n",
            "Epoch [51/100], Step [60/211], Loss: 0.2659\n",
            "Epoch [51/100], Step [61/211], Loss: 0.2469\n",
            "Epoch [51/100], Step [62/211], Loss: 0.1250\n",
            "Epoch [51/100], Step [63/211], Loss: 0.2006\n",
            "Epoch [51/100], Step [64/211], Loss: 0.1964\n",
            "Epoch [51/100], Step [65/211], Loss: 0.1211\n",
            "Epoch [51/100], Step [66/211], Loss: 0.0969\n",
            "Epoch [51/100], Step [67/211], Loss: 0.1635\n",
            "Epoch [51/100], Step [68/211], Loss: 0.2594\n",
            "Epoch [51/100], Step [69/211], Loss: 0.1381\n",
            "Epoch [51/100], Step [70/211], Loss: 0.1756\n",
            "Epoch [51/100], Step [71/211], Loss: 0.2175\n",
            "Epoch [51/100], Step [72/211], Loss: 0.2489\n",
            "Epoch [51/100], Step [73/211], Loss: 0.2880\n",
            "Epoch [51/100], Step [74/211], Loss: 0.1714\n",
            "Epoch [51/100], Step [75/211], Loss: 0.2054\n",
            "Epoch [51/100], Step [76/211], Loss: 0.2269\n",
            "Epoch [51/100], Step [77/211], Loss: 0.2457\n",
            "Epoch [51/100], Step [78/211], Loss: 0.2225\n",
            "Epoch [51/100], Step [79/211], Loss: 0.1861\n",
            "Epoch [51/100], Step [80/211], Loss: 0.2703\n",
            "Epoch [51/100], Step [81/211], Loss: 0.1125\n",
            "Epoch [51/100], Step [82/211], Loss: 0.1996\n",
            "Epoch [51/100], Step [83/211], Loss: 0.1770\n",
            "Epoch [51/100], Step [84/211], Loss: 0.1499\n",
            "Epoch [51/100], Step [85/211], Loss: 0.1809\n",
            "Epoch [51/100], Step [86/211], Loss: 0.2074\n",
            "Epoch [51/100], Step [87/211], Loss: 0.1078\n",
            "Epoch [51/100], Step [88/211], Loss: 0.4382\n",
            "Epoch [51/100], Step [89/211], Loss: 0.1229\n",
            "Epoch [51/100], Step [90/211], Loss: 0.5115\n",
            "Epoch [51/100], Step [91/211], Loss: 0.2518\n",
            "Epoch [51/100], Step [92/211], Loss: 0.5665\n",
            "Epoch [51/100], Step [93/211], Loss: 0.1122\n",
            "Epoch [51/100], Step [94/211], Loss: 0.1881\n",
            "Epoch [51/100], Step [95/211], Loss: 0.1712\n",
            "Epoch [51/100], Step [96/211], Loss: 0.3090\n",
            "Epoch [51/100], Step [97/211], Loss: 0.2972\n",
            "Epoch [51/100], Step [98/211], Loss: 0.1121\n",
            "Epoch [51/100], Step [99/211], Loss: 0.3428\n",
            "Epoch [51/100], Step [100/211], Loss: 0.2387\n",
            "Epoch [51/100], Step [101/211], Loss: 0.3743\n",
            "Epoch [51/100], Step [102/211], Loss: 0.1388\n",
            "Epoch [51/100], Step [103/211], Loss: 0.3337\n",
            "Epoch [51/100], Step [104/211], Loss: 0.2540\n",
            "Epoch [51/100], Step [105/211], Loss: 0.3312\n",
            "Epoch [51/100], Step [106/211], Loss: 0.2210\n",
            "Epoch [51/100], Step [107/211], Loss: 0.1328\n",
            "Epoch [51/100], Step [108/211], Loss: 0.2958\n",
            "Epoch [51/100], Step [109/211], Loss: 0.1775\n",
            "Epoch [51/100], Step [110/211], Loss: 0.2386\n",
            "Epoch [51/100], Step [111/211], Loss: 0.2860\n",
            "Epoch [51/100], Step [112/211], Loss: 0.2475\n",
            "Epoch [51/100], Step [113/211], Loss: 0.1233\n",
            "Epoch [51/100], Step [114/211], Loss: 0.2833\n",
            "Epoch [51/100], Step [115/211], Loss: 0.3467\n",
            "Epoch [51/100], Step [116/211], Loss: 0.3631\n",
            "Epoch [51/100], Step [117/211], Loss: 0.3481\n",
            "Epoch [51/100], Step [118/211], Loss: 0.4312\n",
            "Epoch [51/100], Step [119/211], Loss: 0.2141\n",
            "Epoch [51/100], Step [120/211], Loss: 0.2730\n",
            "Epoch [51/100], Step [121/211], Loss: 0.3309\n",
            "Epoch [51/100], Step [122/211], Loss: 0.2444\n",
            "Epoch [51/100], Step [123/211], Loss: 0.2098\n",
            "Epoch [51/100], Step [124/211], Loss: 0.2212\n",
            "Epoch [51/100], Step [125/211], Loss: 0.4167\n",
            "Epoch [51/100], Step [126/211], Loss: 0.1633\n",
            "Epoch [51/100], Step [127/211], Loss: 0.2168\n",
            "Epoch [51/100], Step [128/211], Loss: 0.1224\n",
            "Epoch [51/100], Step [129/211], Loss: 0.1482\n",
            "Epoch [51/100], Step [130/211], Loss: 0.2252\n",
            "Epoch [51/100], Step [131/211], Loss: 0.1767\n",
            "Epoch [51/100], Step [132/211], Loss: 0.2686\n",
            "Epoch [51/100], Step [133/211], Loss: 0.3101\n",
            "Epoch [51/100], Step [134/211], Loss: 0.1861\n",
            "Epoch [51/100], Step [135/211], Loss: 0.2115\n",
            "Epoch [51/100], Step [136/211], Loss: 0.1810\n",
            "Epoch [51/100], Step [137/211], Loss: 0.1940\n",
            "Epoch [51/100], Step [138/211], Loss: 0.1871\n",
            "Epoch [51/100], Step [139/211], Loss: 0.1576\n",
            "Epoch [51/100], Step [140/211], Loss: 0.5335\n",
            "Epoch [51/100], Step [141/211], Loss: 0.0892\n",
            "Epoch [51/100], Step [142/211], Loss: 0.3832\n",
            "Epoch [51/100], Step [143/211], Loss: 0.3425\n",
            "Epoch [51/100], Step [144/211], Loss: 0.3009\n",
            "Epoch [51/100], Step [145/211], Loss: 0.2283\n",
            "Epoch [51/100], Step [146/211], Loss: 0.0795\n",
            "Epoch [51/100], Step [147/211], Loss: 0.1734\n",
            "Epoch [51/100], Step [148/211], Loss: 0.3936\n",
            "Epoch [51/100], Step [149/211], Loss: 0.2314\n",
            "Epoch [51/100], Step [150/211], Loss: 0.1947\n",
            "Epoch [51/100], Step [151/211], Loss: 0.1853\n",
            "Epoch [51/100], Step [152/211], Loss: 0.3055\n",
            "Epoch [51/100], Step [153/211], Loss: 0.3708\n",
            "Epoch [51/100], Step [154/211], Loss: 0.3510\n",
            "Epoch [51/100], Step [155/211], Loss: 0.2979\n",
            "Epoch [51/100], Step [156/211], Loss: 0.1492\n",
            "Epoch [51/100], Step [157/211], Loss: 0.2165\n",
            "Epoch [51/100], Step [158/211], Loss: 0.1717\n",
            "Epoch [51/100], Step [159/211], Loss: 0.1957\n",
            "Epoch [51/100], Step [160/211], Loss: 0.2646\n",
            "Epoch [51/100], Step [161/211], Loss: 0.1454\n",
            "Epoch [51/100], Step [162/211], Loss: 0.3284\n",
            "Epoch [51/100], Step [163/211], Loss: 0.2212\n",
            "Epoch [51/100], Step [164/211], Loss: 0.2359\n",
            "Epoch [51/100], Step [165/211], Loss: 0.1026\n",
            "Epoch [51/100], Step [166/211], Loss: 0.2701\n",
            "Epoch [51/100], Step [167/211], Loss: 0.2917\n",
            "Epoch [51/100], Step [168/211], Loss: 0.4430\n",
            "Epoch [51/100], Step [169/211], Loss: 0.1907\n",
            "Epoch [51/100], Step [170/211], Loss: 0.2186\n",
            "Epoch [51/100], Step [171/211], Loss: 0.1979\n",
            "Epoch [51/100], Step [172/211], Loss: 0.1584\n",
            "Epoch [51/100], Step [173/211], Loss: 0.1876\n",
            "Epoch [51/100], Step [174/211], Loss: 0.4181\n",
            "Epoch [51/100], Step [175/211], Loss: 0.2244\n",
            "Epoch [51/100], Step [176/211], Loss: 0.1954\n",
            "Epoch [51/100], Step [177/211], Loss: 0.0886\n",
            "Epoch [51/100], Step [178/211], Loss: 0.1428\n",
            "Epoch [51/100], Step [179/211], Loss: 0.1779\n",
            "Epoch [51/100], Step [180/211], Loss: 0.3965\n",
            "Epoch [51/100], Step [181/211], Loss: 0.3241\n",
            "Epoch [51/100], Step [182/211], Loss: 0.1439\n",
            "Epoch [51/100], Step [183/211], Loss: 0.2208\n",
            "Epoch [51/100], Step [184/211], Loss: 0.3358\n",
            "Epoch [51/100], Step [185/211], Loss: 0.2908\n",
            "Epoch [51/100], Step [186/211], Loss: 0.2215\n",
            "Epoch [51/100], Step [187/211], Loss: 0.4214\n",
            "Epoch [51/100], Step [188/211], Loss: 0.3371\n",
            "Epoch [51/100], Step [189/211], Loss: 0.1741\n",
            "Epoch [51/100], Step [190/211], Loss: 0.2341\n",
            "Epoch [51/100], Step [191/211], Loss: 0.2263\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(0,num_epochs+1):\n",
        "  train(epoch = epoch, model = model, optimizer = optimizer)\n",
        "  if (epoch%2==0):\n",
        "    torch.save(model, '/content/drive/MyDrive/mask_rcnn_res152v2_{}ep'.format(epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhS1sKGVc-w7"
      },
      "source": [
        "# Fase di testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proseguiamo con il notebook di testing per il modello addestrato. Il file prende ispirazione dal file di esempio caricato nella cartella del progetto dai docenti con qualche modifica per essere reso effetivamente fruibile per il testing"
      ],
      "metadata": {
        "id": "BKUcIHLgo67m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMlQQKW0dAuL",
        "outputId": "c4b88a34-f607-4236-87be-494360ef40a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n"
          ]
        }
      ],
      "source": [
        "test_set = TTPLADataset('/content/drive/MyDrive/testset', '/content/drive/MyDrive/test.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa fase viene predisposto il dataloader per il test set. Vengono effettuate le predizioni e vengono salvate in un file json sul drive. Questo file json sarà utile più avanti quando dovremo calcolare le metriche di nostro interesse come la mAP(50%) su box e segmentazione."
      ],
      "metadata": {
        "id": "FdDxlu54o_f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi3rjl5XY3Cd"
      },
      "outputs": [],
      "source": [
        "THRESHOLD_SEGM = 0.5\n",
        "model.eval()\n",
        "batch_size = 1\n",
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = False, collate_fn =lambda batch: tuple(zip(*batch)) )\n",
        "\n",
        "results = []\n",
        "for images, labels in test_dataloader:\n",
        "\n",
        "  images = [image.to(device) for image in images]\n",
        "  prediction = model(images)[0]\n",
        "  classes = prediction['labels'].cpu().numpy()\n",
        "  scores = prediction['scores'].cpu().detach().numpy()\n",
        "  boxes = prediction['boxes'].view(-1,4).cpu().detach().numpy()\n",
        "  masks = torch.where(prediction['masks'].view(-1,700,700).cpu().detach()>THRESHOLD_SEGM,1,0).numpy()\n",
        "  for i in range(masks.shape[0]): #per ogni oggetto identificato\n",
        "    image_id = labels[0]['image_id']\n",
        "    bbox = boxes[i]\n",
        "    bbox = [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]]\n",
        "    bbox = [round(float(x)*10)/10 for x in bbox]\n",
        "    rle = pycocotools.mask.encode(np.asfortranarray(masks[i].astype(np.uint8)))\n",
        "    rle['counts'] = rle['counts'].decode('ascii')\n",
        "    results.append({\n",
        "                'image_id': int(image_id),\n",
        "                'category_id': int(classes[i])-1,\n",
        "                'bbox': bbox,\n",
        "                'segmentation': rle,\n",
        "                'score': float(scores[i]),\n",
        "            })\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "with open('/content/drive/MyDrive/pred.json', 'w') as fp: #salvataggio del json di predizione sul drive\n",
        "    json.dump(results, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sfrutto adesso le funzioni messe a disposizione di *pycocotools* per il calcolo dei risultati."
      ],
      "metadata": {
        "id": "DeO_ixXhpDmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhL681YUZUmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d1aa8c-114b-4987-a479-80a6631bcea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.07s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "test = '/content/drive/MyDrive/test.json'\n",
        "pred = '/content/drive/MyDrive/pred.json'\n",
        "gt = COCO(test)\n",
        "detections = gt.loadRes(pred)\n",
        "\n",
        "imgIds=sorted(gt.getImgIds())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bhNO49PZUwj",
        "outputId": "57176ef6-28df-4113-87da-581dcaae7b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=1.07s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.14s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.367\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.563\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.416\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.238\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.336\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.384\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.280\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.430\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.443\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.255\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.382\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.641\n"
          ]
        }
      ],
      "source": [
        "annType = 'bbox'\n",
        "cocoEval = COCOeval(gt, detections, annType)\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCVxprIaZU_L",
        "outputId": "6b947386-8624-4f7e-d1cc-ba91cf5494b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=1.44s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.13s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.354\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.199\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.155\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.263\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.207\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.269\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.270\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.058\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.456\n"
          ]
        }
      ],
      "source": [
        "annType = 'segm'\n",
        "cocoEval = COCOeval(gt, detections, annType)\n",
        "cocoEval.params.imgIds  = imgIds\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNKmVHn7bZI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-I9FSycw-Ss"
      },
      "source": [
        "# Codici di prova"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWG3sKz2A2ja"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "juYjpgHExE8H",
        "outputId": "db0594fe-83f3-486c-aecf-59b4259393b6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e48842ae96b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustomLossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomLossFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbb_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbb_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "#class logits, labels\n",
        "\n",
        "class CustomLossFunction(nn.Module):\n",
        "  def __init__(self, cls_loss, bb_loss, seg_loss, cls_weight, bb_weight, seg_weight):\n",
        "    super(CustomLossFunction, self).__init__()\n",
        "    self.cls_loss = cls_loss\n",
        "    self.bb_loss = bb_loss\n",
        "    self.seg_loss = seg_loss\n",
        "    self.cls_weight = cls_weight\n",
        "    self.bb_weight = bb_weight\n",
        "    self.seg_weight = seg_weight\n",
        "\n",
        "  def forward(self,predictions,labels):\n",
        "    prediction = predictions[0]\n",
        "    #loss di classificazione\n",
        "    #scores di classificazione: uno per ogni oggetto predetto\n",
        "    labels_cls = labels['labels']\n",
        "    pred_cls = prediction['labels']\n",
        "    loss_cls = self.cls_loss(pred_cls,labels_cls)\n",
        "    #loss di bbox\n",
        "    labels_bbox = labels['boxes']\n",
        "    pred_bbox = prediction['boxes']\n",
        "    loss_bbox = self.bb_loss(pred_bbox,labels_bbox)\n",
        "    #loss per le maschere: sigmoide su tutti i pixel e poi cross entropy\n",
        "    labels_masks = labels['masks']\n",
        "    pred_masks = prediction['masks']\n",
        "    loss_masks = self.seg_loss(pred_masks,labels_masks)\n",
        "\n",
        "    return self.cls_weight*loss_cls + self.bb_weight*loss_bbox + self.seg_weight*loss_masks"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Z-I9FSycw-Ss"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOokgHcODlN1cqegCY5Sddw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}